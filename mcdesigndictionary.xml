<?xml version="1.0"?>
<mcdesigndictionary>
  <designdictionary id="1" detail="MA vs EA" parentid="0" description="&amp;lt;h4&amp;gt;MA vs EA (Krosnogor, 2005)&amp;lt;/h4&amp;gt;[br]&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;It is now well established that pure EAs are not well suited to fine tuning search in complex combinatorial spaces and that hybridization with other techniques can greatly improve the efficiency of search. MAs are extensions of EAs that apply separate processes to refine individuals, for example, improving their fitness by hillclimbing. (Krosnogor cited some references for this).&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;a meme is taken to represent a learning or development strategy, thus a memetic model exhibits the plasticity of individuals that a strictly genetic model fails to catpure &amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;From an optimization point of view, MAs have been shown to be both more efficient (i.e., requiring orders of magnitude fewer evaluations to find optima) and more effective (i.e., identifying higher quality solutions) than traditional EAs for some problem domains.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;It has been argued that the success of MAs is due to the tradeoff between the exploration abilities of the EA, and the exploitation abilities of the local search used. By transfering information between different runs of the local search (by means of genetic operators) the MA is capable of performing a much more efficient search&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;[br][br]&amp;lt;h4&amp;gt;MA vs EA (Ong, 2004)&amp;lt;/h4&amp;gt;[br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;GA Limitation:[br]GAs are capable of exploring and exploiting promising regions of the search space.[br]They can, however, take a relatively long time to locate the local optimum in a region of convergence (and may sometimes not find the optimum with sufficient precision).[br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;Exploration vs Exploitation:[br]Torn and Zilinskas, in the section entitled &amp;lt;i&amp;gt;Global Search[br]Methods: Exploration and Exploitation&amp;lt;/i&amp;gt;, observe that two competing goals govern the design of global search methods: exploration is important to ensure global reliability, i.e., every part of the domain is searched enough to provide a reliable estimate of the global optimum; exploitation is also important since it concentrates the search effort around the best solutions found so far by searching their neighborhoods to produce better[br]solutions[br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;Advantages of MA: [br]Many search algorithms achieve these two goals using a combination of dedicated global and local searches. These are commonly known as hybrid methods. Hybrid genetic algorithm-local search (GA-LS) methods, which incorporate local improvement procedures with  raditional GAs may, thus, be used to improve the performance of GAs in search.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br]&amp;lt;h4&amp;gt;MA vs EA (Jakob, 2006)&amp;lt;/h4&amp;gt;[br]The large number of evaluations is caused by the property of EA of being strong in discovering interesting regions of a given search space (exploration), but unfortunately[br]weak in finding the precise optimum (exploitation) due to their lacking exploitation of[br]local information.[br][br]&amp;lt;h4&amp;gt;MA vs EA (Ong, 2006)&amp;lt;/h4&amp;gt;[br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;Pros:[br]&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;Traditional GAs generally suffer from excessively slow convergence to locate a precise enough solution because of their failure to exploit local information.&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;[br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;Cons[br]&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;One drawback of the MA is that in order for it to be useful on a problem instance, one often needs to carry out extensive tuning of the control parameters&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;h4&amp;gt;MA vs GA (Wilfried, 2007)&amp;lt;/h4&amp;gt;[br]MAs integrate local search in the offspring production part of an EA and, thus, introduce additional strategy parameters controlling the frequency and intensity of the local search among others.[br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;Pros:[br]The benefit of this approach is a speed-up of the resulting hybrid usually in the magnitude of factors. [br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;Cons:[br]The draw back is the greater amount of strategy parameters which have to be adjusted properly.The necessary tuning of strategy parameters is one of the most important obstacles to the broad application of EAs to real-world problems. This can be summarised by the following statement: despite the wide scope of application of Evolutionary[br]Algorithms, they are not widely applied.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;"/>
  <designdictionary id="3" detail="Hybridization" parentid="0" description="How EAs and local search are combined is an extremely important issue that influences the final solution quality and the computational efficiency of the algorithm."/>
  <designdictionary id="4" detail="LS Frequency" parentid="47" description="It is necessary to control the operation of the LS over the total visited solutions. This is because the additional function evaluations required for total search can be very expensive and the MA could become a multi-restart LS and not take advantage of the qualities of the EAs.[br][br]&amp;lt;h4&amp;gt;Cost Benefit Adaptation in Multimeme Algorithms (Jakob 2006; Jakob, 2007)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The basic idea is to use the costs measured in evaluations caused by and the benefit[br]measured in fitness gain obtained from an LS run to control the selection of a particular value of frequency&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;[br]For the fitness gain a relative measure is used, because a certain amount of fitness improvement is much easier to achieve in the beginning of a search than in the end. The relative fitness gain &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; is based on a normalised fitness function in[br]the range of 0 and &amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt;, which turns every task into a maximisation problem. &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; is the ratio between the achieved fitness improvement (&amp;lt;m&amp;gt;f_{LS}&amp;lt;/m&amp;gt;&#x2212;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt;) and the maximum possible one (&amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt;&#x2212;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt;), where [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_{LS}&amp;lt;/m&amp;gt; is the fitness obtained by the LS and &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt; the fitness of the offspring as produced by the evolution.&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br]&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;A set of levels is defined for the frequency parameter, each of which has a probability &amp;lt;m&amp;gt;p&amp;lt;/m&amp;gt; and a value &amp;lt;m&amp;gt;v&amp;lt;/m&amp;gt; containing for each level an appropriate value of the LS frequency. Three consecutive levels are always active, i.e. have a probability &amp;lt;m&amp;gt;p&amp;lt;/m&amp;gt; greater than zero. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Initially, a likeliness of 0.5 is assigned to the lowest level, 0.3 to the next one,[br]and 0.2 to the last one, ensuring that the search will start coarsely. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;[br]If either each level was used at &amp;lt;m&amp;gt;L_{min}&amp;lt;/m&amp;gt; times or they all have been used &amp;lt;m&amp;gt;L_{max}&amp;lt;/m&amp;gt; in total since the last adjustment, The probabilities of the levels are adjusted. The new relation among the active levels &amp;lt;m&amp;gt;L_1&amp;lt;/m&amp;gt;, &amp;lt;m&amp;gt;L_2&amp;lt;/m&amp;gt;, and &amp;lt;m&amp;gt;L_3&amp;lt;/m&amp;gt; is calculated as follows (For each active level &amp;lt;m&amp;gt;L_i&amp;lt;/m&amp;gt; the required evaluations &amp;lt;m&amp;gt;eval_{L_i,LS_k}&amp;lt;/m&amp;gt; and the obtained &amp;lt;m&amp;gt;rfg_{L_i, LS_k} &amp;lt;/m&amp;gt; are calculated per LS (i.e. for each &amp;lt;m&amp;gt;LS_k&amp;lt;/m&amp;gt;) usage and summed up):&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;{sum{LS_k}{}{rfg_{L_1, LS_k}}} / {sum{LS_k}{}{eval_{L_1, LS_k}}}:{sum{LS_k}{}{rfg_{L_2, LS_k}}} / {sum{LS_k}{}{eval_{L_2, LS_k}}}:{sum{LS_k}{}{rfg_{L_3, LS_k}}} / {sum{LS_k}{}{eval_{L_3, LS_k}}}&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;After the above computation, if the lowest or highest active level is given a probability of more than 0.5, the next lower or higher, respectively, is added. The level at the opposite end is dropped and its likeliness is added to its neighbour. The new level is given a probability equal to 20% from the sum of the probabilities of the other two levels. This causes a move of three consecutive levels along the scale of possible ones according to their performance determined by the achieved fitness gain and the required evaluations.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;To ensure mobility in both directions none of the three active levels may have a probability below 0.1.&amp;lt;/p&amp;gt;[br][br][br]&amp;lt;h4&amp;gt;Fitness based LS Frequency Adaptation(Gacia, 2008)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;(Gacia, 2008) have included in the algorithm the Adaptive &amp;lt;m&amp;gt;P_{LS}&amp;lt;/m&amp;gt; Mechanism, which is an adaptive fitness-based method that is very simple. Indeed, this scheme assigns a LS probability value to each chromosome generated by crossover and mutation, &amp;lt;m&amp;gt;c_{new}&amp;lt;/m&amp;gt;:&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;P_{LS}=delim{lbrace}{matrix{2}{2}{1 {f(c_{new}) better than f(C_{worst})} 0.0625 {otherwise}}}{}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]where &amp;lt;m&amp;gt;f&amp;lt;/m&amp;gt; is the fitness function and &amp;lt;m&amp;gt;C_{worst}&amp;lt;/m&amp;gt; is the current worst element in the population. Applying LS to as little of 5% of each population results in[br]faster convergence to good solutions.[br][br]&amp;lt;h4&amp;gt;Simulated Heating (Zitzler, 2000; Bambha, 2004)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The authors introduce a technique, called &amp;lt;b&amp;gt;simulated heating&amp;lt;/b&amp;gt;, that systematically incorporates parameterized local search into the framework of global search. Let &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; be a parameter in the local search (e.g. LS intensity, LS frequency, size of LS neighborhood). The idea of simulated heating can be summarized as follows: Instead of keeping &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; constant for the entire optimization process, &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is initially given a low value (leading to low LS cost &amp;lt;i&amp;gt;C(p)&amp;lt;/i&amp;gt; and accuracy &amp;lt;i&amp;gt;A(p)&amp;lt;/i&amp;gt;) and increase it at certain points in time (which in turn increases &amp;lt;i&amp;gt;C(p)&amp;lt;/i&amp;gt; and &amp;lt;i&amp;gt;A(b)&amp;lt;/i&amp;gt;). &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;According to the authors, If &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is fixed, each iteration of the overall optimization procedure requires approximately the same amount of time. In contrast, with simulated heating the time resources used per iteration are lower at the beginning and higher at the end. That is, in the first half of the time a greater number of iterations is performed than in the second half with regard to a single run. In other words, the goal is to focus on the global search at the beginning and to find promising regions first; for this phase, LS runs with low accuracy, which in turn allows an greater number of optimization steps of GS. Afterwards, more time is spent by LS in order to improve the solutions found So far and/or to assess them more accurately. As a consequence, fewer global search operations are possible during this phase of optimization. &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is systematically increased in the course of time. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;The Heating Scheme &amp;lt;i&amp;gt;H(t)&amp;lt;/i&amp;gt; for parameter &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is a function of optimization iterations &amp;lt;i&amp;gt;t&amp;lt;/i&amp;gt; that defines the value of &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; at different iteration &amp;lt;i&amp;gt;t&amp;lt;/i&amp;gt;&amp;lt;/p&amp;gt;[br]In realizing simulated heating, there are two fundamental ways of implementing a heating scheme &amp;lt;i&amp;gt;H&amp;lt;/i&amp;gt;: by &amp;lt;b&amp;gt;static&amp;lt;/b&amp;gt; or &amp;lt;b&amp;gt;dynamic&amp;lt;/b&amp;gt; adaptation mechanisms. In the first case, it is assumed that the heating scheme is fixed per optimization run. Thus, it may be computed at compile-time or directly before the actual optimization. In the[br]latter case, the heating scheme evolves during run-time, i.e., is computed during the optimization run. Hence, it may vary for different runs&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Let a set of values &amp;lt;m&amp;gt;delim{lbrace}{p_1, p_2, cdots, p_i, cdots, p_m}{rbrace}&amp;lt;/m&amp;gt; be defined for the parameter &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt;, where &amp;lt;m&amp;gt;p_1 &amp;lt; p_2 &amp;lt; cdots &amp;lt; p_i &amp;lt; cdots &amp;lt; p_m&amp;lt;/m&amp;gt;, Example of Dynamic Heating Schemes are as follows: &amp;lt;/p&amp;gt;[br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Fixed Number of Iterations Per Parameter value(FIP): [br]Here, the next parameter value &amp;lt;m&amp;gt;p_i&amp;lt;/m&amp;gt; is taken when for a number &amp;lt;i&amp;gt;tStag&amp;lt;/i&amp;gt; of iterations the quality of the best solution in the solution candidate set has not improved. As a consequence, for each parameter a different amount of time may be considered until the stagnation condition is fulfilled.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Fixed Amount of Time Per Parameter value(FTP): [br]In this case, the next parameter &amp;lt;m&amp;gt;p_i&amp;lt;/m&amp;gt; is taken when for &amp;lt;i&amp;gt;Tstag&amp;lt;/i&amp;gt; seconds the quality of the best solution in the solution candidate set has not improved. As a consequence, for each parameter a different number of iterations may be considered until the stagnation condition is fulfilled.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br]"/>
  <designdictionary id="5" detail="Individual Selection for LS" parentid="3" description="Which individuals in the population should be improved by local search, how should they be chosen"/>
  <designdictionary id="6" detail="LS Intensity" parentid="47" description="&amp;lt;h4&amp;gt;Truncated LS (Krasnogor, 2005)&amp;lt;/h4&amp;gt;[br]In cannonical local search, local search continues until a local optima is found. This may take a long time, and in the continuous domain proof of local optimality may be decidedly nontrivial. Many of the local search procedures embedded within the MAs in the literature are not standard in this sense, that is, they usually perform a shorter &#x201C;truncated&#x201D; local search.[br][br]&amp;lt;h4&amp;gt;Cost Benefit Adaptation in Multimeme Algorithms (Jakob 2006; Jakob, 2007)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The basic idea is to use the costs measured in evaluations caused by and the benefit[br]measured in fitness gain obtained from an LS run to control the selection of a particular value of intensity (e.g. number of iteration that a local searcher will take)&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;[br]For the fitness gain a relative measure is used, because a certain amount of fitness improvement is much easier to achieve in the beginning of a search than in the end. The relative fitness gain &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; is based on a normalised fitness function in[br]the range of 0 and &amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt;, which turns every task into a maximisation problem. &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; is the ratio between the achieved fitness improvement (&amp;lt;m&amp;gt;f_{LS}&amp;lt;/m&amp;gt;&#x2212;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt;) and the maximum possible one (&amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt;&#x2212;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt;), where [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_{LS}&amp;lt;/m&amp;gt; is the fitness obtained by the LS and &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt; the fitness of the offspring as produced by the evolution.&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br]&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;A set of levels is defined for the intensity parameter, each of which has a probability &amp;lt;m&amp;gt;p&amp;lt;/m&amp;gt; and a value &amp;lt;m&amp;gt;v&amp;lt;/m&amp;gt; containing for each level an appropriate value of LS intensity. Three consecutive levels are always active, i.e. have a probability &amp;lt;m&amp;gt;p&amp;lt;/m&amp;gt; greater than zero. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Initially, a likeliness of 0.5 is assigned to the lowest level, 0.3 to the next one,[br]and 0.2 to the last one, ensuring that the search will start coarsely. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;[br]If either each level was used at &amp;lt;m&amp;gt;L_{min}&amp;lt;/m&amp;gt; times or they all have been used &amp;lt;m&amp;gt;L_{max}&amp;lt;/m&amp;gt; in total since the last adjustment, The probabilities of the levels are adjusted. The new relation among the active levels &amp;lt;m&amp;gt;L_1&amp;lt;/m&amp;gt;, &amp;lt;m&amp;gt;L_2&amp;lt;/m&amp;gt;, and &amp;lt;m&amp;gt;L_3&amp;lt;/m&amp;gt; is calculated as follows (For each active level &amp;lt;m&amp;gt;L_i&amp;lt;/m&amp;gt; the required evaluations &amp;lt;m&amp;gt;eval_{L_i,LS_k}&amp;lt;/m&amp;gt; and the obtained &amp;lt;m&amp;gt;rfg_{L_i, LS_k} &amp;lt;/m&amp;gt; are calculated per LS (i.e. for each &amp;lt;m&amp;gt;LS_k&amp;lt;/m&amp;gt;) usage and summed up):&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;{sum{LS_k}{}{rfg_{L_1, LS_k}}} / {sum{LS_k}{}{eval_{L_1, LS_k}}}:{sum{LS_k}{}{rfg_{L_2, LS_k}}} / {sum{LS_k}{}{eval_{L_2, LS_k}}}:{sum{LS_k}{}{rfg_{L_3, LS_k}}} / {sum{LS_k}{}{eval_{L_3, LS_k}}}&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;After the above computation, if the lowest or highest active level is given a probability of more than 0.5, the next lower or higher, respectively, is added. The level at the opposite end is dropped and its likeliness is added to its neighbour. The new level is given a probability equal to 20% from the sum of the probabilities of the other two levels. This causes a move of three consecutive levels along the scale of possible ones according to their performance determined by the achieved fitness gain and the required evaluations.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;To ensure mobility in both directions none of the three active levels may have a probability below 0.1.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Adaptation by Improvement Rate (Guo, 2005)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The local search used in the algorithm is a Simulated Annealing (SA), and the number of iteration &amp;lt;m&amp;gt;iter&amp;lt;/m&amp;gt; neighborhood perturbation at a particular annealing temperature &amp;lt;m&amp;gt;T&amp;lt;/m&amp;gt; is made self-adjusted based on the improvement rate &amp;lt;m&amp;gt;ir&amp;lt;/m&amp;gt;. The &amp;lt;m&amp;gt;ir&amp;lt;/m&amp;gt; at generation &amp;lt;m&amp;gt;g&amp;lt;/m&amp;gt; is defined as:&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;ir=delim{|}{delim{lbrace}{f(a) in nd(g); exists f(b) in nd(g-1); f(a) &amp;lt; f(b)}{rbrace}}{|} / delim{|}{nd(g)}{|}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;where &amp;lt;m&amp;gt;nd(g)&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;nd(g-1)&amp;lt;/m&amp;gt; are respectively the sets of all nondominated solutions held in the external archive at generations &amp;lt;m&amp;gt;g&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;(g-1)&amp;lt;/m&amp;gt;. With the evolution proceeding, &amp;lt;m&amp;gt;ir&amp;lt;/m&amp;gt; decreases gradually towards a small value close to zero, as indicates a bigger room for further improvement at the initial stage and lower probability of producing new nondominated solutions at the final stage. For better &amp;lt;m&amp;gt;ir&amp;lt;/m&amp;gt;, it is essential to tune the local search to reproduce fitter individuals, especial as the approximation approaches to the Pareto front. Thereby, the number iter of local exploration at each temperature in the SA should follow the reverse trend of &amp;lt;m&amp;gt;ir&amp;lt;/m&amp;gt; along the evolution, &amp;lt;m&amp;gt;iter&amp;lt;/m&amp;gt; at generation &amp;lt;m&amp;gt;g&amp;lt;/m&amp;gt; is determined as:[br][br]&amp;lt;m&amp;gt;iter_g(ir)=delim{lbrace}{matrix{4}{2}{{ub} {0&amp;lt;=ir &amp;lt;alpha} {ub-2(ub-lb)({ir-alpha_1}/g)^2} {alpha_1&amp;lt;=ir&amp;lt;alpha} {lb+2(ub-lb)({ir+1-alpha_2-g}/g)^2} {alpha&amp;lt;=ir&amp;lt;alpha_2} {lb} {alpha_2&amp;lt;=ir&amp;lt;1}}}{}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;where &amp;lt;m&amp;gt;lb, ub&amp;lt;/m&amp;gt; is the lower and upper bound of the number of iterations, &amp;lt;m&amp;gt;0 &amp;lt; alpha_1 &amp;lt; alpha &amp;lt; alpha_2 &amp;lt; 1&amp;lt;/m&amp;gt; is some interval cut on the value range of &amp;lt;m&amp;gt;ir&amp;lt;/m&amp;gt;.&amp;lt;/p&amp;gt;[br][br][br]&amp;lt;h4&amp;gt;Adjust LS Intensity based on Simulated Heating (Zitzlet, 2000)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The authors introduce a technique, called &amp;lt;b&amp;gt;simnluted heating&amp;lt;/b&amp;gt;, that systematically incorporates parameterized local search into the framework of global search. Let &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; be a parameter in the local search (e.g. LS intensity). The idea of simulated heating can be summarized as follows: Instead of keeping &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; constant for the entire optimization process, &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is initially given a low value (leading to low LS cost &amp;lt;i&amp;gt;C(p)&amp;lt;/i&amp;gt; and accuracy &amp;lt;i&amp;gt;A(p)&amp;lt;/i&amp;gt;) and increase it at certain points in time (which in turn increases &amp;lt;i&amp;gt;C(p)&amp;lt;/i&amp;gt; and &amp;lt;i&amp;gt;A(b)&amp;lt;/i&amp;gt;). &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;According to the authors, If &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is fixed, each iteration of the overall optimization procedure requires approximately the same amount of time. In contrast, with simulated heating the time resources used per iteration are lower at the beginning and higher at the end. That is, in the first half of the time a greater number of iterations is performed than in the second half with regard to a single run.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;In other words, the goal is to focus on the global search at the beginning and to find promising regions first; for this phase, LS runs with low accuracy, which in turn allows an greater number of optimization steps of GS. Afterwards, more time is spent by LS in order to improve the solutions found So far and/or to assess them more accurately. As a consequence, fewer global search operations are possible during this phase of optimization. &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is systematically increased in the course of time. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;The authors use the term simulated heating for this approach by analogy to simulated annealing where a &amp;quot;temperature&amp;quot; is continuously decreased according to a given cooling[br]scheme. However simulated heating has little similarity to the simulated annealing process&amp;lt;/p&amp;gt;[br]"/>
  <designdictionary id="7" detail="LS Settings" parentid="3" description="&amp;lt;p&amp;gt;Adaptation in this category attempts to adaptively determine the parameter settings in a local search operator, based on change in the population induced by the global and local search interaction. &amp;lt;/p&amp;gt;[br][br][br]&amp;lt;h4&amp;gt;Scale factor in memetic differential evolution (Neri, 2009)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The local search in (Neri, 2009) is applied to optimize the scale factor of DE, the scale factor &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt; is used during the crossover of DE to control its jump. The local search consists of two local searchers: namely Golden Section Search and Hill Climbing. During each iteration, the offspring solution &amp;lt;m&amp;gt;x_i&amp;lt;/m&amp;gt; is obtained using the DE crossover and the scale factor &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt;, if &amp;lt;m&amp;gt;x_i&amp;lt;/m&amp;gt; is the best, a probability &amp;lt;m&amp;gt;rho&amp;lt;/m&amp;gt; is applied to randomly select Golden Section Search or Hill Climbing as the local search to be applied to &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt;.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;The fitness &amp;lt;m&amp;gt;f(F)&amp;lt;/m&amp;gt; must be defined for the local search to identify the better scale factor in the neighborhood of current scale factor. the paper use the following procedure to compute &amp;lt;m&amp;gt;f(F)&amp;lt;/m&amp;gt;&amp;lt;/p&amp;gt;[br][br]&amp;lt;table border=&amp;quot;1&amp;quot;&amp;gt;[br]&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;[br]&amp;lt;b&amp;gt;Procedure::&amp;lt;/b&amp;gt;Compute &amp;lt;m&amp;gt;f(F)&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br]&amp;lt;b&amp;gt;BEGIN&amp;lt;/b&amp;gt; &amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;&amp;lt;m&amp;gt;v=x_t + F(x_{r1} - x_{r2})&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;&amp;lt;m&amp;gt;u=crossover(x_t, x_{r3})&amp;lt;/m&amp;gt;; &amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;&amp;lt;m&amp;gt;return&amp;lt;/m&amp;gt; &amp;lt;m&amp;gt;f(F)=fitness(u)&amp;lt;/m&amp;gt;; &amp;lt;br /&amp;gt;[br]&amp;lt;b&amp;gt;END&amp;lt;/b&amp;gt;&amp;lt;br /&amp;gt;[br]&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;[br]&amp;lt;/table&amp;gt;[br][br]&amp;lt;p&amp;gt;where &amp;lt;m&amp;gt;u&amp;lt;/m&amp;gt; is the offspring solution produced using DE crossover and the scale factor &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt;, and &amp;lt;m&amp;gt;f(F)=fitness(u)&amp;lt;/m&amp;gt; where &amp;lt;m&amp;gt;fitness(u)&amp;lt;/m&amp;gt; is the fitness of the offspring solution &amp;lt;m&amp;gt;u&amp;lt;/m&amp;gt;. The local search choice probability &amp;lt;m&amp;gt;rho&amp;lt;/m&amp;gt; seems is predefined value, therefore the LS selection is random.&amp;lt;/p&amp;gt; "/>
  <designdictionary id="8" detail="LS Frequency" parentid="9" description="It is necessary to control the operation of the LS over the total visited solutions. This is because the additional function evaluations required for total search can be very expensive and the MA could become a multi-restart LS and not take advantage of the qualities of the EAs.[br][br]&amp;lt;h4&amp;gt;Cost Benefit Adaptation in Multimeme Algorithms (Jakob 2006; Jakob, 2007)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The basic idea is to use the costs measured in evaluations caused by and the benefit[br]measured in fitness gain obtained from an LS run to control the selection of a particular value of frequency&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;[br]For the fitness gain a relative measure is used, because a certain amount of fitness improvement is much easier to achieve in the beginning of a search than in the end. The relative fitness gain &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; is based on a normalised fitness function in[br]the range of 0 and &amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt;, which turns every task into a maximisation problem. &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; is the ratio between the achieved fitness improvement (&amp;lt;m&amp;gt;f_{LS}&amp;lt;/m&amp;gt;&#x2212;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt;) and the maximum possible one (&amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt;&#x2212;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt;), where [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_{LS}&amp;lt;/m&amp;gt; is the fitness obtained by the LS and &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt; the fitness of the offspring as produced by the evolution.&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br]&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;A set of levels is defined for the frequency parameter, each of which has a probability &amp;lt;m&amp;gt;p&amp;lt;/m&amp;gt; and a value &amp;lt;m&amp;gt;v&amp;lt;/m&amp;gt; containing for each level an appropriate value of the LS frequency. Three consecutive levels are always active, i.e. have a probability &amp;lt;m&amp;gt;p&amp;lt;/m&amp;gt; greater than zero. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Initially, a likeliness of 0.5 is assigned to the lowest level, 0.3 to the next one,[br]and 0.2 to the last one, ensuring that the search will start coarsely. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;[br]If either each level was used at &amp;lt;m&amp;gt;L_{min}&amp;lt;/m&amp;gt; times or they all have been used &amp;lt;m&amp;gt;L_{max}&amp;lt;/m&amp;gt; in total since the last adjustment, The probabilities of the levels are adjusted. The new relation among the active levels &amp;lt;m&amp;gt;L_1&amp;lt;/m&amp;gt;, &amp;lt;m&amp;gt;L_2&amp;lt;/m&amp;gt;, and &amp;lt;m&amp;gt;L_3&amp;lt;/m&amp;gt; is calculated as follows (For each active level &amp;lt;m&amp;gt;L_i&amp;lt;/m&amp;gt; the required evaluations &amp;lt;m&amp;gt;eval_{L_i,LS_k}&amp;lt;/m&amp;gt; and the obtained &amp;lt;m&amp;gt;rfg_{L_i, LS_k} &amp;lt;/m&amp;gt; are calculated per LS (i.e. for each &amp;lt;m&amp;gt;LS_k&amp;lt;/m&amp;gt;) usage and summed up):&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;{sum{LS_k}{}{rfg_{L_1, LS_k}}} / {sum{LS_k}{}{eval_{L_1, LS_k}}}:{sum{LS_k}{}{rfg_{L_2, LS_k}}} / {sum{LS_k}{}{eval_{L_2, LS_k}}}:{sum{LS_k}{}{rfg_{L_3, LS_k}}} / {sum{LS_k}{}{eval_{L_3, LS_k}}}&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;After the above computation, if the lowest or highest active level is given a probability of more than 0.5, the next lower or higher, respectively, is added. The level at the opposite end is dropped and its likeliness is added to its neighbour. The new level is given a probability equal to 20% from the sum of the probabilities of the other two levels. This causes a move of three consecutive levels along the scale of possible ones according to their performance determined by the achieved fitness gain and the required evaluations.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;To ensure mobility in both directions none of the three active levels may have a probability below 0.1.&amp;lt;/p&amp;gt;[br][br][br]&amp;lt;h4&amp;gt;Fitness based LS Frequency Adaptation(Gacia, 2008)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;(Gacia, 2008) have included in the algorithm the Adaptive &amp;lt;m&amp;gt;P_{LS}&amp;lt;/m&amp;gt; Mechanism, which is an adaptive fitness-based method that is very simple. Indeed, this scheme assigns a LS probability value to each chromosome generated by crossover and mutation, &amp;lt;m&amp;gt;c_{new}&amp;lt;/m&amp;gt;:&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;P_{LS}=delim{lbrace}{matrix{2}{2}{1 {f(c_{new}) better than f(C_{worst})} 0.0625 {otherwise}}}{}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]where &amp;lt;m&amp;gt;f&amp;lt;/m&amp;gt; is the fitness function and &amp;lt;m&amp;gt;C_{worst}&amp;lt;/m&amp;gt; is the current worst element in the population. Applying LS to as little of 5% of each population results in[br]faster convergence to good solutions.[br][br]&amp;lt;h4&amp;gt;Simulated Heating (Zitzler, 2000; Bambha, 2004)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The authors introduce a technique, called &amp;lt;b&amp;gt;simulated heating&amp;lt;/b&amp;gt;, that systematically incorporates parameterized local search into the framework of global search. Let &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; be a parameter in the local search (e.g. LS intensity, LS frequency, size of LS neighborhood). The idea of simulated heating can be summarized as follows: Instead of keeping &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; constant for the entire optimization process, &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is initially given a low value (leading to low LS cost &amp;lt;i&amp;gt;C(p)&amp;lt;/i&amp;gt; and accuracy &amp;lt;i&amp;gt;A(p)&amp;lt;/i&amp;gt;) and increase it at certain points in time (which in turn increases &amp;lt;i&amp;gt;C(p)&amp;lt;/i&amp;gt; and &amp;lt;i&amp;gt;A(b)&amp;lt;/i&amp;gt;). &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;According to the authors, If &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is fixed, each iteration of the overall optimization procedure requires approximately the same amount of time. In contrast, with simulated heating the time resources used per iteration are lower at the beginning and higher at the end. That is, in the first half of the time a greater number of iterations is performed than in the second half with regard to a single run. In other words, the goal is to focus on the global search at the beginning and to find promising regions first; for this phase, LS runs with low accuracy, which in turn allows an greater number of optimization steps of GS. Afterwards, more time is spent by LS in order to improve the solutions found So far and/or to assess them more accurately. As a consequence, fewer global search operations are possible during this phase of optimization. &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is systematically increased in the course of time. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;The Heating Scheme &amp;lt;i&amp;gt;H(t)&amp;lt;/i&amp;gt; for parameter &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is a function of optimization iterations &amp;lt;i&amp;gt;t&amp;lt;/i&amp;gt; that defines the value of &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; at different iteration &amp;lt;i&amp;gt;t&amp;lt;/i&amp;gt;&amp;lt;/p&amp;gt;[br]In realizing simulated heating, there are two fundamental ways of implementing a heating scheme &amp;lt;i&amp;gt;H&amp;lt;/i&amp;gt;: by &amp;lt;b&amp;gt;static&amp;lt;/b&amp;gt; or &amp;lt;b&amp;gt;dynamic&amp;lt;/b&amp;gt; adaptation mechanisms. In the first case, it is assumed that the heating scheme is fixed per optimization run. Thus, it may be computed at compile-time or directly before the actual optimization. In the[br]latter case, the heating scheme evolves during run-time, i.e., is computed during the optimization run. Hence, it may vary for different runs&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Let a set of values &amp;lt;m&amp;gt;delim{lbrace}{p_1, p_2, cdots, p_i, cdots, p_m}{rbrace}&amp;lt;/m&amp;gt; be defined for the parameter &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt;, where &amp;lt;m&amp;gt;p_1 &amp;lt; p_2 &amp;lt; cdots &amp;lt; p_i &amp;lt; cdots &amp;lt; p_m&amp;lt;/m&amp;gt;, Example of Dynamic Heating Schemes are as follows: &amp;lt;/p&amp;gt;[br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Fixed Number of Iterations Per Parameter value(FIP): [br]Here, the next parameter value &amp;lt;m&amp;gt;p_i&amp;lt;/m&amp;gt; is taken when for a number &amp;lt;i&amp;gt;tStag&amp;lt;/i&amp;gt; of iterations the quality of the best solution in the solution candidate set has not improved. As a consequence, for each parameter a different amount of time may be considered until the stagnation condition is fulfilled.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Fixed Amount of Time Per Parameter value(FTP): [br]In this case, the next parameter &amp;lt;m&amp;gt;p_i&amp;lt;/m&amp;gt; is taken when for &amp;lt;i&amp;gt;Tstag&amp;lt;/i&amp;gt; seconds the quality of the best solution in the solution candidate set has not improved. As a consequence, for each parameter a different number of iterations may be considered until the stagnation condition is fulfilled.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br]"/>
  <designdictionary id="9" detail="Preservation of Diversity" parentid="0" description="The problem of premature convergence, whereby the population converges around some suboptimal point, can be particularly problematic for MAs. If the local search is applied until each point has been moved to a local optimum, then this can lead to a loss of diversity within the population unless new local minima are constantly identified. Alternatively, even if local search is terminated before local optimality, an induced search space with wide basins of attractions could also result in premature convergence to the suboptimal solution at the center of a wide basin of attraction. A number of approaches have been developed to combat this problem:[br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;Local search frequency: applying local search to a small fraction of the population (which helps ensure that the rest of the population is diverse).&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Mulitple memes: using multiple local searchers, where each one induces a different search space with distinct local optima&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Local search move acceptance criteria: Monte carlo or fuzzy criteria, that explicitly controls diversity in the local search stage&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;"/>
  <designdictionary id="10" detail="Lamarckian vs Baldwinian" parentid="3" description="&amp;lt;p&amp;gt;The hybridization of EA with local search gives rise to the concepts of Lamarckian evolution and Baldwin effect, which are the often studied techniques in MAs. &amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Lamarckian Learning&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;In Lamarckian evolution individuals improve during their lifetime through local search and the improvement is passed to the next generation. The individuals are selected based on improved fitness and are transferred to the next generation with the improvement incorporated in the genotype.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Baldwinian Learning&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The Baldwin effect utilized in EAs was first investigated by Hinton and Nolan in. Unlike Lamarckian evolution, the improvement does not change the genetic structure (genotype) of the individual that is transferred to the next generation. The individual is kept the same as before local search, but the selection is based on the improved fitness after local search. Baldwin effect follows natural evolution (Darwinian), i.e., learning improves the fitness and selection is based on fitness. The improvement is passed indirectly to the next generation through fitness in Baldwin effect.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Pros and Cons&amp;lt;/h4&amp;gt;[br]While Lamarckian learning may disrupt the schema processing of a GA, Baldwin learning certainly aggravates the mapping problem of multiple genotypes to one phenotype. In a comparison of Baldwin and Lamarckian learning, (Whitley, 1994) showed that utilizing either form of learning would be more effective than the classical GA without any local improvement procedure. They argued that, while Lamarckian learning is faster, it may be susceptible to premature convergence to a local optimum as compared to Baldwin learning.[br][br]In (Yao, 2005), both Lamarckian evolution and Baldwin effect have been examined in combining an evolutionary algorithm and local search. Their results show that there is no significant difference between Lamarckian-evolution-style combination and Baldwin-effect-style combination.[br][br]"/>
  <designdictionary id="11" detail="Local Search Method" parentid="0" description="&amp;lt;h4&amp;gt;Significance (Ong, 2004)&amp;lt;/h4&amp;gt;[br]Davis argues that hybridizing GAs with the most successful local search method for a particular problem gives one the best of bothworlds: correctly implemented, these algorithms should do no worse than the traditional GA or LS alone. Clearly, what this implies is that unless one knows which local search method most suits the problem in hand (along with its correct parameters settings), a MA may not perform at its optimum or worse, it may perform less well than using the GA alone. The influence of the local search method employed has been shown in and to have a major impact on the search performance of MAs. These experiments conducted on two different local methods, demonstrated that the performance obtained by MAs can indeed be worse than that obtained by the GA or LS alone. The varied suitability of LSs to different problems also helps[br]explain why a variety of MAs have emerged in the literature.[br][br]&amp;lt;h4&amp;gt;Challenge (Ong, 2004)&amp;lt;/h4&amp;gt;[br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;[br]Lack of Domain Knowledge: [br]The greatest barrier to further progress is that, with so many local search methods available in the literature, it is almost impossible to know which is most relevant to a problem when one has only limited knowledge of its cost surface before one starts.[br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;Probem Dependence:[br]Moreover, LS(s) by themselves are known to work very differently with different design problems, even among problems from the[br]same design domain. Depending on the complexity of a design problem, local search methods that may have proven to be successful in the past might not work so well, or at all, on others&#x2014;an outcome that is often referred to as the &amp;quot;no free lunch theorem for search&amp;quot;[br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;/ol&amp;gt;"/>
  <designdictionary id="22" detail="Deterministic" parentid="11" description="Deterministic search algorithms are exact search algorithm in which given an initial state, the state at each stage of search can be determined exactly the same as in any independent simulation run."/>
  <designdictionary id="23" detail="Stochastic" parentid="11" description="&amp;lt;h4&amp;gt;Stochastic Process&amp;lt;/h4&amp;gt;[br]Stochastic (meaning &amp;quot;for aim or guess&amp;quot; in Greek) means random. A stochastic process is one whose behavior is non-deterministic, in that a system\'s subsequent state is determined both by the process\'s predictable actions and by a random element. However, according to M. Kac and E. Nelson, any kind of time development (be it deterministic or essentially probabilistic) which is analyzable in terms of probability deserves the name of stochastic process.[br][br]&amp;lt;h4&amp;gt;Stochastic Local Search&amp;lt;/h4&amp;gt;[br]Stochastic local search is a non-deterministic local search in which the next state (candidate solution) searched by the local search is non-deterministically (randomly) determined[br][br]"/>
  <designdictionary id="24" detail="SQP" parentid="22" description="Sequential Quadratic Programming (SQP) is one of the most popular and robust algorithms for nonlinear continuous optimization. The method is based on solving a series of subproblems designed to minimize a quadratic model of the objective subject to a linearization of the constraints. If the problem is unconstrained, then the method reduces to Newton\'s method for finding a point where the gradient of the objective vanishes. If the problem has only equality constraints, then the method is equivalent to applying Newton\'s method to the first-order optimality conditions, or Karush-Kuhn-Tucker conditions, of the problem. A number of packages (including NPSOL, NLPQL, OPSYC, OPTIMA, MATLAB, and SQP) are founded on this approach.[br][br]&amp;lt;h4&amp;gt;Algorithm Basics&amp;lt;/h4&amp;gt;[br]Consider a nonlinear programming problem of the form:&amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;min_{x}f(x)&amp;lt;/m&amp;gt;, s.t. &amp;lt;m&amp;gt;b(x) &amp;gt;= 0&amp;lt;/m&amp;gt;, &amp;lt;m&amp;gt;c(x)=0&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]The Lagrangian for this problem is&amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;L(x, lambda, sigma)=f(x)-lambda^T b(x) - sigma^T c(x)&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]where &amp;lt;m&amp;gt;lambda&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;sigma&amp;lt;/m&amp;gt; are Lagrange multipliers. At an iterate &amp;lt;m&amp;gt;x_k&amp;lt;/m&amp;gt;, a basic sequential quadratic programming algorithm defines an appropriate search direction &amp;lt;m&amp;gt;d_k&amp;lt;/m&amp;gt; as a solution to the quadratic programming subproblem[br][br]&amp;lt;img src=&amp;quot;images/e89d741a07ca6e5c85f59f9550882221.png&amp;quot; /&amp;gt;[br][br][br][br]"/>
  <designdictionary id="25" detail="NAHC" parentid="22" description="&amp;lt;h4&amp;gt;Hill Climbing&amp;lt;/h4&amp;gt;[br]Hill climbing attempts to maximize (or minimize) a function &amp;lt;m&amp;gt;f(x)&amp;lt;/m&amp;gt;, where &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; are discrete states. These states are typically represented by vertices in a graph, where edges in the graph encode nearness or similarity of a graph. Hill climbing will follow the graph from vertex to vertex, always locally increasing (or decreasing) the value of f, until a local maximum (or local minimum) xm is reached. Hill climbing can also operate on a continuous space: in that case, the algorithm is called gradient ascent (or gradient descent if the function is minimized).[br][br]&amp;lt;h4&amp;gt;Next Ascent Hill Climbing (NAHC) (Mitchell, 1997)&amp;lt;/h4&amp;gt;[br]Suppose the representation of a candidate solution is a binary string, the NAHC works as follows:[br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Choose a string at random. Call this string current-hilltop.&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;If the optimum has been found, stop and return it. If max evaluations has been[br]equaled or exceeded, stop and return the highest hilltop that was found. Otherwise continue to step 3.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Mutate single bits in the string from left to right, recording the fitnesses of the[br]resulting strings. If any increase in fitness is found, then set current-hilltop to that[br]increased fitness string, without evaluating any more single-bit mutations of the[br]original string. Go to step 2 with the new current-hilltop, but continue mutating[br]the new string starting after the bit position at which the previous fitness increase[br]was found.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;If no increases in fitness were found, save current-hilltop and go to step 1.&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;"/>
  <designdictionary id="26" detail="SAHC" parentid="22" description="&amp;lt;h4&amp;gt;Hill Climbing&amp;lt;/h4&amp;gt;[br]Hill climbing attempts to maximize (or minimize) a function &amp;lt;m&amp;gt;f(x)&amp;lt;/m&amp;gt;, where &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; are discrete states. These states are typically represented by vertices in a graph, where edges in the graph encode nearness or similarity of a graph. Hill climbing will follow the graph from vertex to vertex, always locally increasing (or decreasing) the value of f, until a local maximum (or local minimum) xm is reached. Hill climbing can also operate on a continuous space: in that case, the algorithm is called gradient ascent (or gradient descent if the function is minimized).[br][br]&amp;lt;h4&amp;gt;Steepest Ascent Hill Climbing (SAHC) (Mitchell, 1997)&amp;lt;/h4&amp;gt;[br]Suppose the representation of a candidate solution is a binary string, the SAHC works as follows:[br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Choose a string at random. Call this string current-hilltop.&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;If the optimum has been found, stop and return it. If max evaluations has been[br]equaled or exceeded, stop and return the highest hilltop that was found. Other-[br]wise continue to step 3.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Systematically mutate each bit in the string from left to right, recording the[br]fitnesses of the resulting strings.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;If any of the resulting strings give a fitness increase, then set current-hilltop to[br]the resulting string giving the highest fitness increase, and go to step 2.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;If there is no fitness increase, then save current-hilltop in a list of all hilltops found and go to step 1. Otherwise, go to step 2 with the new current-hilltop.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;"/>
  <designdictionary id="27" detail="RMHC" parentid="23" description="&amp;lt;h4&amp;gt;Hill Climbing&amp;lt;/h4&amp;gt;[br]Hill climbing attempts to maximize (or minimize) a function &amp;lt;m&amp;gt;f(x)&amp;lt;/m&amp;gt;, where &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; are discrete states. These states are typically represented by vertices in a graph, where edges in the graph encode nearness or similarity of a graph. Hill climbing will follow the graph from vertex to vertex, always locally increasing (or decreasing) the value of f, until a local maximum (or local minimum) xm is reached. Hill climbing can also operate on a continuous space: in that case, the algorithm is called gradient ascent (or gradient descent if the function is minimized).[br][br]&amp;lt;h4&amp;gt;Random Mutation Hill Climbing (RMHC) (Mitchell, 1997)&amp;lt;/h4&amp;gt;[br]Suppose the representation of a candidate solution is a binary string, the RMHC works as follows:[br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Choose a string at random. Call this string best-evaluated.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;If the optimum has been found, stop and return it. If max evaluations has been[br]equaled or exceeded, stop and return the current value of best-evaluated. Otherwise go to step 3.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Choose a locus at random to mutate. If the mutation leads to an equal or higher[br]fitness, then set best-evaluated to the resulting string, and go to step 2.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br]&amp;lt;h4&amp;gt;RMHC for continuous domain optimization&amp;lt;/h4&amp;gt;[br]&amp;lt;table border=&amp;quot;1&amp;quot;&amp;gt;[br]&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;[br]&amp;lt;m&amp;gt;d left delim{[}{0, 1}{]}&amp;lt;/m&amp;gt; or &amp;lt;m&amp;gt;d left rand()(x-x_g)&amp;lt;/m&amp;gt; (where &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; is the current solution or &amp;lt;m&amp;gt;x_g&amp;lt;/m&amp;gt; is the global best solution) &amp;lt;br /&amp;gt;[br]if &amp;lt;m&amp;gt;f(x-d)&amp;lt;/m&amp;gt; &amp;gt; f(x)&amp;lt;/m&amp;gt; then &amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;&amp;lt;m&amp;gt;x left x-d&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br]else if &amp;lt;m&amp;gt;f(x+d) &amp;gt; f(x)&amp;lt;/m&amp;gt; then &amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;&amp;lt;m&amp;gt;x left x+d&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br]end if &amp;lt;br /&amp;gt;[br]repeat until termination &amp;lt;br /&amp;gt;[br]&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;[br]&amp;lt;/table&amp;gt;"/>
  <designdictionary id="28" detail="Branch and Bound" parentid="22" description="&amp;lt;h4&amp;gt;Basic Principle&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;Branch and bound (BB) is a general algorithm for finding optimal solutions of various optimization problems, especially in discrete and combinatorial optimization. It consists of a systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded en masse, by using upper and lower estimated bounds of the quantity being optimized.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Origin&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The method was first proposed by A. H. Land and A. G. Doig in 1960 for linear programming.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Basic Technique&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;Suppose that the goal is to find the minimum value of a function &amp;lt;m&amp;gt;f(x)&amp;lt;/m&amp;gt;, where &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; ranges over some set &amp;lt;m&amp;gt;S&amp;lt;/m&amp;gt; of candidate solutions (the search space). &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;A branch-and-bound procedure requires two tools:[br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Branching: a splitting procedure that, given a set &amp;lt;m&amp;gt;S&amp;lt;/m&amp;gt; of candidate solutions, returns &amp;lt;m&amp;gt;k&amp;lt;/m&amp;gt; (&amp;lt;m&amp;gt;k &amp;gt;= 2&amp;lt;/m&amp;gt;) smaller sets &amp;lt;m&amp;gt;S_1, S_2, cdots, S_i, cdots, S_k&amp;lt;/m&amp;gt; whose union covers &amp;lt;m&amp;gt;S&amp;lt;/m&amp;gt;. Note that the minimum of &amp;lt;m&amp;gt;f(x)&amp;lt;/m&amp;gt; over &amp;lt;m&amp;gt;S&amp;lt;/m&amp;gt; is &amp;lt;m&amp;gt;min delim{lbrace}{v_1, v_2, cdots, v_i, cdots, v_k}{rbrace}&amp;lt;/m&amp;gt; where each &amp;lt;m&amp;gt;v_i&amp;lt;/m&amp;gt; is the minimum of &amp;lt;m&amp;gt;f(x)&amp;lt;/m&amp;gt; within &amp;lt;m&amp;gt;S_i&amp;lt;/m&amp;gt;. This step is called branching, since its recursive application defines a tree structure (the search tree) whose nodes are the subsets of &amp;lt;m&amp;gt;S&amp;lt;/m&amp;gt;.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Bounding: a procedure that computes upper and lower bounds for the minimum value of &amp;lt;m&amp;gt;f(x)&amp;lt;/m&amp;gt; within a given subset &amp;lt;m&amp;gt;S_i&amp;lt;/m&amp;gt;. This step is called bounding.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br]&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;The key idea of the BB algorithm is: if the lower bound for some tree node (set of candidate solutions) &amp;lt;m&amp;gt;A&amp;lt;/m&amp;gt; is greater than the upper bound for some other node &amp;lt;m&amp;gt;B&amp;lt;/m&amp;gt;, then A may be safely discarded from the search. This step is called &amp;lt;b&amp;gt;pruning&amp;lt;/b&amp;gt;, and is usually implemented by maintaining a global variable &amp;lt;m&amp;gt;m&amp;lt;/m&amp;gt; (shared among all nodes of the tree) that records the minimum upper bound seen among all subregions examined so far. Any node whose lower bound is greater than m can be discarded.[br]&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;The recursion stops when the current candidate set &amp;lt;m&amp;gt;S&amp;lt;/m&amp;gt; is reduced to a single element; or also when the upper bound for set &amp;lt;m&amp;gt;S&amp;lt;/m&amp;gt; matches the lower bound. Either way, any element of S will be a minimum of the function within S.[br]&amp;lt;/p&amp;gt;"/>
  <designdictionary id="29" detail="Beam Search" parentid="22" description="&amp;lt;h4&amp;gt;Basic Principles&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;Beam search is a heuristic search algorithm that is an optimization of best-first search that reduces its memory requirement. Best-first search is a graph search which orders all partial solutions (states) according to some heuristic which attempts to predict how close a partial solution is to a complete solution (goal state). In beam search, only a predetermined number of best partial solutions are kept as candidates.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Beam search uses breadth-first search to build its search tree. At each level of the tree, it generates all successors of the states at the current level, sorting them in order of increasing heuristic values.However, it only stores a predetermined number of states at each level (called the beam width). The smaller the beam width, the more states are pruned. Therefore, with an infinite beam width, no states are pruned and beam search is identical to breadth-first search. [br]&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Pros and Cons&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The beam width bounds the memory required to perform the search, at the expense of risking completeness (possibility that it will not terminate) and optimality (possibility that it will not find the best solution). The reason for this risk is that the goal state could potentially be pruned.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Bean Width&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The beam width can either be fixed or variable. [br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;[br]In a fixed beam width, a maximum number of successor states is kept. [br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;In a variable beam width, a threshold is set around the current best state. All states that fall outside this threshold are discarded. Thus, in places where the best path is obvious, a minimal number of states is searched. In places where the best path is ambiguous, many paths will be searched.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Origin&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The term &amp;quot;beam search&amp;quot; was coined by Raj Reddy, Carnegie Mellon University, 1976.&amp;lt;/p&amp;gt;"/>
  <designdictionary id="30" detail="Dynamic Programming" parentid="22" description="&amp;lt;h4&amp;gt;Basic Principles&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;Dynamic programming is an algorithmic paradigm in which a problem is solved by identifying a collection of subproblems and tackling them one by one, smallest first,[br]using the answers to small problems to help figure out larger ones, until the whole lot of them is solved.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;It is applicable to problems that exhibit the properties of overlapping subproblems and optimal substructure (described below). When applicable, the method takes much less time than naive methods.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Top-down dynamic programming simply means storing the results of certain calculations, which are then re-used later because the same calculation is a sub-problem in a larger calculation. Bottom-up dynamic programming involves formulating a complex calculation as a recursive series of simpler calculations.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Dynamic programming in mathematical optimization&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;In terms of mathematical optimization, dynamic programming usually refers to a simplification of a decision by breaking it down into a sequence of decision steps over time. This is done by defining a sequence of value functions &amp;lt;m&amp;gt;V_1 , V_2 , ... V_n&amp;lt;/m&amp;gt;, with an argument &amp;lt;m&amp;gt;y&amp;lt;/m&amp;gt; representing the state of the system at times &amp;lt;m&amp;gt;i&amp;lt;/m&amp;gt; from 1 to &amp;lt;m&amp;gt;n&amp;lt;/m&amp;gt;. The definition of &amp;lt;m&amp;gt;V_n(y)&amp;lt;/m&amp;gt; is the value obtained in state &amp;lt;m&amp;gt;y&amp;lt;/m&amp;gt; at the last time &amp;lt;m&amp;gt;n&amp;lt;/m&amp;gt;. The values &amp;lt;m&amp;gt;V_i&amp;lt;/m&amp;gt; at earlier times &amp;lt;m&amp;gt;i=n-1,n-2,...,2,1&amp;lt;/m&amp;gt; can be found by working backwards, using a recursive relationship called the Bellman equation. &amp;lt;/p&amp;gt;[br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;[br]For &amp;lt;m&amp;gt;i=2,...n, V_i -1&amp;lt;/m&amp;gt; at any state &amp;lt;m&amp;gt;y&amp;lt;/m&amp;gt; is calculated from &amp;lt;m&amp;gt;V_i&amp;lt;/m&amp;gt; by maximizing a simple function (usually the sum) of the gain from decision &amp;lt;m&amp;gt;i-1&amp;lt;/m&amp;gt; and the function &amp;lt;m&amp;gt;V_i&amp;lt;/m&amp;gt; at the new state of the system if this decision is made. Since &amp;lt;m&amp;gt;V_i&amp;lt;/m&amp;gt; has already been calculated, for the needed states, the above operation yields &amp;lt;m&amp;gt;V_i -1&amp;lt;/m&amp;gt; for all the needed states. [br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;[br]Finally, &amp;lt;m&amp;gt;V_1&amp;lt;/m&amp;gt; at the initial state of the system is the value of the optimal solution. The optimal values of the decision variables can be recovered, one by one, by tracking back the calculations already performed.[br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;/ol&amp;gt;[br][br]&amp;lt;h4&amp;gt;Case Study: TSP&amp;lt;/h4&amp;gt;[br]The real problem in solving TSP is to define the stages, states, and decisions:[br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;b&amp;gt;Stage&amp;lt;/b&amp;gt;: let stage &amp;lt;m&amp;gt;t&amp;lt;/m&amp;gt; represent visiting &amp;lt;m&amp;gt;t&amp;lt;/m&amp;gt; cities, &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;b&amp;gt;Decision&amp;lt;/b&amp;gt;: let the decision be where to go next. &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;b&amp;gt;State&amp;lt;/b&amp;gt;: the state has to include information about all the cities visited, plus the city we ended up in. So a state is represented by a pair &amp;lt;m&amp;gt;(i,S)&amp;lt;/m&amp;gt; where &amp;lt;m&amp;gt;S&amp;lt;/m&amp;gt; is the set of &amp;lt;m&amp;gt;t&amp;lt;/m&amp;gt; cities already visited and &amp;lt;m&amp;gt;i&amp;lt;/m&amp;gt; is the last city visited (so &amp;lt;m&amp;gt;i in S&amp;lt;/m&amp;gt;). &amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br]We further define[br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;c_{ij}&amp;lt;/m&amp;gt;: the distance between node &amp;lt;m&amp;gt;i&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;j&amp;lt;/m&amp;gt;&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_t(i, S)&amp;lt;/m&amp;gt;: the value of state &amp;lt;m&amp;gt;(i, S)&amp;lt;/m&amp;gt; at stage &amp;lt;m&amp;gt;t&amp;lt;/m&amp;gt;. In TSP, &amp;lt;m&amp;gt;f_t(i, S)&amp;lt;/m&amp;gt; is the total distances of visiting the city sequence &amp;lt;m&amp;gt;S&amp;lt;/m&amp;gt; &amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]The steps for Dynamic Programming in solving TSP is as follows:&amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;f_{t+1}(j, S)=min_{j notin S}delim{lbrace}{c_{ij}+f_t(i, S)}{rbrace}&amp;lt;/m&amp;gt;"/>
  <designdictionary id="31" detail="k-exchange" parentid="22" description="&amp;lt;h4&amp;gt;2-opt&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;2-opt is a simple local search algorithm first proposed by Croes in 1958 for solving the traveling salesman problem&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;A 2-opt move consists of eliminating two edges and reconnecting the two resulting paths in a different way to obtain a new tour. There is only one way to reconnect the paths that yield a different tour. Among all pairs of edges whose 2-opt exchange decreases the length we choose the pair that gives the shortest tour. This procedure is then iterated until no such pair of edges is found. The resulting tour is called 2-optimal. &amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Variable Depth Search&amp;lt;/h4&amp;gt;[br]For k = 1 or 2, the k-exchange (or similarly k-distance) neighborhoods can often be efficiently searched, but on average the resulting local optima may be poor. For larger values of k, the k-exchange neighborhoods yield better local optima but the effort spent to search the neighborhood might be too large. Variable-depth search methods are techniques that search the k-exchange neighborhood partially. The goal in this partial search is to find solutions that are close in objective function value to the global[br]optima while dramatically reducing the time to search the neighborhood. Typically, they do not guarantee to be local optima. In VLSN search algorithms, we are interested in several types of algorithms for searching a portion of the k-exchange neighborhood. an Example of variable depth search is the Lin-Kernighan algorithm for TSP."/>
  <designdictionary id="32" detail="HJA" parentid="22" description="&amp;lt;h4&amp;gt;Basic Principles&amp;lt;/h4&amp;gt;[br]The Hooke-Jeeves Algorithm (HJA) is a deterministic local searcher which has a steepest descent pivot rule. [br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;[br]HJA initializes [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;the exploratory radius &amp;lt;m&amp;gt;h_{HJA-0}&amp;lt;/m&amp;gt;, &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;an initial candidate solution &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; and &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;a &amp;lt;m&amp;gt;n * n&amp;lt;/m&amp;gt; direction exploratory matrix &amp;lt;m&amp;gt;U=diag (w(1),w(2), . . . ,w(i), . . . ,w(n))&amp;lt;/m&amp;gt;, where &amp;lt;m&amp;gt;w(i)&amp;lt;/m&amp;gt; is the width of the range of variability of the &amp;lt;m&amp;gt;i^{th}&amp;lt;/m&amp;gt; variable. (&amp;lt;m&amp;gt;U(i, :)&amp;lt;/m&amp;gt; the &amp;lt;m&amp;gt;i^{th}&amp;lt;/m&amp;gt; row of the direction matrix)[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;[br]The HJA consists of an exploratory move and a pattern move.[br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;b&amp;gt;Exploratory Move&amp;lt;/b&amp;gt;: &amp;lt;br /&amp;gt;[br]Indicating with &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; the current best candidate solution and with &amp;lt;m&amp;gt;h_{HJA}&amp;lt;/m&amp;gt; the generic radius of the search, the HJA during the exploratory move samples solutions &amp;lt;m&amp;gt;x(i)+h_{HJA} U(i, :)&amp;lt;/m&amp;gt;(&amp;quot;+&amp;quot; move) (&amp;lt;m&amp;gt;n&amp;lt;/m&amp;gt; solutions sampled with &amp;lt;m&amp;gt;i = 1, 2, cdots , n&amp;lt;/m&amp;gt;) or &amp;lt;m&amp;gt;x(i)-h_{HJA} U(i, :)&amp;lt;/m&amp;gt; (&amp;quot;-&amp;quot; move) with &amp;lt;m&amp;gt;i = 1, 2, cdots , n&amp;lt;/m&amp;gt; (those solutions are sampled only along those directions which turned out unsuccessful during the &amp;quot;+&amp;quot; move). If a new current best is found &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; is updated and the pattern move is executed. If a new current best is not found, &amp;lt;m&amp;gt;h_{HJA}&amp;lt;/m&amp;gt; is halved and the exploration is repeated.[br][br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;&amp;lt;b&amp;gt;Pattern Move&amp;lt;/b&amp;gt;:&amp;lt;br /&amp;gt;[br]The HJA pattern move is an aggressive attempt of the algorithm to exploit promising search directions. Rather than centering the following exploration at the most promising explored candidate solution (&amp;lt;m&amp;gt;x_{best}&amp;lt;/m&amp;gt;), the HJA attempts to move further. The algorithm centers the subsequent exploratory move at &amp;lt;m&amp;gt;x_{best}+h_{HJA}U(i, :)&amp;lt;/m&amp;gt; or &amp;lt;m&amp;gt;x_{best}-h_{HJA}U(i, :)&amp;lt;/m&amp;gt;. If this second exploratory move does not outperform f(x) (the exploratory move fails), then an exploratory move with &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; as the center is performed. [br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;The HJA stops when the budget condition of a predefined number of fitness evaluations is reached.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;"/>
  <designdictionary id="33" detail="Golden Section Search" parentid="22" description="&amp;lt;h4&amp;gt;Basic Principle&amp;lt;/h4&amp;gt;[br][br]&amp;lt;p&amp;gt;In this algorithm and lower bound and upper bound is generated: &amp;lt;m&amp;gt;delim{[}{a, b}{]}&amp;lt;/m&amp;gt;&amp;lt;/p&amp;gt; [br][br]&amp;lt;p&amp;gt;[br]During each iteration: [br][br]&amp;lt;table border=&amp;quot;1&amp;quot;&amp;gt;[br]&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;[br]&amp;lt;b&amp;gt;[Step 1:]&amp;lt;/b&amp;gt;&amp;lt;br /&amp;gt;[br]two values are generated: &amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;x_1 = b - {b-a} / {phi}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;x_2 = a + {b-a} / {phi}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br]&amp;lt;b&amp;gt;[Step 2:]&amp;lt;/b&amp;gt;&amp;lt;br /&amp;gt;[br]If &amp;lt;m&amp;gt;f(x_1)&amp;lt;/m&amp;gt; is better than &amp;lt;m&amp;gt;f(x_2)&amp;lt;/m&amp;gt;, then&amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;&amp;lt;m&amp;gt;b left x_2&amp;lt;/m&amp;gt;; &amp;lt;br /&amp;gt;[br]else &amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;&amp;lt;m&amp;gt;a left x_1&amp;lt;/m&amp;gt;.  &amp;lt;br /&amp;gt;[br]end&amp;lt;br /&amp;gt;[br]&amp;lt;b&amp;gt;[Step 3:]&amp;lt;/b&amp;gt;&amp;lt;br /&amp;gt;[br]if Computational budget is not exceeded, go to Step 1. &amp;lt;br /&amp;gt;[br]&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;[br]&amp;lt;/table&amp;gt;"/>
  <designdictionary id="34" detail="Rosenbrock Algorithm" parentid="22" description="&amp;lt;h4&amp;gt;Basic Principle&amp;lt;/h4&amp;gt;[br]The Rosenbrock Algorithm (RA) works on a solution and attempts to improve upon it by means of a local search logic. From a starting point &amp;lt;m&amp;gt;x_0&amp;lt;/m&amp;gt;, a trial is made in all &amp;lt;m&amp;gt;n&amp;lt;/m&amp;gt; orthogonal directions of the &amp;lt;m&amp;gt;n&amp;lt;/m&amp;gt;-dimensional decision space. [br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;When a success is scored (including equality of the objective function values), the changed variable vector is retained and the step length is multiplied by a positive factor &amp;lt;m&amp;gt;alpha &amp;gt; 1&amp;lt;/m&amp;gt;. [br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;For a failure, the vector of variables is left unchanged and the step length is[br]multiplied by a negative factor &amp;lt;m&amp;gt;-1 &amp;lt; beta &amp;lt; 0&amp;lt;/m&amp;gt;. [br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]Following the Rosenbrock\'s suggestion, &amp;lt;m&amp;gt;alpha = 3&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;beta = -0.5&amp;lt;/m&amp;gt; has been set. This process is repeated until at least one success followed by a failure is registered in each direction. When such a condition is satisfied, the orthogonalization procedure of Gram and Schmidt (see Birkho and Lane 1953) is executed and the search, with the new set of directions, begins again. The algorithm is stopped when a budget condition is exceeded."/>
  <designdictionary id="35" detail="Nelder-Mead Simplex" parentid="22" description="&amp;lt;h4&amp;gt;Basic Principle&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The Nelder-Mead simplex search method (NM) is a local search method that doesn\'t require any gradient information. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;A simplex is formed by &amp;lt;m&amp;gt;N+1&amp;lt;/m&amp;gt; points in &amp;lt;m&amp;gt;N&amp;lt;/m&amp;gt; dimensional[br]space. At each step, the worst point &amp;lt;m&amp;gt;x_{worst}&amp;lt;/m&amp;gt; is reflected to with respect the centriod &amp;lt;m&amp;gt;overline{x}&amp;lt;/m&amp;gt; of the other &amp;lt;m&amp;gt;N&amp;lt;/m&amp;gt; points. The NM use three operators to improve the worst point: reflection ( &amp;lt;m&amp;gt;x_r&amp;lt;/m&amp;gt; ), contraction ( &amp;lt;m&amp;gt;x_c&amp;lt;/m&amp;gt; ) and expansion ( &amp;lt;m&amp;gt;x_{cc}&amp;lt;/m&amp;gt; ). In every iteration, a new simplex is formed and the method continues until reaching some stopping criteria.[br]&amp;lt;/p&amp;gt;[br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;x_r = 2 overline{x} - x_{worst}&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;x_c = (overline{x}+x_r)&amp;lt;/m&amp;gt;&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;x_{cc} = (overline{x}+x_{worst}) / 2 &amp;lt;/m&amp;gt;&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;"/>
  <designdictionary id="36" detail="Simulated Annealing" parentid="23" description="&amp;lt;h4&amp;gt;Basic Principle&amp;lt;/h4&amp;gt;[br]Simulated annealing (SA) is a generic probabilistic metaheuristic for the global optimization problem of applied mathematics, namely locating a good approximation to the global minimum of a given function in a large search space. The cannonical form of SA works as follows:[br][br]&amp;lt;table border=&amp;quot;1&amp;quot;&amp;gt;[br]&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;[br]&amp;lt;i&amp;gt;x&amp;lt;/i&amp;gt;: initial solution &amp;lt;br /&amp;gt;[br]while not terminated &amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;for i=1 to &amp;lt;m&amp;gt;I^T_{max}&amp;lt;/m&amp;gt; do &amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;lt;i&amp;gt;x\'&amp;lt;/i&amp;gt; &amp;lt;m&amp;gt;left&amp;lt;/m&amp;gt; perturb(&amp;lt;i&amp;gt;x&amp;lt;/i&amp;gt;) &amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;if &amp;lt;i&amp;gt;f(x\')&amp;lt;/i&amp;gt; is better than &amp;lt;i&amp;gt;f(x)&amp;lt;/i&amp;gt; &amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;lt;i&amp;gt;x&amp;lt;/i&amp;gt; &amp;lt;m&amp;gt;left&amp;lt;/m&amp;gt; &amp;lt;i&amp;gt;x\'&amp;lt;/i&amp;gt; &amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;else &amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;lt;i&amp;gt;x&amp;lt;/i&amp;gt; &amp;lt;m&amp;gt;left&amp;lt;/m&amp;gt; &amp;lt;i&amp;gt;x\'&amp;lt;/i&amp;gt; with probability &amp;lt;m&amp;gt;P(T)&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;end &amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;end &amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;cooling_scheme(&amp;lt;i&amp;gt;T&amp;lt;/i&amp;gt;)&amp;lt;br /&amp;gt;[br]end[br]&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;[br]&amp;lt;/table&amp;gt;[br][br]&amp;lt;p&amp;gt;The state-transition probability &amp;lt;m&amp;gt;P(T)&amp;lt;/m&amp;gt; is dependent on the annealing Temperature &amp;lt;i&amp;gt;T&amp;lt;/i&amp;gt;, and &amp;lt;m&amp;gt;I^T_{max}&amp;lt;/m&amp;gt; is the number of iteration run at fixed annealing temperature &amp;lt;i&amp;gt;T&amp;lt;/i&amp;gt;.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;cooling_scheme(&amp;lt;i&amp;gt;T&amp;lt;/i&amp;gt;) reduce the annealing temperature &amp;lt;i&amp;gt;T&amp;lt;/i&amp;gt;, and thus &amp;lt;m&amp;gt;P(T)&amp;lt;/m&amp;gt;&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Probablistic State Transition based on Annealing Temperature&amp;lt;/h4&amp;gt;[br][br]&amp;lt;h4&amp;gt;Cooling Scheme&amp;lt;/h4&amp;gt;"/>
  <designdictionary id="37" detail="Crossover-based Local Search" parentid="23" description="&amp;lt;h4&amp;gt;Basic Principle&amp;lt;/h4&amp;gt;[br]The local search makes use of the crossover operator and use it as the local search. "/>
  <designdictionary id="38" detail="Tabu Search" parentid="23" description="&amp;lt;h4&amp;gt;Basic Principle&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;Tabu search is a metaheuristic algorithm that can be used for solving combinatorial optimization problems, such as the traveling salesman problem (TSP). Tabu search uses a local or neighbourhood search procedure to iteratively move from a solution &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; to a solution &amp;lt;i&amp;gt;x\'&amp;lt;/i&amp;gt; in the neighbourhood of &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt;, until some stopping criterion has been satisfied. To explore regions of the search space that would be left unexplored by the local search procedure (see local optimality), tabu search modifies the neighbourhood structure of each solution as the search progresses. The solutions admitted to &amp;lt;m&amp;gt;N_t(x)&amp;lt;/m&amp;gt;, the new neighbourhood, are determined through the use of memory structures. The search then progresses by iteratively moving from a solution &amp;lt;i&amp;gt;x&amp;lt;/i&amp;gt; to a solution &amp;lt;i&amp;gt;x\'&amp;lt;/i&amp;gt; in &amp;lt;m&amp;gt;N_t (x)&amp;lt;/m&amp;gt;.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Tabu List&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;Perhaps the most important type of memory structure used to determine the solutions admitted to &amp;lt;m&amp;gt;N _t(x)&amp;lt;/m&amp;gt; is the tabu list. In its simplest form, a tabu list is a short-term memory which contains the solutions that have been visited in the recent past (less than &amp;lt;m&amp;gt;n&amp;lt;/m&amp;gt; iterations ago, where &amp;lt;m&amp;gt;n&amp;lt;/m&amp;gt; is the number of previous solutions to be stored (n is also called the tabu tenure)). Tabu search excludes solutions in the tabu list from &amp;lt;m&amp;gt;N_t (x)&amp;lt;/m&amp;gt;. A variation of a tabu list prohibits solutions that have certain attributes (e.g., solutions to the traveling salesman problem (TSP) which include undesirable arcs) or prevent certain moves (e.g. an arc that was added to a TSP tour cannot be removed in the next n moves). Selected attributes in solutions recently visited are labeled &amp;quot;tabu-active.&amp;quot; Solutions that contain tabu-active elements are &#x201C;tabu&#x201D;. This type of short-term memory is also called &amp;quot;recency-based&amp;quot; memory.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Aspiration Criteria&amp;lt;/h4&amp;gt;[br]Tabu lists containing attributes can be more effective for some domains, although they raise a new problem. When a single attribute is marked as tabu, this typically results in more than one solution being tabu. Some of these solutions that must now be avoided could be of excellent quality and might not have been visited. To mitigate this problem, &amp;quot;aspiration criteria&amp;quot; are introduced: these override a solution\'s tabu state, thereby including the otherwise-excluded solution in the allowed set. A commonly used aspiration criterion is to allow solutions which are better than the currently-known best solution.[br]"/>
  <designdictionary id="39" detail="SLS" parentid="23" description="&amp;lt;h4&amp;gt;Basic Principle&amp;lt;/h4&amp;gt;[br][br]Stochastic Local Searcher (SLS) initializes a starting solution &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; and an exploratory radius &amp;lt;m&amp;gt;sigma_{SLS-0}&amp;lt;/m&amp;gt;. After initialization, &amp;lt;m&amp;gt;2 * n&amp;lt;/m&amp;gt; perturbation vectors &amp;lt;m&amp;gt;h_{SLS}&amp;lt;/m&amp;gt; are generated (&amp;lt;m&amp;gt;n&amp;lt;/m&amp;gt; being the dimensionality of the search space); these vectors have the same length as &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt;, and each element is a random number generated from a normal distribution with mean value 0 and standard deviation &amp;lt;m&amp;gt;\\sigma_{SLS}&amp;lt;/m&amp;gt;. Each of these vectors lead to a point &amp;lt;m&amp;gt;x+h_{SLS}&amp;lt;/m&amp;gt; whose fitness value is calculated and saved. If the most successful perturbed point has a better fitness value than the initial solution, this is replaced with the former, otherwise &amp;lt;m&amp;gt;sigma_{SLS}&amp;lt;/m&amp;gt; is halved, an another set of perturbations vectors is generated. The SLS is stopped after a pre-defined number of evaluations."/>
  <designdictionary id="40" detail="VNS" parentid="23" description="&amp;lt;h4&amp;gt;Basic Principles&amp;lt;/h4&amp;gt;[br]Variable Neighborhood Search (VNS) is based on the facts:[br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;A local minimum with respect to one neighborhood structure is not necessary so for another;&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;A global minimum is a local minimum with respect to all possible neighborhood structures.&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;For many problems local minima with respect to one or several neighborhoods are relatively close to each other.&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br]&amp;lt;h4&amp;gt;Reduced VNS&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;Initialization. Select the set of neighborhood structures &amp;lt;m&amp;gt;N_k&amp;lt;/m&amp;gt;, for &amp;lt;m&amp;gt;k = 1, cdots , k_{max}&amp;lt;/m&amp;gt;, that will be used in the search; find an initial solution &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt;; choose a stopping condition; &amp;lt;/p&amp;gt;[br][br]&amp;lt;ul style=&amp;quot;list-style-type:none&amp;quot;&amp;gt;[br]&amp;lt;li&amp;gt;Repeat until the stopping condition is met:&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Set k = 1;&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Repeat the following steps until &amp;lt;m&amp;gt;k = k_{max}:&amp;lt;/li&amp;gt;[br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Shaking. Generate a point &amp;lt;m&amp;gt;x^{*}&amp;lt;/m&amp;gt; at random from the &amp;lt;m&amp;gt;k^{th}&amp;lt;/m&amp;gt; neighborhood of &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; &amp;lt;m&amp;gt;(x^{*} in N_k(x))&amp;lt;/m&amp;gt;; &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Move or not. If this point is better than the incumbent, move there (&amp;lt;m&amp;gt;x left x^{*}&amp;lt;/m&amp;gt;), and continue the search with &amp;lt;m&amp;gt;N_1&amp;lt;/m&amp;gt;; otherwise, set &amp;lt;m&amp;gt;k = k+1&amp;lt;/m&amp;gt;;&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;End&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;/ol&amp;gt;"/>
  <designdictionary id="41" detail="ES" parentid="23" description="&amp;lt;h4&amp;gt;Basic Principles&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;Evolution strategy (ES) is an optimization technique based on ideas of adaptation and evolution. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Evolution strategies use natural problem-dependent representations, and primarily mutation and selection as search operators. As common with evolutionary algorithms, the operators are applied in a loop. An iteration of the loop is called a generation. The sequence of generations is continued until a termination criterion is met.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Mutation&amp;lt;/h4&amp;gt;[br]As far as real-valued search spaces are concerned, mutation is normally performed by adding a normally distributed random value to each vector component. The step size or mutation strength (i.e. the standard deviation of the normal distribution) is often governed by self-adaptation. Individual step sizes for each coordinate or correlations between coordinates are either governed by self-adaptation or by covariance matrix adaptation (CMA-ES).[br][br]&amp;lt;h4&amp;gt;Self-Adaptation of the Mutation Step Size&amp;lt;/h4&amp;gt;[br]It was observed in evolution strategies that significant progress toward the fitness/objective function\'s optimum, generally, can only happen in a narrow band of the mutation step size &amp;lt;m&amp;gt;sigma&amp;lt;/m&amp;gt;. That narrow band is called evolution window.[br][br]There are three well-known methods to adapt the mutation step size &amp;lt;m&amp;gt;sigma&amp;lt;/m&amp;gt; in evolution strategies:[br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;(1/5-th) Success Rule &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Self-Adaptation (for example through log-normal mutations) &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Cumulative Step Size Adaptation (CSA) &amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br][br]&amp;lt;h4&amp;gt;Selection&amp;lt;/h4&amp;gt;[br]The (environmental) selection in evolution strategies is deterministic and only based on the fitness rankings, not on the actual fitness values. The simplest ES operates on a population of size two: the current point (parent) and the result of its mutation. There are several population update models:[br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;(1 + 1)-ES: Only if the mutant\'s fitness is at least as good as the parent one, it becomes the parent of the next generation. Otherwise the mutant is disregarded. [br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;(1+&amp;lt;m&amp;gt;lambda&amp;lt;/m&amp;gt;)-ES: &amp;lt;m&amp;gt;lambda&amp;lt;/m&amp;gt; mutants can be generated and compete with 1 parent. &amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;(1, &amp;lt;m&amp;gt;lambda&amp;lt;/m&amp;gt;)-ES:  the best mutant becomes the parent of the next generation while the current parent is always disregarded.&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;(&amp;lt;m&amp;gt;mu&amp;lt;/m&amp;gt;/&amp;lt;m&amp;gt;rho&amp;lt;/m&amp;gt;+, &amp;lt;m&amp;gt;lambda&amp;lt;/m&amp;gt;)-ES: This is the comptemporary derivative which use a population of &amp;lt;m&amp;gt;mu&amp;lt;/m&amp;gt; parents and also recombination as an additional operator. This is believed to make them less prone to get stuck in local optima.&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;h4&amp;gt;Origin&amp;lt;/h4&amp;gt;[br]ES was created in the early 1960s and developed further along the 1970s and later by Ingo Rechenberg, Hans-Paul Schwefel and his co-workers, and belongs to the more general class of evolutionary computation or artificial evolution.[br][br][br]"/>
  <designdictionary id="42" detail="ANN Training" parentid="23" description="&amp;lt;h4&amp;gt;Basic Principles&amp;lt;/h4&amp;gt;[br]Those are techniques typically used to training an artificial neural network. Their uses are justified in many cases since the solution representation is a neural network."/>
  <designdictionary id="43" detail="Clustering-based Local Search" parentid="23" description="&amp;lt;h4&amp;gt;Basic Principles&amp;lt;/h4&amp;gt;[br]clustering as local search is normally used when the MA algorithm trying to solve clustering and/or rule/feature extraction"/>
  <designdictionary id="44" detail="LS Move Acceptance Criteria" parentid="9" description="&amp;lt;h4&amp;gt;Dominance based move acceptance (Zinflou, 2008)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;In (Zinflou, 2008), two local searches are retained: the 3-Opt limited arc exchange method (Johnson and McGeoch 1997) and the Or-Opt method (Or 1976).[br]In PMSMO, a movement from a solution &amp;lt;i&amp;gt;x&amp;lt;/i&amp;gt; towards a neighboring solution &amp;lt;i&amp;gt;y&amp;lt;/i&amp;gt; is carried out in both cases. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;First, if the neighboring solution &amp;lt;m&amp;gt;y&amp;lt;/m&amp;gt; dominates the starting solution &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; then a movement is carried out. If, on the contrary, there is no dominance relationship between &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;y&amp;lt;/m&amp;gt;, for each generation, a weight &amp;lt;m&amp;gt;w_i&amp;lt;/m&amp;gt; is determined randomly, where the sum of the weights &amp;lt;m&amp;gt;w_i&amp;lt;/m&amp;gt; is equal to 1 for each of the objectives &amp;lt;m&amp;gt;i&amp;lt;/m&amp;gt; of the problem to be solved. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;The random determination of the weights orients the search in different directions for each generation. The authors then calculate the normalized value &amp;lt;m&amp;gt;n&amp;lt;/m&amp;gt; according to[br]the definition:&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;n=prod{i=1}{m}({y_i} / {x_i})^{w_i}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;where &amp;lt;m&amp;gt;m&amp;lt;/m&amp;gt; represents the total number of objectives. In the context of a minimization, if &amp;lt;m&amp;gt;n&amp;lt;1&amp;lt;/m&amp;gt; a movement is carried out. In the contrary case, no movement is carried out.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Fitnessb-based Diversity Adaptation on Simulated Annealing Temperature (Krasnogor, 2005)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;In (Krasnogor, 2005), The Simulated Annealing (SA) is used as the local search in which the annealing temperature (which determines the move acceptance criteria) is dynamically adjusted based on population diversity. The objective is to keep the population diversity whilee using local search together with a GA. In (Krasnogor, 2005), the annealing temperature &amp;lt;m&amp;gt;T&amp;lt;/m&amp;gt; is set to: &amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;T=1 / delim{|}{f_{max} - f_{avg}}{|}&amp;lt;/m&amp;gt;[br][br]Where &amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;f_{avg}&amp;lt;/m&amp;gt; are the maximum fitness and the average fitness over individuals in the current population. Therefore the probability for accepting a neighboring solution s_n is given by:[br][br]&amp;lt;m&amp;gt;P(s left s_n)=delim{lbrace}{matrix {2}{2}{1 {Delta f &amp;gt; 0} e^{{k Delta f} / {f_{max} - f_{avg}}} otherwise}}{}&amp;lt;/m&amp;gt;[br][br]&amp;lt;p&amp;gt;Where &amp;lt;m&amp;gt;Delta f&amp;lt;/m&amp;gt; is the fitness improvement of &amp;lt;m&amp;gt;s_n&amp;lt;/m&amp;gt; over the current solution &amp;lt;m&amp;gt;s&amp;lt;/m&amp;gt;. The entire population shared the same temperature &amp;lt;m&amp;gt;T&amp;lt;/m&amp;gt;. This temperature determines the extent to which decreasing fitness moves will be allowed. As the spread of fitnesses within the population converges the temperature rises. As a consequence, each individual in the population will be more likely to be changed, exploring the search space. The extent by which a worsening move will be accepted is afunction of both the individual fitness (i.e. its location in the search space) and the global state of the population (i.e. measured by the temperature). Eventually, the fitness will spread, lowering the population temperature. &amp;lt;/m&amp;gt;"/>
  <designdictionary id="45" detail="LS Neighborhood Size" parentid="47" description="&amp;lt;h4&amp;gt;Simulated Heating (Zitzler, 2000; Bambha, 2004)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The authors introduce a technique, called &amp;lt;b&amp;gt;simulated heating&amp;lt;/b&amp;gt;, that systematically incorporates parameterized local search into the framework of global search. Let &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; define the size of the LS neighborhood in the local search. The idea of simulated heating can be summarized as follows: Instead of keeping &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; constant for the entire optimization process, &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is initially given a low value (leading to low LS cost &amp;lt;i&amp;gt;C(p)&amp;lt;/i&amp;gt; and accuracy &amp;lt;i&amp;gt;A(p)&amp;lt;/i&amp;gt;) and increase it at certain points in time (which in turn increases &amp;lt;i&amp;gt;C(p)&amp;lt;/i&amp;gt; and &amp;lt;i&amp;gt;A(b)&amp;lt;/i&amp;gt;). &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;According to the authors, If &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is fixed, each iteration of the overall optimization procedure requires approximately the same amount of time. In contrast, with simulated heating the time resources used per iteration are lower at the beginning and higher at the end. That is, in the first half of the time a greater number of iterations is performed than in the second half with regard to a single run. In other words, the goal is to focus on the global search at the beginning and to find promising regions first; for this phase, LS runs with low accuracy, which in turn allows an greater number of optimization steps of GS. Afterwards, more time is spent by LS in order to improve the solutions found So far and/or to assess them more accurately. As a consequence, fewer global search operations are possible during this phase of optimization. &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is systematically increased in the course of time. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;The Heating Scheme &amp;lt;i&amp;gt;H(t)&amp;lt;/i&amp;gt; for parameter &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is a function of optimization iterations &amp;lt;i&amp;gt;t&amp;lt;/i&amp;gt; that defines the value of &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; at different iteration &amp;lt;i&amp;gt;t&amp;lt;/i&amp;gt;&amp;lt;/p&amp;gt;[br]In realizing simulated heating, there are two fundamental ways of implementing a heating scheme &amp;lt;i&amp;gt;H&amp;lt;/i&amp;gt;: by &amp;lt;b&amp;gt;static&amp;lt;/b&amp;gt; or &amp;lt;b&amp;gt;dynamic&amp;lt;/b&amp;gt; adaptation mechanisms. In the first case, it is assumed that the heating scheme is fixed per optimization run. Thus, it may be computed at compile-time or directly before the actual optimization. In the[br]latter case, the heating scheme evolves during run-time, i.e., is computed during the optimization run. Hence, it may vary for different runs&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Let a set of values &amp;lt;m&amp;gt;delim{lbrace}{p_1, p_2, cdots, p_i, cdots, p_m}{rbrace}&amp;lt;/m&amp;gt; be defined for the parameter &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt;, where &amp;lt;m&amp;gt;p_1 &amp;lt; p_2 &amp;lt; cdots &amp;lt; p_i &amp;lt; cdots &amp;lt; p_m&amp;lt;/m&amp;gt;, Example of Dynamic Heating Schemes are as follows: &amp;lt;/p&amp;gt;[br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Fixed Number of Iterations Per Parameter value(FIP): [br]Here, the next parameter value &amp;lt;m&amp;gt;p_i&amp;lt;/m&amp;gt; is taken when for a number &amp;lt;i&amp;gt;tStag&amp;lt;/i&amp;gt; of iterations the quality of the best solution in the solution candidate set has not improved. As a consequence, for each parameter a different amount of time may be considered until the stagnation condition is fulfilled.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Fixed Amount of Time Per Parameter value(FTP): [br]In this case, the next parameter &amp;lt;m&amp;gt;p_i&amp;lt;/m&amp;gt; is taken when for &amp;lt;i&amp;gt;Tstag&amp;lt;/i&amp;gt; seconds the quality of the best solution in the solution candidate set has not improved. As a consequence, for each parameter a different number of iterations may be considered until the stagnation condition is fulfilled.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br][br]"/>
  <designdictionary id="47" detail="GS vs LS tradeoff" parentid="0" description="This category discusses about the various ways in which GS can incorporate LS into its execution sequence to utilize the complementary nature of GS and LS."/>
  <designdictionary id="48" detail="Micro GA" parentid="23" description="This category of algorithm deploys another GA as the local search inside a GA (which is the global search). Usually MAs that depoly micro GA as local search use two loops, the outer loop is the GA that search the coarse feature of the problem (for example, in determining the optimal ANN network structure for a particular problem), while the inner loop is the micro GA that further refines the search (for example, by determining what whould be the optimal parameters and weights of the ANN network that is determined by the outer loop GA"/>
  <designdictionary id="49" detail="Population Size" parentid="3" description="&amp;lt;h4&amp;gt;FAMA: Fitness Diversity based on &amp;lt;m&amp;gt;xi&amp;lt;/m&amp;gt; (Caponio, 2007)&amp;lt;/h4&amp;gt;[br]In FAMA (Caponio, 2007), at the end of each generation the fitness diversity measure &amp;lt;m&amp;gt;xi&amp;lt;/m&amp;gt; is also used to adaptively control the size of the FAMA population as given by[br]&amp;lt;m&amp;gt;S_{pop}={S^f}_{pop}+{S^{nu}}_{pop}(1-xi)&amp;lt;/m&amp;gt;[br][br]where &amp;lt;m&amp;gt;{S^{f}}_{pop}&amp;lt;/m&amp;gt; is the minimum size of the population and &amp;lt;m&amp;gt;{S^{nu}}_{pop}&amp;lt;/m&amp;gt; is the maximal increase of the population size. If &amp;lt;m&amp;gt;xi approx 1&amp;lt;/m&amp;gt; the population contains high diversity and therefore a small number of solutions need to be exploited. if &amp;lt;m&amp;gt;xi approx 0&amp;lt;/m&amp;gt; the population contains low diversity and will likely converge; in such conditions a larger population size is required to increase the exploration. "/>
  <designdictionary id="50" detail="Mutation Rate" parentid="9" description="&amp;lt;h4&amp;gt;FAMA: Fitness Diversity based on &amp;lt;m&amp;gt;xi&amp;lt;/m&amp;gt;&amp;lt;/h4&amp;gt;[br][br]In FAMA (Caponio, 2007), the mutation rate also dynamically depends on the fitness diversity measure &amp;lt;m&amp;gt;xi&amp;lt;/m&amp;gt;: &amp;lt;m&amp;gt;p_m = 0.4 (1 - xi)&amp;lt;/m&amp;gt;[br][br]"/>
</mcdesigndictionary>
