<?xml version="1.0"?>
<mcmemeticdictionary>
  <memeticdictionary id="1" detail="2&lt;sup&gt;nd&lt;/sup&gt; Gen" parentid="19" description="&amp;lt;h4&amp;gt;Concept&amp;lt;/h4&amp;gt;[br]In this category of MAs, it is given a pool of memes, and before each local search is carried out, a meme is &amp;lt;b&amp;gt;selected&amp;lt;/b&amp;gt; from the &amp;lt;b&amp;gt;given meme pool&amp;lt;/b&amp;gt; to be used as the local search heuristic. The adaptive strategies are &amp;lt;b&amp;gt;structured&amp;lt;/b&amp;gt; to promote cooperation and competition among the different LSs or memes, working together to accomplish the shared optimization goal.[br][br]&amp;lt;h4&amp;gt;Principle&amp;lt;/h4&amp;gt;[br]The idea behind the adaptive strategies is that as the search progresses, the effectiveness of each LS in dealing with the current problem is learned. Knowledge about the current population of solutions and each LS is, thus, built dynamically online,[br]so identifying the strengths and weaknesses of the LSs for the problem currently being worked on, given its current state.[br][br]&amp;lt;h4&amp;gt;Universal Darwinism&amp;lt;/h4&amp;gt;[br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Memetic Transmission&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Memetic Selection&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br]&amp;lt;h4&amp;gt;Pros&amp;lt;/h4&amp;gt;[br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Promotes both cooperation and competition among various problem-specific memes &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Favors neighborhood structures containing high quality solutions that may be arrived at low computational efforts.&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;"/>
  <memeticdictionary id="51" detail="3&lt;sup&gt;rd&lt;/sup&gt; Gen" parentid="19" description=""/>
  <memeticdictionary id="32" detail="Short Term Memory" parentid="30" description="&amp;lt;h4&amp;gt;Meta-Lamarckian: Sub-Problem Decomposition&amp;lt;/h4&amp;gt;[br]The adaptation strategy uses the immediate reward &amp;lt;m&amp;gt;eta = beta delim{|}{pf - cf}{|} / mu&amp;lt;/m&amp;gt; as the fitness, &amp;lt;m&amp;gt;f_{M}&amp;lt;/m&amp;gt; , of a meme &amp;lt;m&amp;gt;M&amp;lt;/m&amp;gt; associated with the structure of the solution chromosome &amp;lt;m&amp;gt;s&amp;lt;/m&amp;gt; before it undergoes the local search secified by the meme. And the tuple (&amp;lt;m&amp;gt;M, f_{M}, s&amp;lt;/m&amp;gt;) is used as the training data for the adaptation strategy.[br][br]&amp;lt;h4&amp;gt;Hyper-Heuristic: Tabu&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The tabu adaptation strategy uses short-term memory structures: once a potential meme &amp;lt;m&amp;gt;M&amp;lt;/m&amp;gt; has been determined, it is marked as &amp;quot;taboo&amp;quot; so that &amp;lt;m&amp;gt;M&amp;lt;/m&amp;gt; is not used repeatedly.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;In the implementation, rank of a heuristic/meme is modified based on a reinforcement learning mechanism that considers the change in the quality of a candidate solution. Ranks are allowed to change within a predetermined range. A tabu list is used for maintaining the low level heuristic(s) generating worsening moves. Tabu duration is used to set the maximum number of iterations for which a heuristic can stay in the tabu list.&amp;lt;/p&amp;gt;"/>
  <memeticdictionary id="5" detail="Hyperheuristic Adaptation" parentid="1" description="&amp;lt;h4&amp;gt;Hyper-Heuristic&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;A hyper-heuristic method operates on top of a set of heuristics. The most appropriate heuristic is determined and applied automatically by the technique at each decision point to solve a given problem.&amp;lt;/p&amp;gt;[br]&amp;lt;p&amp;gt;A hyper-heuristic acts as a heuristic scheduler over a set of heuristics that does the[br]scheduling in a &amp;lt;i&amp;gt;deterministic&amp;lt;/i&amp;gt; or a &amp;lt;i&amp;gt;non-deterministic&amp;lt;/i&amp;gt; way.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Hyper-Heuristic Adaptive MA (Ozcan, 2006)&amp;lt;/h4&amp;gt;[br]A strategy that manages the choice of which meme should be applied at any given time, depending upon the characteristics of the memes and the region of the solution space currently under exploration.With hyperheuristic, multiple memes were considered in the evolution search. [br][br]&amp;lt;h4&amp;gt;Meme Selection Strategies&amp;lt;/h4&amp;gt;[br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;Greedy&amp;lt;br /&amp;gt;[br]This strategy experiments with every meme on the individual to undergo local search, and chooses the meme that resultin the biggest improvement. Since it is a brute-force method, the drawback of greedy hyperheuristic is clearly the high computational cost.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]Choice Function&amp;lt;br /&amp;gt;[br]Choice function incorporate multiple metrics of goodness is used to assess how effective a meme is, based upon the current state of knowledge about the region of the solution space under exploration. The choice function proposed &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt; is composed of three components. [br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;The first component represented by &amp;lt;m&amp;gt;f_1&amp;lt;/m&amp;gt; reflects the recent improvement made by a meme and expresses the idea that if a meme has recently performed well, it is likely to continue to be effective. [br]&amp;lt;li&amp;gt;The second component &amp;lt;m&amp;gt;f_2&amp;lt;/m&amp;gt; describes the improvement contributed by the consecutive pairs of memes, and [br]&amp;lt;li&amp;gt;the last component &amp;lt;m&amp;gt;f_3&amp;lt;/m&amp;gt; records the period elapsed since a meme was last used.&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br]There are several different selection strategies based on &amp;lt;m&amp;gt;f_1, f_2, f_3&amp;lt;/m&amp;gt;:[br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;StraightChoice:[br]the meme that yields the best value for the choice function &amp;lt;i&amp;gt;F&amp;lt;/i&amp;gt; is chosen at each decision point.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;DecomposeChoice:[br]The Decompchoice strategy considers each component in &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt;, i.e., &amp;lt;m&amp;gt;f_1, f_2&amp;lt;/m&amp;gt;, and &amp;lt;m&amp;gt;f_3&amp;lt;/m&amp;gt;, individually. In particular, the strategy experiments with each of the meme and records the best local improvement based on , &amp;lt;m&amp;gt;f_1, f_2, f_3&amp;lt;/m&amp;gt;,  and &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt; individually. Subsequently, the meme that results in the best improvement among those identified is used. This implies that up[br]to four memes will be individually tested in the case when all the highest ranked performing memes are different for &amp;lt;m&amp;gt;f_1, f_2, f_3&amp;lt;/m&amp;gt;, and &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt;[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;TabuChoice:[br]Using the choice function, a tabu-list created may be used to narrow down the choice of memes at each decision point. Rank of a heuristic/meme is modified based on a reinforcement learning mechanism that considers the change in the quality of a candidate solution. Ranks are allowed to change within a predetermined range. A tabu list is used for maintaining the low level heuristic(s) generating worsening moves. Tabu duration is used to set the maximum number of iterations for which a heuristic can stay in the tabu list.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;RankedChoice:[br]Memes are ranked according to &amp;lt;i&amp;gt;F&amp;lt;/i&amp;gt;, and the top ranking memes are experimented individually, and only the meme that yields the largest improvement proceeds with individual learning.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;RouletteChoice:[br]A meme is chosen with probability relative to the overall improvement, i.e., &amp;lt;m&amp;gt;{F(M_e)} / sum{e=1}{n}{F(M_e)} &amp;lt;/m&amp;gt;, where &amp;lt;m&amp;gt;n&amp;lt;/m&amp;gt; is the number of memes in the given meme pool[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br]&amp;lt;/li&amp;gt;[br][br][br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;h4&amp;gt;Hyperheuristic in local searchers (Ozcan, 2006)&amp;lt;/h4&amp;gt;[br](Ozcan, 2006) proposed two adaptive heuristics that utilize a set of constraint-based hill climbers in a co-operative manner. A hyper-heuristic mechanism is used for managing a set of low-level heuristics. At each step, an appropriate heuristic is chosen and applied to a candidate solution. Both adaptive heuristics can be considered as hyper-heuristics. Memetic algorithms employs each hyper-heuristic separately as a single hill climber.[br][br][br]"/>
  <memeticdictionary id="6" detail="1&lt;sup&gt;st&lt;/sup&gt; Gen" parentid="19" description=""/>
  <memeticdictionary id="56" detail="LS Move Acceptance Criteria" parentid="58" description="&amp;lt;h4&amp;gt;Dominance based move acceptance (Zinflou, 2008)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;In (Zinflou, 2008), two local searches are retained: the 3-Opt limited arc exchange method (Johnson and McGeoch 1997) and the Or-Opt method (Or 1976).[br]In PMSMO, a movement from a solution &amp;lt;i&amp;gt;x&amp;lt;/i&amp;gt; towards a neighboring solution &amp;lt;i&amp;gt;y&amp;lt;/i&amp;gt; is carried out in both cases. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;First, if the neighboring solution &amp;lt;m&amp;gt;y&amp;lt;/m&amp;gt; dominates the starting solution &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; then a movement is carried out. If, on the contrary, there is no dominance relationship between &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;y&amp;lt;/m&amp;gt;, for each generation, a weight &amp;lt;m&amp;gt;w_i&amp;lt;/m&amp;gt; is determined randomly, where the sum of the weights &amp;lt;m&amp;gt;w_i&amp;lt;/m&amp;gt; is equal to 1 for each of the objectives &amp;lt;m&amp;gt;i&amp;lt;/m&amp;gt; of the problem to be solved. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;The random determination of the weights orients the search in different directions for each generation. The authors then calculate the normalized value &amp;lt;m&amp;gt;n&amp;lt;/m&amp;gt; according to[br]the definition:&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;n=prod{i=1}{m}({y_i} / {x_i})^{w_i}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;where &amp;lt;m&amp;gt;m&amp;lt;/m&amp;gt; represents the total number of objectives. In the context of a minimization, if &amp;lt;m&amp;gt;n&amp;lt;1&amp;lt;/m&amp;gt; a movement is carried out. In the contrary case, no movement is carried out.&amp;lt;/p&amp;gt;[br]"/>
  <memeticdictionary id="33" detail="Design Issues" parentid="0" description="&amp;lt;p&amp;gt;Here the adaptive memetic algorithms are classified based on the design issue that the adaptation attempts to solve.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Adaptive dynamic schemes can be further subdivided into two categories. The former aims to adaptively execute local searcher parameter settings (e.g. amount of iterations), while the latter adaptively performs a choice of the most suitable meme at each stage of the evolution.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt; Local Search Parameter Settings&amp;lt;/h4&amp;gt;[br]This class of MAs attempts to adaptively execute local searcher parameter settings (e.g. amount of iterations). The adaptive parameter settings include[br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Local Search Intensity&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Local Search Frequency&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Population Size&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Mutation Rate&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;LS Settings:[br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;LS Move Acceptance Criteria&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Size of LS Neighborhood&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br]&amp;lt;h4&amp;gt; Meme Selection&amp;lt;/h4&amp;gt;[br]In this category, the cooperation and competition among memes is promoted by means of an adaptive parameter which measures the sucess rate of local searchers on the individuals. Examples includes the Adaptive MAs in (Ong, 2006)"/>
  <memeticdictionary id="34" detail="Cost Benefit" parentid="38" description="The basic idea is to use the costs measured in evaluations caused by and the benefit[br]measured in fitness gain obtained from an LS run to control the selection of an[br]LS out of a given set and the intensity of their search as well as the frequency of[br]their usage. [br][br]&amp;lt;div style=&amp;quot;color:red&amp;quot;&amp;gt;Note that it is also possible to apply cost-benefit on the population level instead of individual level (E.g. Caponia, 2009)&amp;lt;/div&amp;gt;"/>
  <memeticdictionary id="35" detail="Fitness Diversity" parentid="38" description="The coordination and setting of the local search  is executed by means of a measurement of fitness diversity over the individuals of the population, in order to prevent premature convergence and stagnation and therefore guarantee a more robust algorithm.[br][br]&amp;lt;h4&amp;gt;FAMA and MDA&amp;lt;/h4&amp;gt;[br]The Fast Adaptive MA (FAMA) and Memetic Differential Algorithm (MDA) coordinate multiple local search by means of a measurement of fitness diversity over the individuals of the population. At the end of each iteration the control parameter &amp;lt;m&amp;gt;lambda in delim{lbrace}{xi, psi, nu, chi}{rbrace}&amp;lt;/m&amp;gt; is calculated. [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;xi&amp;lt;/m&amp;gt; is given by:[br]&amp;lt;m&amp;gt;xi = min delim{lbrace}{delim{|}{{f_{best}-f_{avg}} / {f_{best}}}{|}, 1}{rbrace}&amp;lt;/m&amp;gt;[br](Measure: How close is the average fitness to the best one?)[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;psi&amp;lt;/m&amp;gt; is given by:[br]&amp;lt;m&amp;gt;psi = 1 - delim{|}{{f_{avg} - f_{best}} / {f_{worst} - f_{best}}}{|}&amp;lt;/m&amp;gt;[br](Measure: If we sort all the fitness values over a line, which position is occupied by the average fitness?) [br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;nu&amp;lt;/m&amp;gt; is given by:[br]&amp;lt;m&amp;gt;nu = min delim{lbrace}{sigma_f / delim{|}{f_{avg}}{|}, 1}{rbrace}&amp;lt;/m&amp;gt;[br](Measure: How sparse are the fitness values within the population?) [br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;chi&amp;lt;/m&amp;gt; is given by:[br]&amp;lt;m&amp;gt;chi = delim{|}{f_{best} - f_{avg}}{|} / {max delim{|}{f_{best} - f_{avg}}{|}_k}&amp;lt;/m&amp;gt;[br](Measure: How much the super-fit outperforms the remaining part of the population?)[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;p&amp;gt;where &amp;lt;m&amp;gt;f_{best}, f_{avg}, f_{worst}&amp;lt;/m&amp;gt; are respectively best, average and worst fitness over the individuals of the population. &amp;lt;m&amp;gt;sigma_{f}&amp;lt;/m&amp;gt; is the standard deviation over the fitness values of individuals of the population. (Note that in &amp;lt;m&amp;gt;chi&amp;lt;/m&amp;gt; &amp;lt;m&amp;gt;max delim{|}{f_{best} - f_{avg}}{|}_k&amp;lt;/m&amp;gt; is the maximum difference observed (e.g.,at the &amp;lt;m&amp;gt;k^{th}&amp;lt;/m&amp;gt; generation), beginning from the start of the optimization process)&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;It must be noted that the control parameters shown above are three different measurements of fitness diversity that can take values in the interval [0, 1]. The limit conditions &amp;lt;m&amp;gt;lambda = 0&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;lambda = 1&amp;lt;/m&amp;gt; mean no fitness diversity and high fitness diversity respectively. &amp;lt;m&amp;gt;lambda&amp;lt;/m&amp;gt; is then used for performing online the parameter setting of some quantities of both evolutionary framework and local searchers and for executing the choice of the meme.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt; Fitness Diversity based on &amp;lt;m&amp;gt;xi&amp;lt;/m&amp;gt;(Caponio, 2007)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The FAMA (Caponio, 2007) employs two local searchers: the Hooke-Jeeves Algorithm (HJA)[br]and the Nelder-Mead Algorithm (NMA). The two local searchers clearly have different features in terms of pivot rule and neighborhood generating function. The algorithm uses &amp;lt;m&amp;gt;xi&amp;lt;/m&amp;gt; to coordinate local searchers &amp;lt;/p&amp;gt;[br][br]The conditions for use of the local searchers are[br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;[br]if &amp;lt;m&amp;gt;0.05 &amp;lt;= xi &amp;lt;= 0.5&amp;lt;/m&amp;gt;, &amp;lt;m&amp;gt;n+1&amp;lt;/m&amp;gt; individuals of the population selected pseudo-randomly. The NMA is applied to these &amp;lt;m&amp;gt;n+1&amp;lt;/m&amp;gt; individuals and the solutions are inserted into the starting population.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]if &amp;lt;m&amp;gt;xi &amp;lt; 0.1&amp;lt;/m&amp;gt; the HJA is applied to the best individual of the population and the solution returned by the HJA is inserted into the population[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;h4&amp;gt;Fitness Diversity based on &amp;lt;m&amp;gt;psi&amp;lt;/m&amp;gt;(Neri 2007; Neri, 2008)&amp;lt;/h4&amp;gt;[br]The FAMA (Neri, 2008) implements two local searches: namely Hooke-Jeeves Algorithm (HJA) and Simulated Annealing (SA). From the author\'s point of view, SA is a bit of like a &amp;quot;global&amp;quot; local search (due to its ability to accept inferior solution with certain probability) and HJA is &amp;quot;local&amp;quot; (deterministic local search). &amp;lt;m&amp;gt;psi&amp;lt;/m&amp;gt; measures the population diversity in terms of fitness and is relative to the range of the fitness values &amp;lt;m&amp;gt;delim{[}{f_{best}, f_{worst}}{]}&amp;lt;/m&amp;gt; in the population. A low diversity (&amp;lt;m&amp;gt;psi approx 0&amp;lt;/m&amp;gt;) means that the population is converging (possibly in a suboptimal plateau). &amp;lt;m&amp;gt;psi&amp;lt;/m&amp;gt; has been used in order to control coordination among the local searchers:[br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;The SA is activated by the condition &amp;lt;m&amp;gt;psi in delim{[}{0.1, 0.5}{]}&amp;lt;/m&amp;gt;. This adaptive rule is based on the observation that for values of &amp;lt;m&amp;gt;psi &amp;gt; 0.5&amp;lt;/m&amp;gt;, the fitness diversity is high and then the evolutionary framework needs to have a high exploitation of the available genotypes.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;h4&amp;gt;Fitness Diversity based on &amp;lt;m&amp;gt;nu&amp;lt;/m&amp;gt;(Tirronen, 2007; Caponia 2008)&amp;lt;/h4&amp;gt;[br]To coordinate the two local searchers HJA and the SLS in the algorithm, the Memetic Differential Evolution (Tirronen, 2007) after each generation of DE, computes the fitness diversity measure &amp;lt;m&amp;gt;nu&amp;lt;/m&amp;gt; (a measurement of the fitness diversity and distribution of the fitness values within the population). If &amp;lt;m&amp;gt;nu approx 0&amp;lt;/m&amp;gt; all the fitness values are similar amongst each other, while if &amp;lt;m&amp;gt;nu approx 1&amp;lt;/m&amp;gt; fitness values are different, meaning that some individuals perform much better than others. The value of &amp;lt;m&amp;gt;nu&amp;lt;/m&amp;gt; determines whether local searchers should be launched and, if so, which one should be used:[br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;[br]if &amp;lt;m&amp;gt;nu &amp;gt;= 0.5&amp;lt;/m&amp;gt;, no local searchers are activated, and the DE is launched again for the next Spop evaluations;[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]if &amp;lt;m&amp;gt;0.4 &amp;lt;= nu &amp;lt; 0.5&amp;lt;/m&amp;gt;, one individual of the population is pseudo-randomly chosen and the SLS is applied to it;[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]if &amp;lt;m&amp;gt;0.25 &amp;lt;= \\nu &amp;lt; 0.4&amp;lt;/m&amp;gt; the SLS is applied to an individual pseudo-randomly chosen, while the HJA is applied to that individual scoring the best performance;[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]if &amp;lt;m&amp;gt;nu &amp;lt; 0.25&amp;lt;/m&amp;gt; the HJA is applied to the individual with the best performance.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br]&amp;lt;h4&amp;gt;Fitness Diversity based on &amp;lt;m&amp;gt;chi&amp;lt;/m&amp;gt; (Caponio, 2009)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The super-fit control adaptation coordinate local searchers--Nelder-Mead Simplex (NMA) and Rosenbroke Algorithm (RA) -- using the fitness diversity measure &amp;lt;m&amp;gt;chi&amp;lt;/m&amp;gt;. The SFMDE (Super-fit Memetic Differential Evolution) adaptation (and coordination of the local searchers) is based on an attempt to intelligently balance the DE\'s necessity to generate a super-fit individual and prevent stagnation due to an excessive difference between the best performing individual and the others. More specifically, for each local searcher a generalized beta distribution function is generated:&amp;lt;/p&amp;gt;[br]&amp;lt;m&amp;gt;p(chi)={1} / {B(alpha, beta)}  {(chi - a)^{alpha - 1} (b - chi)^{beta - 1}} / {(b-a)^{alpha + beta - 1}}&amp;lt;/m&amp;gt;[br]&amp;lt;p&amp;gt;where &amp;lt;m&amp;gt;a&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;b&amp;lt;/m&amp;gt; are, respectively, the inferior and superior limits of the distribution; &amp;lt;m&amp;gt;B(alpha, beta)&amp;lt;/m&amp;gt; is the beta function; &amp;lt;m&amp;gt;alpha&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;beta&amp;lt;/m&amp;gt; are the shape parameters. For the RA &amp;lt;m&amp;gt;alpha=2&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;beta=5&amp;lt;/m&amp;gt; has been set; for the NMA 2 and 2. Both distribution functions have been normalized with respect to the maximum of the distribution in order to establish a codomain in [0, 1]. At each generation of the SFMDE, a value of &amp;lt;m&amp;gt;chi&amp;lt;/m&amp;gt; is used for determining the probability of activating each of the local searchers. Each of the probability values are compared with a pseudo-random number generated between 0 and 1. If this number is lower than the probability value, the corresponding local search is performed. In addition the RA has been set to be applied to the best performing individual of the population while the NMA is applied to a pseudo-randomly selected individual. [br][br]&amp;lt;h4&amp;gt;Fitness Diversity based on &amp;lt;m&amp;gt;tau&amp;lt;/m&amp;gt; (Kononova, 2008)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The online fitness diversity measure (used to select which of the three local searchers: discrete HJA (DHJA), integer SA (ISA), and their combination), &amp;lt;m&amp;gt;tau&amp;lt;/m&amp;gt; is designed to estimate the balance between the tails of the empirical probability density function (histogram) of the fitness values. &amp;lt;/p&amp;gt;[br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;[br]At the end of each generation &amp;lt;m&amp;gt;i&amp;lt;/m&amp;gt; of the evolutionary framework, the wideness &amp;lt;m&amp;gt;d_i&amp;lt;/m&amp;gt; of the interval of fitness values in the current population is calculated by[br]&amp;lt;m&amp;gt;d_i = f^i_{max} - f^i_{min}&amp;lt;/m&amp;gt;[br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;[br]the number of points &amp;lt;m&amp;gt;n^i_1&amp;lt;/m&amp;gt; in the current population whose fitness falls within the first third of &amp;lt;m&amp;gt;d_i&amp;lt;/m&amp;gt;, i.e. those whose fitness belongs to the subinterval[br]&amp;lt;m&amp;gt;delim{[}{f^i_{min}, f^i_{min} + d_i/3}{]}&amp;lt;/m&amp;gt;, is found[br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;[br]the number of points &amp;lt;m&amp;gt;n_3^i&amp;lt;/m&amp;gt; of the current population whose fitness falls within the last third of &amp;lt;m&amp;gt;d_i&amp;lt;/m&amp;gt;, i.e. those whose fitness belongs to the subinterval &amp;lt;m&amp;gt;delim{[}{f^i_{max}-{d_i} / 3, f^i_{max}}{]}&amp;lt;/m&amp;gt;, is found[br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;[br]their difference is found, &amp;lt;m&amp;gt;tau_3 = n^i_3 - n^i_1&amp;lt;/m&amp;gt;[br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;[br]for a future convenience &amp;lt;m&amp;gt;tau_3&amp;lt;/m&amp;gt; is rescaled and shifted to be within [0, 1] &amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;tau_3=0.5+{tau_3} / {2 * N_{pop}}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br]where &amp;lt;m&amp;gt;N_{pop}&amp;lt;/m&amp;gt; is the size of the population of the evolutionary framework. &amp;lt;m&amp;gt;tau_3&amp;lt;/m&amp;gt; is designed to be used in the manner proposed as a rule to understand the proper necessities of the population of the evolutionary framework. [br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br]In order to launch a suitable Local Searcher, the fitness diversity measure &amp;lt;m&amp;gt;tau&amp;lt;/m&amp;gt; is used. More precisely, at the end of each generation of the Evolutionary Framework a decision is taken based on the value of $\\tau_3$ for the current population according to the following rule:[br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;if &amp;lt;m&amp;gt;tau_3 in&amp;lt;/m&amp;gt; [0, 0.04] then ISA+DHJA is launched&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;if &amp;lt;m&amp;gt;tau_3 in&amp;lt;/m&amp;gt; (0.04, 0.4] then DHJA is launched,&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;if &amp;lt;m&amp;gt;tau_3 in&amp;lt;/m&amp;gt; (0.4, 0.6) then no Local Searcher is launched&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;if &amp;lt;m&amp;gt;tau_3 in&amp;lt;/m&amp;gt; [0.6, 1] then ISA is launched.&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br]"/>
  <memeticdictionary id="17" detail="Multimeme MAs" parentid="1" description="&amp;lt;h4&amp;gt;Concept and Definitions&amp;lt;/h4&amp;gt;[br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;b&amp;gt;Multimeme MAs (Ong, 2006)&amp;lt;/b&amp;gt;:[br]Multimeme MAs use a simple inheritance mechanism for discrete combinatorial search. Each individual is represented and[br]composed by its genetic material and memetic material. The memetic material encoded into its genetic part specifies the[br]meme that will be used to perform local search in the neighborhood of the solution. [br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;&amp;lt;b&amp;gt;MmA (Neri, 2008)&amp;lt;/b&amp;gt;:[br]Krosnogor proposes that the distinction between MAs and Multimeme Algorithms (MmAs) meaning that the former uses only one (usually complex) local search while the latter[br]employs a set of (usually simple) local searchers. Moreover, MmAs (self-)adaptively select from this set which heuristic to use for different stages of the search process.[br]Therefore, when a MmA is designed, the problem regarding how the hybridization can efficiently be performed arises i.e. how the evolutionary framework can intelligently execute the coordination of local searchers.[br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;&amp;lt;b&amp;gt;MultiMeme EAs (Jakob, 2006)&amp;lt;/b&amp;gt;:[br]multimeme EAs do not employ one complex or sophisticated LS (which is the way cannonical MA works), but a set of more or less simple LSs. From this set, it is selected adaptively which one is to be used for different individuals in the course of evolution.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;h4&amp;gt;Simple Inheritance Mechnism (Ozcan, 2007)&amp;lt;/h4&amp;gt;[br](Ozcan, 2007) defines and expands the original SIM and suggest the following meme selection strategy possibilities:[br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;[br]Use the meme of a randomly selected individual in the population (MS1).[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]Use the meme of the best parent. If their fitness are the same select the meme randomly from the parents (MS2).[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]Use the meme owned by the individual which the hill climbing wil be applied to (self-meme), this is the traditional method (MS3).[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]Use the best individuals meme within the population[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]Use the meme of the individual with the median fitness. [br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]Use a mate selection method (e.g., tournament, ranking) as meme selection method, and use the meme of the returned individual.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]Encode meme selection method as a meme (&amp;lt;b&amp;gt;meta-meme&amp;lt;/b&amp;gt;?)[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br]Although the possibility is suggested, (Ozcan, 2007) only experiment MS1, MS2, MS3.[br][br]&amp;lt;h4&amp;gt;Cost Benefit scheme (Jakob, 2006)&amp;lt;/h4&amp;gt;[br]The meme selection is based on the cost and benefit of performing a LS, more details can be found at &amp;quot;Meme Fitness&amp;lt;m&amp;gt;right&amp;lt;/m&amp;gt;Measure&amp;quot;[br][br]&amp;lt;h4&amp;gt;FAMA (Neri, 2008)&amp;lt;/h4&amp;gt;[br]This is a class of Multimeme MAs that use fitness diversity to adaptively select a meme from the meme pool. More details can be found at &amp;quot;Meme Fitness&amp;lt;m&amp;gt;right&amp;lt;/m&amp;gt;Measure&amp;quot;"/>
  <memeticdictionary id="18" detail="Meta-Lamarckian Learning" parentid="1" description="&amp;lt;h4&amp;gt;Concept&amp;lt;/h4&amp;gt;[br]Since the study on using multiple memes in a MA search concentrated on Lamarckian learning, it was termed as meta-Lamarckian learning[br][br]The approach of using multiple LSs during a MA search in the spirit of Lamarckian[br]learning is here termed Meta-Lamarckian learning[br][br]&amp;lt;h4&amp;gt;Sub Problem Decomposition&amp;lt;/h4&amp;gt;[br]The rationale behind the Sub-problem Decomposition strategy was to decompose the original search problem cost surface, which is often large and complex, into many sub-partitions[br]dynamically, and attempts to choose the most competitive[br]meme for each sub-partition. To choose a suitable meme at each decision point, the strategy gathers knowledge about the ability of the memes to search on a particular region of the search space from a database of past experiences archived during the initial EA search. The memes identified then form the candidate memes that will compete, based on their rewards, to decide on which meme will proceed with the local improvement. In this manner, it was shown that the strategy proposed creates opportunities for joint operations between different memes in solving the problem as a whole, because the diverse memes help to improve the overall population based on their areas of specialization. [br][br]&amp;lt;h4&amp;gt;Biased Roulette Wheel&amp;lt;/h4&amp;gt;[br]Assign accumulated fitness to each meme, and at each decision point, select a meme from the meme pool using biased roulette wheel based on the meme fitness"/>
  <memeticdictionary id="31" detail="Long Term Memory" parentid="30" description="&amp;lt;h4&amp;gt;Meta-Lamarckian: Biased Roulette Wheel&amp;lt;/h4&amp;gt;[br]In this adaptation strategy, after each local search, the reward &amp;lt;m&amp;gt;eta = beta delim{|}{pf - cf}{|} / mu&amp;lt;/m&amp;gt; is added to the fitness, &amp;lt;m&amp;gt;f_{M}&amp;lt;/m&amp;gt; of a meme &amp;lt;m&amp;gt;M&amp;lt;/m&amp;gt;. and &amp;lt;m&amp;gt;f_{M}&amp;lt;/m&amp;gt; is used to stochastically select a meme to be applied to any solution &amp;lt;m&amp;gt;s&amp;lt;/m&amp;gt; using biased roulette wheel[br][br]&amp;lt;h4&amp;gt;Hyper-Heuristic: Ant Colony Optimization&amp;lt;/h4&amp;gt;[br]"/>
  <memeticdictionary id="19" detail="Universal Darwinism" parentid="0" description="&amp;lt;h4&amp;gt;&amp;lt;m&amp;gt;1^{st}&amp;lt;/m&amp;gt; generation&amp;lt;/h4&amp;gt;[br]The first generation of MA refers to hybrid algorithms, a marriage between a population-based global search (often in the form of an evolutionary algorithm) coupled with a cultural evolutionary stage. This first generation of MA although encompasses characteristics of cultural evolution (in the form of local refinement) in the search cycle, it may not qualify as a true evolving system according to Universal Darwinism, since all the core principles of inheritance/ memetic transmission, variation and selection are missing. This suggests why the term MA stirred up criticisms and controversies among researchers when first introduced in [1].[br][br]&amp;lt;h4&amp;gt;&amp;lt;m&amp;gt;2^{nd}&amp;lt;/m&amp;gt; generation&amp;lt;/h4&amp;gt;[br]Multi-meme [3], Hyper-heuristic [4] and Meta-Lamarckian MA [5] are referred to as second generation MA exhibiting the principles of memetic transmission and selection in their design. In Multi-meme MA, the memetic material is encoded as part of the genotype. Subsequently, the decoded meme of each respective individual / chromosome is then used to perform a local refinement. The memetic material is then transmitted through a simple inheritance mechanism from parent to offspring(s). On the other hand, in hyper-heuristic and meta-Lamarckian MA, the pool of candidate memes considered will compete, based on their past merits in generating local improvements through a reward mechanism, deciding on which meme to be selected to proceed for future local refinements. Memes with a higher reward have a greater chance of being replicated or copied. For a review on second generation MA, i.e., MA considering multiple individual learning methods within an evolutionary system, the reader is referred to [6].[br][br]&amp;lt;h4&amp;gt;&amp;lt;m&amp;gt;3^{rd}&amp;lt;/m&amp;gt; generation&amp;lt;/h4&amp;gt;[br]Co-evolution [7] and self-generation MAs [8] may be regarded as 3rd generation MA where all three principles satisfying the definitions of a basic evolving system has been considered. In contrast to 2nd generation MA which assumes the pool of memes to be used being known a priori, a rule-based representation of local search is co-adapted alongside candidate solutions within the evolutionary system, thus capturing regular repeated features or patterns in the problem space.[br]"/>
  <memeticdictionary id="20" detail="Meme Fitness" parentid="0" description="In adaptative MA, fitness from the MA environment is essential to determine the direction and magnitude of the change to the strategy parameters and/or heuristic. This is usually measured based on the performance of the particular meme or its parameters and/or the population dynamic of the MA. This section provide the categorization of issues involved in adaptive dynamic of MA. More specifically, it details the following categories:[br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;Scale of meme fitness measurement&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Approach to measure the significance of a meme at various stage of the memetic evolution.&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Scheme used to select meme based on its measured significance&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;"/>
  <memeticdictionary id="23" detail="Type of Learning" parentid="0" description="Adaptation requires understanding and learning from the past and current information obtained from the MA environment. Therefore, various learning paradigms have been employed for the adaptation process. These can be categorized into:[br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;Supervised learning&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Unsupervised learning&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Reinforcement learning&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;"/>
  <memeticdictionary id="24" detail="Supervised Learning" parentid="23" description="&amp;lt;h4&amp;gt;Concept&amp;lt;/h4&amp;gt;[br]Supervised learning is a machine learning technique for deducing a function from training data. The training data consist of pairs of input objects (typically vectors), and desired outputs. The output of the function can be a continuous value (called regression), or can predict a class label of the input object (called classification). The task of the supervised learner is to predict the value of the function for any valid input object after having seen a number of training examples (i.e. pairs of input and target output). To achieve this, the learner has to generalize from the presented data to unseen situations in a &amp;quot;reasonable&amp;quot; way (see inductive bias). (Compare with unsupervised learning.) The parallel task in human and animal psychology is often referred to as concept learning.[br][br]&amp;lt;h4&amp;gt;Meta Lamarckian Learning [Sub-Problem Decomposition]&amp;lt;/h4&amp;gt;[br]In this meme selection strategy, k historical solutions that are closest to the current solution is classified into meme groups based on the memes that are applied to them, then the average meme fitness for each group is computed, the meme that obtain the highest average meme fitness is used as the selected meme. This approach is similar to the k-Nearest-Neighbor classification algorithm which is a supervised learning algorithm. In this case, the set of data used for the supervised learning is the k-nearest neighboring solutions from MA\'s historical data, each is defined by its chromosome (from which distance is defined) as input, and its associated meme as output. The strategy is not classified as reinforcement learning in that input/output pair is presented as learning data, and also suboptimal action is obtained (through the determination of which meme yields the highest average meme fitness). "/>
  <memeticdictionary id="25" detail="Unsupervised Learning" parentid="23" description=""/>
  <memeticdictionary id="27" detail="Reinforcement Learning" parentid="23" description="&amp;lt;h4&amp;gt;Concept&amp;lt;/h4&amp;gt;[br]In the standard reinforcement-learning model, an agent is connected to its environment via perception and action. On each step of interaction the agent receives as input, &amp;lt;m&amp;gt;i&amp;lt;/m&amp;gt;, some indication of the current state, &amp;lt;m&amp;gt;s&amp;lt;/m&amp;gt;, of the environment; the agent then chooses an action, &amp;lt;m&amp;gt;a&amp;lt;/m&amp;gt;, to generate as output. The action changes the state of the environment, and the value of this state transition is communicated to the agent through a scalar reinforcement signal, &amp;lt;m&amp;gt;r&amp;lt;/m&amp;gt;. The agent\'s behavior, &amp;lt;m&amp;gt;B&amp;lt;/m&amp;gt;, should choose actions that tend to increase the long-run sum of values of the reinforcement signal. It can learn to do this over time by systematic trial and error, guided by a wide variety of algorithms.[br][br]&amp;lt;h4&amp;gt;Markov Chain&amp;lt;/h4&amp;gt;[br]The environment is typically formulated as a finite-state Markov decision process (MDP), and reinforcement learning algorithms for this context are highly related to dynamic programming techniques. State transition probabilities and reward probabilities in the MDP are typically stochastic but stationary over the course of the problem.[br][br]&amp;lt;h4&amp;gt;Distinction with Supervised Learning&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;Reinforcement learning differs from the more widely studied problem of supervised learning in several ways. The most important difference is that there is no presentation of input/output pairs. Instead, after choosing an action the agent is told the immediate reward and the subsequent state, but is not told which action would have been in its best long-term interests. It is necessary for the agent to gather useful experience about the possible system states, actions, transitions and rewards actively to act optimally. Another difference from supervised learning is that on-line performance is important: the evaluation of the system is often concurrent with learning.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Reinforcement learning differs from the supervised learning problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected. Further, there is a focus on on-line performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off in reinforcement learning has been mostly studied through the multi-armed bandit problem.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Hyperheuristic Adaptive MAs - Memetic Selection [ChoiceFunction]&amp;lt;/h4&amp;gt;[br]The choice function implemented in the hyperheuristic adaptive MAs is an example of reinforcement learning in which, the input &amp;lt;i&amp;gt;i&amp;lt;/i&amp;gt; is the set current meme fitnesses in the meme pool, the state &amp;lt;m&amp;gt;s&amp;lt;/m&amp;gt; is the previously applied meme, while reward &amp;lt;m&amp;gt;r&amp;lt;/m&amp;gt; is added into the &amp;lt;m&amp;gt;f_{1}&amp;lt;/m&amp;gt;,  &amp;lt;m&amp;gt;f_{2}&amp;lt;/m&amp;gt;,  &amp;lt;m&amp;gt;f_{3}&amp;lt;/m&amp;gt; respectively after the action &amp;lt;m&amp;gt;a&amp;lt;/m&amp;gt; select a new state &amp;lt;m&amp;gt;s&amp;lt;/m&amp;gt;\' (in this case the new meme to be applied at the current decision point)[br][br]&amp;lt;h4&amp;gt;Meta-Lamarckian: Biased Roulette Wheel&amp;lt;/h4&amp;gt;[br]In this adaptation strategy, a meme &amp;lt;m&amp;gt;M&amp;lt;/m&amp;gt; is selected stochastically based on its fitness &amp;lt;m&amp;gt;f_{M}&amp;lt;/m&amp;gt; and applied to the solution &amp;lt;m&amp;gt;s&amp;lt;/m&amp;gt; at the current decision point, the reward &amp;lt;m&amp;gt;eta = beta delim{|}{pf - cf}{|} / mu&amp;lt;/m&amp;gt; is then obtained to added to &amp;lt;m&amp;gt;f_{M}&amp;lt;/m&amp;gt;. In this case, the &amp;lt;m&amp;gt;f_{M}&amp;lt;/m&amp;gt; is the long run-sum of reinforcement signal &amp;lt;m&amp;gt;eta&amp;lt;/m&amp;gt;, and the biased roulette wheel selection strategy should choose a meme that tend to increase &amp;lt;m&amp;gt;f_{M}&amp;lt;/m&amp;gt;"/>
  <memeticdictionary id="30" detail="Historical Data Utilization Scheme" parentid="0" description="&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;use a history of previous results to update the probabilities of applying genetic operators, and local search operators&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;uses the information of ancient populations to selectively determine which local search to be used in combination with global search, thus combining global and local information across time.&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;"/>
  <memeticdictionary id="41" detail="Ratio Measurement" parentid="37" description="&amp;lt;h4&amp;gt;Cost Benefit Adaptation in Multimeme Algorithms (Jakob 2006; Jakob, 2007)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The basic idea is to use the costs measured in evaluations caused by and the benefit[br]measured in fitness gain obtained from an LS run to control the selection of an[br]LS out of a given set and the intensity of their search as well as the frequency of[br]their usage.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;[br]For the fitness gain a relative measure is used, because a certain amount of fitness improvement is much easier to achieve in the beginning of a search than in the end. The relative fitness gain &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; is based on a normalised fitness function in[br]the range of 0 and &amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt;, which turns every task into a maximisation problem. &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; is the ratio between the achieved fitness improvement (&amp;lt;m&amp;gt;f_{LS}&amp;lt;/m&amp;gt;&#x2212;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt;) and the maximum possible one (&amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt;&#x2212;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt;), where [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_{LS}&amp;lt;/m&amp;gt; is the fitness obtained by the LS and &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt; the fitness of the offspring as produced by the evolution.&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br]&amp;lt;/p&amp;gt;[br]For each LS the required evaluation &amp;lt;i&amp;gt;eval&amp;lt;/i&amp;gt; and the obtained &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; are calculated per LS usage and summed up. the probability relation between different local searches &amp;lt;m&amp;gt;LS_i&amp;lt;/m&amp;gt; (&amp;lt;m&amp;gt;i in&amp;lt;/m&amp;gt; {1, 2, 3} assuming only 3 LS) is shown below:&amp;lt;br /&amp;gt;[br][br]&amp;lt;m&amp;gt;{sum{}{}{rfg_{LS_1}}} / {sum{}{}{eval_{LS_1}}}:{sum{}{}{rfg_{LS_2}}} / {sum{}{}{eval_{LS_2}}}:{sum{}{}{rfg_{LS_3}}} / {sum{}{}{eval_{LS_3}}}&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br][br]The adaptation strategy is as follows:[br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;[br]If either each &amp;lt;m&amp;gt;LS_i&amp;lt;/m&amp;gt; was used at minimum K times or there have been &amp;lt;m&amp;gt;M_{max}&amp;lt;/m&amp;gt; matings in total since the last adjustment, the new relation of the LS probabilities is computed. [br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;If the probability of one LS drops below &amp;lt;m&amp;gt;P_{min}&amp;lt;/m&amp;gt; for three consecutive alterations, it is ignored from then on. (To avoid premature deactivation, the probability is set to &amp;lt;m&amp;gt;P_{min}&amp;lt;/m&amp;gt; for the first time it is lower than &amp;lt;m&amp;gt;P_{min}&amp;lt;/m&amp;gt;. For the experiments &amp;lt;m&amp;gt;P_{min}&amp;lt;/m&amp;gt; was set to 0.1) [br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br]As erroneous deactivations of an LS were observed using the above scheme (Jakob, 2006), the adaptation speed had to be reduced by comparably high values for the re-adjustment thresholds. As a consequence, the extended LS adaptation procedure (Jakob, 2007) uses the old distribution by summing up one third of the old probabilities and two thirds of the newly calculated ones, thus resulting in a new likelihood of each LS.[br][br]&amp;lt;div style=&amp;quot;color:red&amp;quot;&amp;gt;Note that the fitness measure for meme is quite similar to (Ong, 2004), however, In (Ong, 2004), the ratio between meme fitnesses of different memes makes no sense since it does not use relative measure by absolute measure.&amp;lt;/div&amp;gt;"/>
  <memeticdictionary id="39" detail="Ordinal Scale" parentid="37" description="&amp;lt;h4&amp;gt;Ordinal Scale&amp;lt;/h4&amp;gt;[br]In this meme fitness metric scale type, the numbers assigned to objects or events represent the rank order (1st, 2nd, 3rd etc.) of the entities assessed. [br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Hyper-Heuristic Adaptive MAs (Ozcan, 2003): In the RankedChoice of ChoiceFunction meme selection method, memes are ranked according to &amp;lt;i&amp;gt;F&amp;lt;/i&amp;gt;, and the top ranking memes are experimented individually, and only the meme that yields the largest improvement proceeds with individual learning.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;"/>
  <memeticdictionary id="40" detail="Interval Scale" parentid="37" description="&amp;lt;h4&amp;gt;Interval scale&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;Quantitative attributes are all measurable on interval scales, as any difference between the levels of an attribute can be multiplied by any real number to exceed or equal another difference. Variables measured at the interval level are called &amp;quot;interval variables&amp;quot; or sometimes &amp;quot;scaled variables&amp;quot; as they have units of measurement.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Ratios between numbers on the scale are not meaningful, so operations such as multiplication and division cannot be carried out directly. But ratios of differences can be expressed; for example, one difference can be twice another.&amp;lt;/p&amp;gt;[br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Meta-Lamarckian (Ong, 2004):[br]during Meta-Lamarckian learning, the reward is measured using the improvements contributed by the LS to each chromosome that has been searched using &amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;eta = beta delim{|}{pf - cf}{|} / mu&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br]where [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;pf&amp;lt;/m&amp;gt; is the initial function fitness of a parent chromosome before local search&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;cf&amp;lt;/m&amp;gt; is the final function fitness of the child chromosome obtained after applying local search&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;mu&amp;lt;/m&amp;gt; is the number of LS function evaluation calls made to reach the[br]improved child chromosome or solution&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br]The term provides a simple measure of the rate at which the local search improves a design. &amp;lt;m&amp;gt;beta&amp;lt;/m&amp;gt; signifies the relative reward which scales the absolute reward in[br]proportion to the method\'s ability to produce high quality genotypes when compared with the best global solution obtained so far. Here, &amp;lt;m&amp;gt;beta&amp;lt;/m&amp;gt; is set as (&amp;lt;m&amp;gt;cf / sigma&amp;lt;/m&amp;gt;) or (&amp;lt;m&amp;gt;sigma / cf&amp;lt;/m&amp;gt;), for minimization and maximization problems, respectively. &amp;lt;m&amp;gt;sigma&amp;lt;/m&amp;gt; is the best solution encountered so far in the global search.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br]"/>
  <memeticdictionary id="37" detail="Scale" parentid="20" description="&amp;lt;h4&amp;gt;Ordinal Scale&amp;lt;/h4&amp;gt;[br]In this meme fitness metric scale type, the numbers assigned to objects or events represent the rank order (1st, 2nd, 3rd etc.) of the entities assessed. [br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Hyper-Heuristic Adaptive MAs (Ozcan, 2003): In the RankedChoice of ChoiceFunction meme selection method, memes are ranked according to &amp;lt;i&amp;gt;F&amp;lt;/i&amp;gt;, and the top ranking memes are experimented individually, and only the meme that yields the largest improvement proceeds with individual learning.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br]&amp;lt;h4&amp;gt;Nominal scale&amp;lt;/h4&amp;gt;[br]At the nominal scale, i.e., for a nominal category, one uses labels; for example, rocks can be generally categorized as igneous, sedimentary and metamorphic. For this scale some valid operations are equivalence and set membership. Nominal measures offer names or labels for certain characteristics.[br][br]Variables assessed on a nominal scale are called categorical variables[br][br]&amp;lt;h4&amp;gt;Interval scale&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;Quantitative attributes are all measurable on interval scales, as any difference between the levels of an attribute can be multiplied by any real number to exceed or equal another difference. Variables measured at the interval level are called &amp;quot;interval variables&amp;quot; or sometimes &amp;quot;scaled variables&amp;quot; as they have units of measurement.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Ratios between numbers on the scale are not meaningful, so operations such as multiplication and division cannot be carried out directly. But ratios of differences can be expressed; for example, one difference can be twice another.&amp;lt;/p&amp;gt;[br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Meta-Lamarckian (Ong, 2004):[br]during Meta-Lamarckian learning, the reward is measured using the improvements contributed by the LS to each chromosome that has been searched using &amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;eta = beta delim{|}{pf - cf}{|} / mu&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br]where [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;pf&amp;lt;/m&amp;gt; is the initial function fitness of a parent chromosome before local search&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;cf&amp;lt;/m&amp;gt; is the final function fitness of the child chromosome obtained after applying local search&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;mu&amp;lt;/m&amp;gt; is the number of LS function evaluation calls made to reach the[br]improved child chromosome or solution&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br]The term provides a simple measure of the rate at which the local search improves a design. &amp;lt;m&amp;gt;beta&amp;lt;/m&amp;gt; signifies the relative reward which scales the absolute reward in[br]proportion to the method\'s ability to produce high quality genotypes when compared with the best global solution obtained so far. Here, &amp;lt;m&amp;gt;beta&amp;lt;/m&amp;gt; is set as (&amp;lt;m&amp;gt;cf / sigma&amp;lt;/m&amp;gt;) or (&amp;lt;m&amp;gt;sigma / cf&amp;lt;/m&amp;gt;), for minimization and maximization problems, respectively. &amp;lt;m&amp;gt;sigma&amp;lt;/m&amp;gt; is the best solution encountered so far in the global search.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br]&amp;lt;h4&amp;gt;Ratio measurement&amp;lt;/h4&amp;gt;[br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Multimeme Algorithms (Jakob, 2007):[br]&amp;lt;p&amp;gt;The basic idea is to use the costs measured in evaluations caused by and the benefit[br]measured in fitness gain obtained from an LS run to control the selection of an[br]LS out of a given set and the intensity of their search as well as the frequency of[br]their usage.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;[br]For the fitness gain a relative measure is used, because a certain amount of fitness improvement is much easier to achieve in the beginning of a search than in the end. The relative fitness gain &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; is based on a normalised fitness function in[br]the range of 0 and &amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt;, which turns every task into a maximisation problem. &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; is the ratio between the achieved fitness improvement (&amp;lt;m&amp;gt;f_{LS}&amp;lt;/m&amp;gt;&#x2212;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt;) and the maximum possible one (&amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt;&#x2212;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt;), where [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_{LS}&amp;lt;/m&amp;gt; is the fitness obtained by the LS and &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt; the fitness of the offspring as produced by the evolution.&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br]&amp;lt;/p&amp;gt;[br]For each LS the required evaluation &amp;lt;i&amp;gt;eval&amp;lt;/i&amp;gt; and the obtained &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; are calculated per LS usage and summed up. the probability relation between different local searches &amp;lt;m&amp;gt;LS_i&amp;lt;/m&amp;gt; (&amp;lt;m&amp;gt;i in&amp;lt;/m&amp;gt; {1, 2, 3} assuming only 3 LS) is shown below:&amp;lt;br /&amp;gt;[br][br]&amp;lt;m&amp;gt;{sum{}{}{rfg_{LS_1}}} / {sum{}{}{eval_{LS_1}}}:{sum{}{}{rfg_{LS_2}}} / {sum{}{}{eval_{LS_2}}}:{sum{}{}{rfg_{LS_3}}} / {sum{}{}{eval_{LS_3}}}&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br][br]The adaptation strategy is as follows:[br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;[br]If either each &amp;lt;m&amp;gt;LS_i&amp;lt;/m&amp;gt; was used at minimum $K$ times or there have been $M_{max}$ matings in total since the last adjustment, the new relation of the LS probabilities is computed. [br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;If the probability of one LS drops below &amp;lt;m&amp;gt;P_{min}&amp;lt;/m&amp;gt; for three consecutive alterations, it is ignored from then on. (To avoid premature deactivation, the probability is set to &amp;lt;m&amp;gt;P_{min}&amp;lt;/m&amp;gt; for the first time it is lower than &amp;lt;m&amp;gt;P_{min}&amp;lt;/m&amp;gt;. For the experiments &amp;lt;m&amp;gt;P_{min}&amp;lt;/m&amp;gt; was set to 0.1) [br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br]As erroneous deactivations of an LS were observed using the above scheme (Jakob, 2006), the adaptation speed had to be reduced by comparably high values for the re-adjustment thresholds. As a consequence, the extended LS adaptation procedure (Jakob, 2007) uses the old distribution by summing up one third of the old probabilities and two thirds of the newly calculated ones, thus resulting in a new likelihood of each LS.[br][br]&amp;lt;div style=&amp;quot;color:red&amp;quot;&amp;gt;Note that the fitness measure for meme is quite similar to (Ong, 2004), however, In (Ong, 2004), the ratio between meme fitnesses of different memes makes no sense since it does not use relative measure by absolute measure.&amp;lt;/div&amp;gt;[br][br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;/ol&amp;gt;[br][br][br][br][br][br]"/>
  <memeticdictionary id="38" detail="Measure" parentid="20" description="The category classify the meme adaptation based on [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;what is being measured and &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;on what data is the measure derived from.&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;h4&amp;gt;What is being measured?&amp;lt;/h4&amp;gt;[br]The meme fitness measurement can be based on[br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;b&amp;gt;Cost Benefit&amp;lt;/b&amp;gt;: Improvement on individual solution against the computational cost spent by the meme to achieve the improvement&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;b&amp;gt;Fitness diversity&amp;lt;/b&amp;gt;: The fitness diversity of the individuals in the population&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;h4&amp;gt;What data on which the measure is derive?&amp;lt;/h4&amp;gt;[br]For the cost benefit scheme, measure is derived from either[br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;Each individual solution on which the meme is applied&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Each generation of solutions on which the meme is applied&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]For the fitness diversity scheme, measure is derived from either[br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;current generation of solutions&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;current generation of solutions in comparison against the previous history of solutions&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;"/>
  <memeticdictionary id="42" detail="Multi-Objective Adaptation" parentid="45" description="&amp;lt;h4&amp;gt;Simple Random (Zinflou, 2008) &amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;In (Zinflou, 2008), two local searches are retained: the 3-Opt limited arc exchange method (Johnson and McGeoch 1997) and the Or-Opt method (Or 1976).[br]In PMSMO, a movement from a solution &amp;lt;i&amp;gt;x&amp;lt;/i&amp;gt; towards a neighboring solution &amp;lt;i&amp;gt;y&amp;lt;/i&amp;gt; is carried out in both cases. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;First, if the neighboring solution &amp;lt;m&amp;gt;y&amp;lt;/m&amp;gt; dominates the starting solution &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; then a movement is carried out. If, on the contrary, there is no dominance relationship between &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;y&amp;lt;/m&amp;gt;, for each generation, a weight &amp;lt;m&amp;gt;w_i&amp;lt;/m&amp;gt; is determined randomly, where the sum of the weights &amp;lt;m&amp;gt;w_i&amp;lt;/m&amp;gt; is equal to 1 for each of the objectives &amp;lt;m&amp;gt;i&amp;lt;/m&amp;gt; of the problem to be solved. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;The random determination of the weights orients the search in different directions for each generation. The authors then calculate the normalized value &amp;lt;m&amp;gt;n&amp;lt;/m&amp;gt; according to[br]the definition:&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;n=prod{i=1}{m}({y_i} / {x_i})^{w_i}&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;where &amp;lt;m&amp;gt;m&amp;lt;/m&amp;gt; represents the total number of objectives. In the context of a minimization, if &amp;lt;m&amp;gt;n&amp;lt;1&amp;lt;/m&amp;gt; a movement is carried out. In the contrary case, no movement is carried out.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Cross-dominance adaptation in multi-objective MA (Caponia, 2009)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;In order to determine whether Simulated Annealing (SA) or Rosenbroke Algorithm (RA) is applied as the local searcher, a value called &amp;lt;b&amp;gt;Cross-Dominance&amp;lt;/b&amp;gt;, at the end of each generation, &amp;lt;m&amp;gt;lambda&amp;lt;/m&amp;gt;, is calculated by:&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;&amp;lt;m&amp;gt;lambda=Lambda^{t+1} / N^2&amp;lt;/m&amp;gt;&amp;lt;/p&amp;gt;[br][br]where [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;[br]&amp;lt;m&amp;gt;N&amp;lt;/m&amp;gt; is the size of the population[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;Lambda&amp;lt;/m&amp;gt; is the number of dominances found when comparing the two population at generation &amp;lt;m&amp;gt;t&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;t+1&amp;lt;/m&amp;gt;, each having &amp;lt;m&amp;gt;N&amp;lt;/m&amp;gt; candidate solutions (therefore a total of &amp;lt;m&amp;gt;N^2&amp;lt;/m&amp;gt; comparisons between each solution in &amp;lt;m&amp;gt;P(t)&amp;lt;/m&amp;gt; and that in &amp;lt;m&amp;gt;P(t+1)&amp;lt;/m&amp;gt;). [br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]The algorithm can monitor the overall improvements of the population by means of &amp;lt;m&amp;gt;lambda in delim{[}{0, 1}{]}&amp;lt;/m&amp;gt;. More specifically,[br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;If &amp;lt;m&amp;gt;lambda = 1&amp;lt;/m&amp;gt;, the algorithm is making excellent improvements and all individuals of the population at generation &amp;lt;m&amp;gt;t+1&amp;lt;/m&amp;gt; strictly dominate all individuals at generation &amp;lt;m&amp;gt;t&amp;lt;/m&amp;gt;. [br]&amp;lt;li&amp;gt;if &amp;lt;m&amp;gt;lambda = 0&amp;lt;/m&amp;gt; the algorithm is not leading to any improvement and the new population is equivalent to the old one in terms of dominance.&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;p&amp;gt;In order to perform coordination of the local search, for each local searcher, a generalized Wigner semicircle distribution is generated:&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;p(lambda)={2} / {pi R^2} sqrt{R^2 - (lambda - a)^2} {c} / {{2} / {pi R}}&amp;lt;/m&amp;gt;[br][br]&amp;lt;p&amp;gt;where [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;R&amp;lt;/m&amp;gt; is the radius of the distribution (the shape of the distribution depends on &amp;lt;m&amp;gt;R&amp;lt;/m&amp;gt;), [br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;a&amp;lt;/m&amp;gt; determines the shift of the distribution, [br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;c&amp;lt;/m&amp;gt; is the maximum value of the distribution. [br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br]&amp;lt;p&amp;gt;&amp;lt;m&amp;gt;p(lambda)&amp;lt;/m&amp;gt; returns the probability of the local search activation dependent upon the adaptive parameter &amp;lt;m&amp;gt;lambda&amp;lt;/m&amp;gt;.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;To coordinate local searches, during each generation, &amp;lt;m&amp;gt;p(lambda)&amp;lt;/m&amp;gt; is computed, and a pseudo-random number &amp;lt;m&amp;gt;epsilon in delim{[}{0, 1}{]}&amp;lt;/m&amp;gt; is generated and compared against &amp;lt;m&amp;gt;p(lambda)&amp;lt;/m&amp;gt;, [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;[br]if &amp;lt;m&amp;gt;epsilon &amp;lt; p(lambda)&amp;lt;/m&amp;gt;, execute simulated annealing on 5 individuals pseudo-randomly selected, for 3000 fitness evaluations; replace the 5 individuals with the results of the simulated annealing;[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]otherwise, execute RA on 25 individuals pseudo-randomly selected, for 1000 fitness evaluations;[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;h4&amp;gt;Use of Heuristic Local Search for Single-Objective Optimization in Multi-Objective Memetic Algorithms (Ishibuchi, 2008)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;In (Ishibuchi, 2008), for a &amp;lt;m&amp;gt;k&amp;lt;/m&amp;gt;-objectives optimization problem, the author use the following weighted sum fitness function for local search:&amp;lt;p&amp;gt;[br][br]&amp;lt;m&amp;gt;f(x)=lambda_1 f_1(x) + lambda_2 f_2(x) + cdots + \\lambda_k f_k(x)&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;where &amp;lt;m&amp;gt;vec{lambda}=delim{lbrace}{lambda_1, lambda_2, cdots, lambda_k}{rbrace}&amp;lt;/m&amp;gt; is a weight vector, Before the execution, the author first generate a set of uniformly distributed weight vectors. More specifically, the author generate all integer vectors satisfying the following conditions:&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;lambda_1 + lambda_2 + cdots + lambda_k=d, lambda_i in delim{lbrace}{0, 1, cdots, d}{rbrace} forall i=1, 2, cdots, k&amp;lt;/m&amp;gt;[br][br]&amp;lt;p&amp;gt;where &amp;lt;m&amp;gt;d&amp;lt;/m&amp;gt; is a pre-defined integer. A set of unique weight vectors (specified by the value &amp;lt;m&amp;gt;d&amp;lt;/m&amp;gt;) can be generated for a &amp;lt;m&amp;gt;k&amp;lt;/m&amp;gt;-objective problem. Local search is applied to the chosen candidate solution with the local search application probability &amp;lt;m&amp;gt;P_{LS}&amp;lt;/m&amp;gt;. The weighted sum fitness function with the current weight vector is used as the computed fitness in local search. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Suppose for the &amp;lt;m&amp;gt;k&amp;lt;/m&amp;gt;-objectives optimization problem, there is a general local search procedure &amp;lt;m&amp;gt;LS_G&amp;lt;/m&amp;gt; which is not specific for any objective, and also a set of &amp;lt;m&amp;gt;k&amp;lt;/m&amp;gt; local search procedures &amp;lt;m&amp;gt;LS_S=delim{lbrace}{LS_{S1}, LS_{S2}, cdots, LS_{Sk}}{rbrace}&amp;lt;/m&amp;gt; where &amp;lt;m&amp;gt;LS_{Si}&amp;lt;/m&amp;gt; is specific for the &amp;lt;m&amp;gt;i^{th}&amp;lt;/m&amp;gt; objective. To choose one of the &amp;lt;m&amp;gt;(k+1)&amp;lt;/m&amp;gt; local search procedures for each solution (i.e., how to coordinate the local search procedures in local search), denote the selection probability of each local search procedure as &amp;lt;m&amp;gt;P_G, P_{S1}, P_{S2}, cdots , P_{Sk}&amp;lt;/m&amp;gt; where &amp;lt;m&amp;gt;P_G+P_{S1}+P_{S2}+ cdots +P_{Sk} = 1&amp;lt;/m&amp;gt;. Also denote the selection probability of the specific local search procedures as &amp;lt;m&amp;gt;P_S&amp;lt;/m&amp;gt; where &amp;lt;m&amp;gt;P_S = P_{S1}+P_{S2}+ ... +P_{Sk}&amp;lt;/m&amp;gt;. The paper suggest three ways to select the &amp;lt;m&amp;gt;LS_S&amp;lt;/m&amp;gt; to be applied to an individual solution.[br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;b&amp;gt;Rand version:&amp;lt;/b&amp;gt;[br]Random selection of one of the k specific local search procedures.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Prob version:&amp;lt;/b&amp;gt;[br]Probabilistic selection where &amp;lt;m&amp;gt;P_{S1}:P_{S2}: cdots :P_{Sk} = lambda_1 : lambda_2 : cdots : \\lambda_k&amp;lt;/m&amp;gt;[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;b&amp;gt;Max version:&amp;lt;/b&amp;gt;[br]Deterministic selection using the maximum weight &amp;lt;m&amp;gt;max_i delim{lbrace}{lambda_i delim{|}{ i=1, 2, cdots, k}{}}{rbrace}&amp;lt;/m&amp;gt;.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;h4&amp;gt;Robust Objective (Burke, 2009)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;In (Burke, 2009), the diversity of the population is encouraged by hybridising the underlying genetic algorithm with three different local search operators. The algorithm is working on the multi-objective problem of airline scheduling (e.g. the problem has two objectives &amp;lt;m&amp;gt;R&amp;lt;/m&amp;gt;, &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt;), and proposes three local searches: &amp;lt;m&amp;gt;LS_{R,F}&amp;lt;/m&amp;gt; (for multi-objective LS), &amp;lt;m&amp;gt;LS_{R}&amp;lt;/m&amp;gt; (single objective LS), &amp;lt;m&amp;gt;LS_{F}&amp;lt;/m&amp;gt; (single objective LS). In all cases,the local search is greedy and continues until the local optimum for the partial schedule,with respect to the considered objectives,is found. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Two schemes are proposed in how to select a local search from the three local searches to apply on a candidate solution $x$: &amp;lt;/p&amp;gt;[br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;[br]The first scheme is simple random in which one of the three local searches is picked and applied to &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt;; [br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]&amp;lt;p&amp;gt;The second scheme is a biased roulette wheel selection process based on the relative position of &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; in the fitness space. The bias towards a certain local search is based on the relative position of $x$ with respect to boundaries of the objective space as they are known in the current stage of the search, denoted by &amp;lt;m&amp;gt;R_{min}&amp;lt;/m&amp;gt; (min value in objective &amp;lt;m&amp;gt;R&amp;lt;/m&amp;gt;), &amp;lt;m&amp;gt;R_{max}&amp;lt;/m&amp;gt; (max value in objective &amp;lt;m&amp;gt;R&amp;lt;/m&amp;gt;), &amp;lt;m&amp;gt;F_{max}&amp;lt;/m&amp;gt; (max value in objective &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt;). The relative position of &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; is defined by &amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;d_{R(x)} = 1 - {R_{max} - R(x)} / {R_{max}- R_{min}}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;d_{F(x)} = 1 - {F_{max} - F(x)} / {F_{max}- F_{min}}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;The value of &amp;lt;m&amp;gt;d_{R(x)}&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;d_{F(x)}&amp;lt;/m&amp;gt; are use to calculate the share in objectives &amp;lt;m&amp;gt;R&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt;, denoted by &amp;lt;m&amp;gt;s_{R(x)}&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;s_{R(x)}&amp;lt;/m&amp;gt;, as defined by&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;s_{R(x)}={d_{R(x)}} / {d_{R(x)} + d_{F(x)}}&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;s_{F(x)}={d_{F(x)}} / {d_{R(x)} + d_{F(x)}}&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;Next the values &amp;lt;m&amp;gt;s_{LS_{R, F}}&amp;lt;/m&amp;gt;, &amp;lt;m&amp;gt;s_{LS_{R}}&amp;lt;/m&amp;gt;, &amp;lt;m&amp;gt;s_{LS_{F}}&amp;lt;/m&amp;gt; determine the shares of the local search operators on the roulette wheel that is used to select a local searcher in our biased selection scheme, which are defined by &amp;lt;/p&amp;gt;[br]&amp;lt;m&amp;gt;s_{LS_{R, F}} = {s_{R(x)} + s_{F(x)}} / {2} &amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;s_{LS_{R}} = {s_{R(x)}} / {2} &amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;s_{LS_{F}} = {s_{F(x)}} / {2} &amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br][br]Finally, an archive, &amp;lt;m&amp;gt;A&amp;lt;/m&amp;gt;, is maintained as an external population of well-distributed and non-dominated solutions using the adaptive grid archiving algorithm to prevent good solutions from getting lost due to the stochastic nature of genetic selection or non-improving moves with respect to one objective while a single objective local searcher improves the other objective."/>
  <memeticdictionary id="43" detail="Individual Improvement vs Cost" parentid="34" description="&amp;lt;h4&amp;gt;Meta-Lamarckian (Ong, 2004; Ong, 2006)&amp;lt;/h4&amp;gt;[br]during Meta-Lamarckian learning, the reward is measured using the improvements contributed by the LS to each chromosome that has been searched using &amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;eta = beta delim{|}{pf - cf}{|} / mu&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br]where [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;pf&amp;lt;/m&amp;gt; is the initial function fitness of a parent chromosome before local search&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;cf&amp;lt;/m&amp;gt; is the final function fitness of the child chromosome obtained after applying local search&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;mu&amp;lt;/m&amp;gt; is the number of LS function evaluation calls made to reach the[br]improved child chromosome or solution&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br]The term provides a simple measure of the rate at which the local search improves a design. &amp;lt;m&amp;gt;beta&amp;lt;/m&amp;gt; signifies the relative reward which scales the absolute reward in[br]proportion to the method\'s ability to produce high quality genotypes when compared with the best global solution obtained so far. Here, &amp;lt;m&amp;gt;beta&amp;lt;/m&amp;gt; is set as (&amp;lt;m&amp;gt;cf / sigma&amp;lt;/m&amp;gt;) or (&amp;lt;m&amp;gt;sigma / cf&amp;lt;/m&amp;gt;), for minimization and maximization problems, respectively. &amp;lt;m&amp;gt;sigma&amp;lt;/m&amp;gt; is the best solution encountered so far in the global search.[br][br]&amp;lt;h4&amp;gt;Cost Benefit Adaptation in Multimeme Algorithms (Jakob 2006; Jakob, 2007)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The basic idea is to use the costs measured in evaluations caused by and the benefit[br]measured in fitness gain obtained from an LS run to control the selection of an[br]LS out of a given set and the intensity of their search as well as the frequency of[br]their usage.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;[br]For the fitness gain a relative measure is used, because a certain amount of fitness improvement is much easier to achieve in the beginning of a search than in the end. The relative fitness gain &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; is based on a normalised fitness function in[br]the range of 0 and &amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt;, which turns every task into a maximisation problem. &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; is the ratio between the achieved fitness improvement (&amp;lt;m&amp;gt;f_{LS}&amp;lt;/m&amp;gt;&#x2212;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt;) and the maximum possible one (&amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt;&#x2212;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt;), where [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_{LS}&amp;lt;/m&amp;gt; is the fitness obtained by the LS and &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt; the fitness of the offspring as produced by the evolution.&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br]&amp;lt;/p&amp;gt;[br]For each LS the required evaluation &amp;lt;i&amp;gt;eval&amp;lt;/i&amp;gt; and the obtained &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; are calculated per LS usage and summed up. the probability relation between different local searches &amp;lt;m&amp;gt;LS_i&amp;lt;/m&amp;gt; (&amp;lt;m&amp;gt;i in&amp;lt;/m&amp;gt; {1, 2, 3} assuming only 3 LS) is shown below:&amp;lt;br /&amp;gt;[br][br]&amp;lt;m&amp;gt;{sum{}{}{rfg_{LS_1}}} / {sum{}{}{eval_{LS_1}}}:{sum{}{}{rfg_{LS_2}}} / {sum{}{}{eval_{LS_2}}}:{sum{}{}{rfg_{LS_3}}} / {sum{}{}{eval_{LS_3}}}&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br][br]The adaptation strategy is as follows:[br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;[br]If either each &amp;lt;m&amp;gt;LS_i&amp;lt;/m&amp;gt; was used at minimum $K$ times or there have been $M_{max}$ matings in total since the last adjustment, the new relation of the LS probabilities is computed. [br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;If the probability of one LS drops below &amp;lt;m&amp;gt;P_{min}&amp;lt;/m&amp;gt; for three consecutive alterations, it is ignored from then on. (To avoid premature deactivation, the probability is set to &amp;lt;m&amp;gt;P_{min}&amp;lt;/m&amp;gt; for the first time it is lower than &amp;lt;m&amp;gt;P_{min}&amp;lt;/m&amp;gt;. For the experiments &amp;lt;m&amp;gt;P_{min}&amp;lt;/m&amp;gt; was set to 0.1) [br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br]As erroneous deactivations of an LS were observed using the above scheme (Jakob, 2006), the adaptation speed had to be reduced by comparably high values for the re-adjustment thresholds. As a consequence, the extended LS adaptation procedure (Jakob, 2007) uses the old distribution by summing up one third of the old probabilities and two thirds of the newly calculated ones, thus resulting in a new likelihood of each LS.[br][br]&amp;lt;h4&amp;gt;Cost Benefit with fitness classification (Gutin, 2007)&amp;lt;/h4&amp;gt;[br]In (Gutin, 2007), there are two features of interest:[br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;[br]The local improvement procedure is &amp;lt;b&amp;gt;nondeterministic&amp;lt;/b&amp;gt;. It has several iterations and each iteration applies a certain improvement heuristic (e.g. first swap, best swap, random swap, 3-opt, .etc) with a certain value of its parameter (if it has one) to the given solution. The author have implemented several improvement heuristics and each time the local search choose one of them (together with a value of its parameter) with a probability.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]The particular heuristic choice probability depends on its previous results as follows. The author partition all possible solution into fitness intervals &amp;lt;m&amp;gt;delim{[}{b_i,b_{i+1}}{]}, i = 0,1, &amp;lt;/m&amp;gt;..., where &amp;lt;m&amp;gt;b_i = 10 * 1.1^i&amp;lt;/m&amp;gt; (the partition is based on the TSP in this case). This effectively created a set of class labels &amp;lt;m&amp;gt;{L_1, L_2, cdots, L_i, cdots, L_N}&amp;lt;/m&amp;gt;, and each solution can be assigned a label &amp;lt;m&amp;gt;L_i&amp;lt;/m&amp;gt;[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt; [br]For each improvement heuristic &amp;lt;m&amp;gt;LS_k&amp;lt;/m&amp;gt; and for label &amp;lt;m&amp;gt;L_i&amp;lt;/m&amp;gt;, store the total running time (measured in processor ticks) &amp;lt;m&amp;gt;T(LS_k, L_i)&amp;lt;/m&amp;gt;, run number &amp;lt;m&amp;gt;N(LS_k, L_i)&amp;lt;/m&amp;gt;, and total improvement &amp;lt;m&amp;gt;I(LS_k, L_i)&amp;lt;/m&amp;gt;. Each time an improvement heuristic finishes its work, the authors do the following for the corresponding stored values: [br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Increase &amp;lt;m&amp;gt;N(LS_k, L_i)&amp;lt;/m&amp;gt; by one,&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Add the heuristic running time to its total running time &amp;lt;m&amp;gt;T(LS_k, L_i)&amp;lt;/m&amp;gt;, &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Add &amp;lt;m&amp;gt;f_{before} - f_{after}&amp;lt;/m&amp;gt; to the total improvement &amp;lt;m&amp;gt;I(LS_k, L_i)&amp;lt;/m&amp;gt;, where &amp;lt;m&amp;gt;f_{after}&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;f_{before}&amp;lt;/m&amp;gt; are the solution fitnesses after and before the iteration.[br]&amp;lt;/ol&amp;gt;[br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;[br]The authors calculate the heuristic quality (for the given length\'s interval), &amp;lt;m&amp;gt;Q(LS_k, L_i)&amp;lt;/m&amp;gt; as the total improvement &amp;lt;m&amp;gt;I(LS_k, L_i)&amp;lt;/m&amp;gt; divided by the total running time &amp;lt;m&amp;gt;T(LS_k, L_i)&amp;lt;/m&amp;gt;. If &amp;lt;m&amp;gt;LS_k&amp;lt;/m&amp;gt; is run on solution belonging to &amp;lt;m&amp;gt;L_i&amp;lt;/m&amp;gt; at most twice, &amp;lt;m&amp;gt;q(LS_k, L_i)&amp;lt;/m&amp;gt; is set to 1.[br]&amp;lt;/li&amp;gt;[br][br]&amp;lt;li&amp;gt;[br]The particular heuristic choice probability &amp;lt;m&amp;gt;P(LS_k, L_i)&amp;lt;/m&amp;gt; for a solution belonging to category &amp;lt;m&amp;gt;L_i&amp;lt;/m&amp;gt; is proportional to &amp;lt;br /&amp;gt;[br][br]&amp;lt;m&amp;gt;Q(LS_k, L_i)+{Q(L_i)}/10&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]where &amp;lt;m&amp;gt;Q(L_i)={sum{k=1}{K}{Q(LS_k, L_i)}} / K&amp;lt;/m&amp;gt; is the average heuristic quality for the particular solution category &amp;lt;m&amp;gt;L_i&amp;lt;/m&amp;gt;"/>
  <memeticdictionary id="44" detail="Population Improvement vs Cost" parentid="34" description="In this type of adaptation, the fitness metric is based on population improvement by comparing the fitness of the current population with the previous generation (s)[br][br]&amp;lt;h4&amp;gt;Cross-dominance adaptation in multi-objective MA (Caponia, 2009)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;In order to determine whether Simulated Annealing (SA) or Rosenbroke Algorithm (RA) is applied as the local searcher, a value called &amp;lt;b&amp;gt;Cross-Dominance&amp;lt;/b&amp;gt;, at the end of each generation, &amp;lt;m&amp;gt;lambda&amp;lt;/m&amp;gt;, is calculated by:&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;&amp;lt;m&amp;gt;lambda=Lambda^{t+1} / N^2&amp;lt;/m&amp;gt;&amp;lt;/p&amp;gt;[br][br]where [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;[br]&amp;lt;m&amp;gt;N&amp;lt;/m&amp;gt; is the size of the population[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;Lambda&amp;lt;/m&amp;gt; is the number of dominances found when comparing the two population at generation &amp;lt;m&amp;gt;t&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;t+1&amp;lt;/m&amp;gt;, each having &amp;lt;m&amp;gt;N&amp;lt;/m&amp;gt; candidate solutions (therefore a total of &amp;lt;m&amp;gt;N^2&amp;lt;/m&amp;gt; comparisons between each solution in &amp;lt;m&amp;gt;P(t)&amp;lt;/m&amp;gt; and that in &amp;lt;m&amp;gt;P(t+1)&amp;lt;/m&amp;gt;). [br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]The algorithm can monitor the overall improvements of the population by means of &amp;lt;m&amp;gt;lambda in delim{[}{0, 1}{]}&amp;lt;/m&amp;gt;. More specifically,[br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;If &amp;lt;m&amp;gt;lambda = 1&amp;lt;/m&amp;gt;, the algorithm is making excellent improvements and all individuals of the population at generation &amp;lt;m&amp;gt;t+1&amp;lt;/m&amp;gt; strictly dominate all individuals at generation &amp;lt;m&amp;gt;t&amp;lt;/m&amp;gt;. [br]&amp;lt;li&amp;gt;if &amp;lt;m&amp;gt;lambda = 0&amp;lt;/m&amp;gt; the algorithm is not leading to any improvement and the new population is equivalent to the old one in terms of dominance.&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;p&amp;gt;In order to perform coordination of the local search, for each local searcher, a generalized Wigner semicircle distribution is generated:&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;p(lambda)={2} / {pi R^2} sqrt{R^2 - (lambda - a)^2} {c} / {{2} / {pi R}}&amp;lt;/m&amp;gt;[br][br]&amp;lt;p&amp;gt;where [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;R&amp;lt;/m&amp;gt; is the radius of the distribution (the shape of the distribution depends on &amp;lt;m&amp;gt;R&amp;lt;/m&amp;gt;), [br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;a&amp;lt;/m&amp;gt; determines the shift of the distribution, [br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;c&amp;lt;/m&amp;gt; is the maximum value of the distribution. [br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br]&amp;lt;p&amp;gt;&amp;lt;m&amp;gt;p(lambda)&amp;lt;/m&amp;gt; returns the probability of the local search activation dependent upon the adaptive parameter &amp;lt;m&amp;gt;lambda&amp;lt;/m&amp;gt;.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;To coordinate local searches, during each generation, &amp;lt;m&amp;gt;p(lambda)&amp;lt;/m&amp;gt; is computed, and a pseudo-random number &amp;lt;m&amp;gt;epsilon in delim{[}{0, 1}{]}&amp;lt;/m&amp;gt; is generated and compared against &amp;lt;m&amp;gt;p(lambda)&amp;lt;/m&amp;gt;, [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;[br]if &amp;lt;m&amp;gt;epsilon &amp;lt; p(lambda)&amp;lt;/m&amp;gt;, execute simulated annealing on 5 individuals pseudo-randomly selected, for 3000 fitness evaluations; replace the 5 individuals with the results of the simulated annealing;[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]otherwise, execute RA on 25 individuals pseudo-randomly selected, for 1000 fitness evaluations;[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;h4&amp;gt;Adaptive Hill Climb (Wang, 2009)&amp;lt;/h4&amp;gt;[br]In AHC (Wang, 2009), two local searches are defined: GCHC and SMHC. The GCHC and SMHC operators are both allowed to work in the whole LS loop and are selected by probability to execute one step LS operation at every generation when the MA is running. Let &amp;lt;m&amp;gt;p_{gchc}&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;p_{smhc}&amp;lt;/m&amp;gt; denote the probabilities of applying GCHC and SMHC to the individual that is used for a local search, respectively, where &amp;lt;m&amp;gt;p_{gchc} + p_{smhc} = 1&amp;lt;/m&amp;gt;. [br][br]&amp;lt;p&amp;gt;At the start of this strategy, &amp;lt;m&amp;gt;p_{gchc}&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;p_{smhc}&amp;lt;/m&amp;gt; are both set to 0.5, which means giving a fair competition chance to each LS operator. &amp;lt;/p&amp;gt;[br][br]As each LS operator always makes a biased search, the LS operator which produces more improvements should be given a greater selection probability. Here, an adaptive learning approach is used to adjust the value of &amp;lt;m&amp;gt;p_{gchc}&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;p_{smhc}&amp;lt;/m&amp;gt; for each LS operator. Let &amp;lt;m&amp;gt;eta&amp;lt;/m&amp;gt; denotes the improvement degree of the selected individual when one LS operator is used to refine it and &amp;lt;m&amp;gt;eta&amp;lt;/m&amp;gt; can be calculated by:[br][br]&amp;lt;p&amp;gt;&amp;lt;m&amp;gt;eta=delim{|}{f_{imp} - f_{ini}}{|} / {f_{ini}}&amp;lt;/m&amp;gt;&amp;lt;/p&amp;gt;[br][br]where &amp;lt;m&amp;gt;f_{imp}&amp;lt;/m&amp;gt; is the final fitness of the individual after applying the local search and &amp;lt;m&amp;gt;f_{ini}&amp;lt;/m&amp;gt; is its initial fitness before the local search. At each generation, the degree of improvement of each LS operator is calculated when a predefined number &amp;lt;i&amp;gt;ls_size&amp;lt;/i&amp;gt; of iterations is achieved and then &amp;lt;m&amp;gt;p_{gchc}&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;p_{smhc}&amp;lt;/m&amp;gt;.[br][br]Suppose &amp;lt;m&amp;gt;eta_{gchc}(t)&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;eta_{smhc}(t)&amp;lt;/m&amp;gt;, respectively, denote the total improvement of GCHC and SMHC at generation &amp;lt;m&amp;gt;t&amp;lt;/m&amp;gt;. The LS selection probabilities &amp;lt;m&amp;gt;p_{gchc}(t + 1)&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;p_{smhc}(t + 1)&amp;lt;/m&amp;gt; at generation (&amp;lt;m&amp;gt;t+1&amp;lt;/m&amp;gt;) can be calculated orderly by the following[br][br]&amp;lt;m&amp;gt;p_{gchc}(t+1) = p_{gchc}(t)+ Delta * eta_{gchc}(t)&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;p_{smhc}(t+1) = p_{smhc}(t)+ Delta * eta_{smhc}(t)&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;p_{gchc}(t+1) = {p_{gchc}(t+1)} / {p_{gchc}(t+1)+p_{smhc}(t+1)}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;p_{smhc}(t+1) = {p_{smhc}(t+1)} / {p_{gchc}(t+1)+p_{smhc}(t+1)}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]where &amp;lt;m&amp;gt;Delta&amp;lt;/m&amp;gt; signifies the relative influence of the degree of the improvement on the selection probability. [br][br]&amp;lt;div style=&amp;quot;color:red&amp;quot;&amp;gt;Note that the cost for this adaptation is fixed therefore the term does not appear in &amp;lt;m&amp;gt;eta&amp;lt;/m&amp;gt;&amp;lt;/div&amp;gt;"/>
  <memeticdictionary id="45" detail="Domain Specific Adaptation" parentid="0" description="Adaptive Memetic Algorithms have also been developed for specific problem domain such as multi-objective optimization or dynamic optimization. As these problem domain have different considerations, Domain-specific adaptive memetic algorithms have been developed that are specially designed to solve these domain-specific requirements."/>
  <memeticdictionary id="46" detail="Dynamic Optimization" parentid="45" description="&amp;lt;h4&amp;gt;Adaptive Hill Climb (Wang, 2009)&amp;lt;/h4&amp;gt;[br]In AHC (Wang, 2009), two local searches are defined: GCHC and SMHC. The GCHC and SMHC operators are both allowed to work in the whole LS loop and are selected by probability to execute one step LS operation at every generation when the MA is running. Let &amp;lt;m&amp;gt;p_{gchc}&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;p_{smhc}&amp;lt;/m&amp;gt; denote the probabilities of applying GCHC and SMHC to the individual that is used for a local search, respectively, where &amp;lt;m&amp;gt;p_{gchc} + p_{smhc} = 1&amp;lt;/m&amp;gt;. [br][br]&amp;lt;p&amp;gt;At the start of this strategy, &amp;lt;m&amp;gt;p_{gchc}&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;p_{smhc}&amp;lt;/m&amp;gt; are both set to 0.5, which means giving a fair competition chance to each LS operator. &amp;lt;/p&amp;gt;[br][br]As each LS operator always makes a biased search, the LS operator which produces more improvements should be given a greater selection probability. Here, an adaptive learning approach is used to adjust the value of &amp;lt;m&amp;gt;p_{gchc}&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;p_{smhc}&amp;lt;/m&amp;gt; for each LS operator. Let &amp;lt;m&amp;gt;eta&amp;lt;/m&amp;gt; denotes the improvement degree of the selected individual when one LS operator is used to refine it and &amp;lt;m&amp;gt;eta&amp;lt;/m&amp;gt; can be calculated by:[br][br]&amp;lt;p&amp;gt;&amp;lt;m&amp;gt;eta=delim{|}{f_{imp} - f_{ini}}{|} / {f_{ini}}&amp;lt;/m&amp;gt;&amp;lt;/p&amp;gt;[br][br]where &amp;lt;m&amp;gt;f_{imp}&amp;lt;/m&amp;gt; is the final fitness of the individual after applying the local search and &amp;lt;m&amp;gt;f_{ini}&amp;lt;/m&amp;gt; is its initial fitness before the local search. At each generation, the degree of improvement of each LS operator is calculated when a predefined number &amp;lt;i&amp;gt;ls_size&amp;lt;/i&amp;gt; of iterations is achieved and then &amp;lt;m&amp;gt;p_{gchc}&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;p_{smhc}&amp;lt;/m&amp;gt;.[br][br]Suppose &amp;lt;m&amp;gt;eta_{gchc}(t)&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;eta_{smhc}(t)&amp;lt;/m&amp;gt;, respectively, denote the total improvement of GCHC and SMHC at generation &amp;lt;m&amp;gt;t&amp;lt;/m&amp;gt;. The LS selection probabilities &amp;lt;m&amp;gt;p_{gchc}(t + 1)&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;p_{smhc}(t + 1)&amp;lt;/m&amp;gt; at generation (&amp;lt;m&amp;gt;t+1&amp;lt;/m&amp;gt;) can be calculated orderly by the following[br][br]&amp;lt;m&amp;gt;p_{gchc}(t+1) = p_{gchc}(t)+ Delta * eta_{gchc}(t)&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;p_{smhc}(t+1) = p_{smhc}(t)+ Delta * eta_{smhc}(t)&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;p_{gchc}(t+1) = {p_{gchc}(t+1)} / {p_{gchc}(t+1)+p_{smhc}(t+1)}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;p_{smhc}(t+1) = {p_{smhc}(t+1)} / {p_{gchc}(t+1)+p_{smhc}(t+1)}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]where &amp;lt;m&amp;gt;Delta&amp;lt;/m&amp;gt; signifies the relative influence of the degree of the improvement on the selection probability. [br][br]&amp;lt;div style=&amp;quot;color:red&amp;quot;&amp;gt;Note that the cost for this adaptation is fixed therefore the term does not appear in &amp;lt;m&amp;gt;eta&amp;lt;/m&amp;gt;&amp;lt;/div&amp;gt;"/>
  <memeticdictionary id="47" detail="LS Frequency" parentid="33" description="It is necessary to control the operation of the LS over the total visited solutions. This is because the additional function evaluations required for total search can be very expensive and the MA could become a multi-restart LS and not take advantage of the qualities of the EAs.[br][br]&amp;lt;h4&amp;gt;Cost Benefit Adaptation in Multimeme Algorithms (Jakob 2006; Jakob, 2007)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The basic idea is to use the costs measured in evaluations caused by and the benefit[br]measured in fitness gain obtained from an LS run to control the selection of a particular value of frequency&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;[br]For the fitness gain a relative measure is used, because a certain amount of fitness improvement is much easier to achieve in the beginning of a search than in the end. The relative fitness gain &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; is based on a normalised fitness function in[br]the range of 0 and &amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt;, which turns every task into a maximisation problem. &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; is the ratio between the achieved fitness improvement (&amp;lt;m&amp;gt;f_{LS}&amp;lt;/m&amp;gt;&#x2212;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt;) and the maximum possible one (&amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt;&#x2212;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt;), where [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_{LS}&amp;lt;/m&amp;gt; is the fitness obtained by the LS and &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt; the fitness of the offspring as produced by the evolution.&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br]&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;A set of levels is defined for the frequency parameter, each of which has a probability &amp;lt;m&amp;gt;p&amp;lt;/m&amp;gt; and a value &amp;lt;m&amp;gt;v&amp;lt;/m&amp;gt; containing for each level an appropriate value of the LS frequency. Three consecutive levels are always active, i.e. have a probability &amp;lt;m&amp;gt;p&amp;lt;/m&amp;gt; greater than zero. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Initially, a likeliness of 0.5 is assigned to the lowest level, 0.3 to the next one,[br]and 0.2 to the last one, ensuring that the search will start coarsely. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;[br]If either each level was used at &amp;lt;m&amp;gt;L_{min}&amp;lt;/m&amp;gt; times or they all have been used &amp;lt;m&amp;gt;L_{max}&amp;lt;/m&amp;gt; in total since the last adjustment, The probabilities of the levels are adjusted. The new relation among the active levels &amp;lt;m&amp;gt;L_1&amp;lt;/m&amp;gt;, &amp;lt;m&amp;gt;L_2&amp;lt;/m&amp;gt;, and &amp;lt;m&amp;gt;L_3&amp;lt;/m&amp;gt; is calculated as follows (For each active level &amp;lt;m&amp;gt;L_i&amp;lt;/m&amp;gt; the required evaluations &amp;lt;m&amp;gt;eval_{L_i,LS_k}&amp;lt;/m&amp;gt; and the obtained &amp;lt;m&amp;gt;rfg_{L_i, LS_k} &amp;lt;/m&amp;gt; are calculated per LS (i.e. for each &amp;lt;m&amp;gt;LS_k&amp;lt;/m&amp;gt;) usage and summed up):&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;{sum{LS_k}{}{rfg_{L_1, LS_k}}} / {sum{LS_k}{}{eval_{L_1, LS_k}}}:{sum{LS_k}{}{rfg_{L_2, LS_k}}} / {sum{LS_k}{}{eval_{L_2, LS_k}}}:{sum{LS_k}{}{rfg_{L_3, LS_k}}} / {sum{LS_k}{}{eval_{L_3, LS_k}}}&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;After the above computation, if the lowest or highest active level is given a probability of more than 0.5, the next lower or higher, respectively, is added. The level at the opposite end is dropped and its likeliness is added to its neighbour. The new level is given a probability equal to 20% from the sum of the probabilities of the other two levels. This causes a move of three consecutive levels along the scale of possible ones according to their performance determined by the achieved fitness gain and the required evaluations.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;To ensure mobility in both directions none of the three active levels may have a probability below 0.1.&amp;lt;/p&amp;gt;[br][br][br]&amp;lt;h4&amp;gt;Fitness based LS Frequency Adaptation(Gacia, 2008)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;(Gacia, 2008) have included in the algorithm the Adaptive &amp;lt;m&amp;gt;P_{LS}&amp;lt;/m&amp;gt; Mechanism, which is an adaptive fitness-based method that is very simple. Indeed, this scheme assigns a LS probability value to each chromosome generated by crossover and mutation, &amp;lt;m&amp;gt;c_{new}&amp;lt;/m&amp;gt;:&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;P_{LS}=delim{lbrace}{matrix{2}{2}{1 {f(c_{new}) better than f(C_{worst})} 0.0625 {otherwise}}}{}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]where &amp;lt;m&amp;gt;f&amp;lt;/m&amp;gt; is the fitness function and &amp;lt;m&amp;gt;C_{worst}&amp;lt;/m&amp;gt; is the current worst element in the population. Applying LS to as little of 5% of each population results in[br]faster convergence to good solutions.[br][br]&amp;lt;h4&amp;gt;Simulated Heating (Zitzler, 2000; Bambha, 2004)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The authors introduce a technique, called &amp;lt;b&amp;gt;simulated heating&amp;lt;/b&amp;gt;, that systematically incorporates parameterized local search into the framework of global search. Let &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; be a parameter in the local search (e.g. LS intensity, LS frequency, size of LS neighborhood). The idea of simulated heating can be summarized as follows: Instead of keeping &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; constant for the entire optimization process, &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is initially given a low value (leading to low LS cost &amp;lt;i&amp;gt;C(p)&amp;lt;/i&amp;gt; and accuracy &amp;lt;i&amp;gt;A(p)&amp;lt;/i&amp;gt;) and increase it at certain points in time (which in turn increases &amp;lt;i&amp;gt;C(p)&amp;lt;/i&amp;gt; and &amp;lt;i&amp;gt;A(b)&amp;lt;/i&amp;gt;). &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;According to the authors, If &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is fixed, each iteration of the overall optimization procedure requires approximately the same amount of time. In contrast, with simulated heating the time resources used per iteration are lower at the beginning and higher at the end. That is, in the first half of the time a greater number of iterations is performed than in the second half with regard to a single run. In other words, the goal is to focus on the global search at the beginning and to find promising regions first; for this phase, LS runs with low accuracy, which in turn allows an greater number of optimization steps of GS. Afterwards, more time is spent by LS in order to improve the solutions found So far and/or to assess them more accurately. As a consequence, fewer global search operations are possible during this phase of optimization. &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is systematically increased in the course of time. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;The Heating Scheme &amp;lt;i&amp;gt;H(t)&amp;lt;/i&amp;gt; for parameter &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is a function of optimization iterations &amp;lt;i&amp;gt;t&amp;lt;/i&amp;gt; that defines the value of &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; at different iteration &amp;lt;i&amp;gt;t&amp;lt;/i&amp;gt;&amp;lt;/p&amp;gt;[br]In realizing simulated heating, there are two fundamental ways of implementing a heating scheme &amp;lt;i&amp;gt;H&amp;lt;/i&amp;gt;: by &amp;lt;b&amp;gt;static&amp;lt;/b&amp;gt; or &amp;lt;b&amp;gt;dynamic&amp;lt;/b&amp;gt; adaptation mechanisms. In the first case, it is assumed that the heating scheme is fixed per optimization run. Thus, it may be computed at compile-time or directly before the actual optimization. In the[br]latter case, the heating scheme evolves during run-time, i.e., is computed during the optimization run. Hence, it may vary for different runs&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Let a set of values &amp;lt;m&amp;gt;delim{lbrace}{p_1, p_2, cdots, p_i, cdots, p_m}{rbrace}&amp;lt;/m&amp;gt; be defined for the parameter &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt;, where &amp;lt;m&amp;gt;p_1 &amp;lt; p_2 &amp;lt; cdots &amp;lt; p_i &amp;lt; cdots &amp;lt; p_m&amp;lt;/m&amp;gt;, Example of Dynamic Heating Schemes are as follows: &amp;lt;/p&amp;gt;[br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Fixed Number of Iterations Per Parameter value(FIP): [br]Here, the next parameter value &amp;lt;m&amp;gt;p_i&amp;lt;/m&amp;gt; is taken when for a number &amp;lt;i&amp;gt;tStag&amp;lt;/i&amp;gt; of iterations the quality of the best solution in the solution candidate set has not improved. As a consequence, for each parameter a different amount of time may be considered until the stagnation condition is fulfilled.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Fixed Amount of Time Per Parameter value(FTP): [br]In this case, the next parameter &amp;lt;m&amp;gt;p_i&amp;lt;/m&amp;gt; is taken when for &amp;lt;i&amp;gt;Tstag&amp;lt;/i&amp;gt; seconds the quality of the best solution in the solution candidate set has not improved. As a consequence, for each parameter a different number of iterations may be considered until the stagnation condition is fulfilled.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br]"/>
  <memeticdictionary id="48" detail="LS Intensity" parentid="33" description="&amp;lt;h4&amp;gt;Cost Benefit Adaptation in Multimeme Algorithms (Jakob 2006; Jakob, 2007)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The basic idea is to use the costs measured in evaluations caused by and the benefit[br]measured in fitness gain obtained from an LS run to control the selection of a particular value of intensity (e.g. number of iteration that a local searcher will take)&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;[br]For the fitness gain a relative measure is used, because a certain amount of fitness improvement is much easier to achieve in the beginning of a search than in the end. The relative fitness gain &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; is based on a normalised fitness function in[br]the range of 0 and &amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt;, which turns every task into a maximisation problem. &amp;lt;m&amp;gt;rfg&amp;lt;/m&amp;gt; is the ratio between the achieved fitness improvement (&amp;lt;m&amp;gt;f_{LS}&amp;lt;/m&amp;gt;&#x2212;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt;) and the maximum possible one (&amp;lt;m&amp;gt;f_{max}&amp;lt;/m&amp;gt;&#x2212;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt;), where [br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_{LS}&amp;lt;/m&amp;gt; is the fitness obtained by the LS and &amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;&amp;lt;m&amp;gt;f_{evo}&amp;lt;/m&amp;gt; the fitness of the offspring as produced by the evolution.&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br]&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;A set of levels is defined for the intensity parameter, each of which has a probability &amp;lt;m&amp;gt;p&amp;lt;/m&amp;gt; and a value &amp;lt;m&amp;gt;v&amp;lt;/m&amp;gt; containing for each level an appropriate value of LS intensity. Three consecutive levels are always active, i.e. have a probability &amp;lt;m&amp;gt;p&amp;lt;/m&amp;gt; greater than zero. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Initially, a likeliness of 0.5 is assigned to the lowest level, 0.3 to the next one,[br]and 0.2 to the last one, ensuring that the search will start coarsely. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;[br]If either each level was used at &amp;lt;m&amp;gt;L_{min}&amp;lt;/m&amp;gt; times or they all have been used &amp;lt;m&amp;gt;L_{max}&amp;lt;/m&amp;gt; in total since the last adjustment, The probabilities of the levels are adjusted. The new relation among the active levels &amp;lt;m&amp;gt;L_1&amp;lt;/m&amp;gt;, &amp;lt;m&amp;gt;L_2&amp;lt;/m&amp;gt;, and &amp;lt;m&amp;gt;L_3&amp;lt;/m&amp;gt; is calculated as follows (For each active level &amp;lt;m&amp;gt;L_i&amp;lt;/m&amp;gt; the required evaluations &amp;lt;m&amp;gt;eval_{L_i,LS_k}&amp;lt;/m&amp;gt; and the obtained &amp;lt;m&amp;gt;rfg_{L_i, LS_k} &amp;lt;/m&amp;gt; are calculated per LS (i.e. for each &amp;lt;m&amp;gt;LS_k&amp;lt;/m&amp;gt;) usage and summed up):&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;{sum{LS_k}{}{rfg_{L_1, LS_k}}} / {sum{LS_k}{}{eval_{L_1, LS_k}}}:{sum{LS_k}{}{rfg_{L_2, LS_k}}} / {sum{LS_k}{}{eval_{L_2, LS_k}}}:{sum{LS_k}{}{rfg_{L_3, LS_k}}} / {sum{LS_k}{}{eval_{L_3, LS_k}}}&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;After the above computation, if the lowest or highest active level is given a probability of more than 0.5, the next lower or higher, respectively, is added. The level at the opposite end is dropped and its likeliness is added to its neighbour. The new level is given a probability equal to 20% from the sum of the probabilities of the other two levels. This causes a move of three consecutive levels along the scale of possible ones according to their performance determined by the achieved fitness gain and the required evaluations.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;To ensure mobility in both directions none of the three active levels may have a probability below 0.1.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Adaptation by Improvement Rate (Guo, 2005)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The local search used in the algorithm is a Simulated Annealing (SA), and the number of iteration &amp;lt;m&amp;gt;iter&amp;lt;/m&amp;gt; neighborhood perturbation at a particular annealing temperature &amp;lt;m&amp;gt;T&amp;lt;/m&amp;gt; is made self-adjusted based on the improvement rate &amp;lt;m&amp;gt;ir&amp;lt;/m&amp;gt;. The &amp;lt;m&amp;gt;ir&amp;lt;/m&amp;gt; at generation &amp;lt;m&amp;gt;g&amp;lt;/m&amp;gt; is defined as:&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;ir=delim{|}{delim{lbrace}{f(a) in nd(g); exists f(b) in nd(g-1); f(a) &amp;lt; f(b)}{rbrace}}{|} / delim{|}{nd(g)}{|}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;where &amp;lt;m&amp;gt;nd(g)&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;nd(g-1)&amp;lt;/m&amp;gt; are respectively the sets of all nondominated solutions held in the external archive at generations &amp;lt;m&amp;gt;g&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;(g-1)&amp;lt;/m&amp;gt;. With the evolution proceeding, &amp;lt;m&amp;gt;ir&amp;lt;/m&amp;gt; decreases gradually towards a small value close to zero, as indicates a bigger room for further improvement at the initial stage and lower probability of producing new nondominated solutions at the final stage. For better &amp;lt;m&amp;gt;ir&amp;lt;/m&amp;gt;, it is essential to tune the local search to reproduce fitter individuals, especial as the approximation approaches to the Pareto front. Thereby, the number iter of local exploration at each temperature in the SA should follow the reverse trend of &amp;lt;m&amp;gt;ir&amp;lt;/m&amp;gt; along the evolution, &amp;lt;m&amp;gt;iter&amp;lt;/m&amp;gt; at generation &amp;lt;m&amp;gt;g&amp;lt;/m&amp;gt; is determined as:[br][br]&amp;lt;m&amp;gt;iter_g(ir)=delim{lbrace}{matrix{4}{2}{{ub} {0&amp;lt;=ir &amp;lt;alpha} {ub-2(ub-lb)({ir-alpha_1}/g)^2} {alpha_1&amp;lt;=ir&amp;lt;alpha} {lb+2(ub-lb)({ir+1-alpha_2-g}/g)^2} {alpha&amp;lt;=ir&amp;lt;alpha_2} {lb} {alpha_2&amp;lt;=ir&amp;lt;1}}}{}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;where &amp;lt;m&amp;gt;lb, ub&amp;lt;/m&amp;gt; is the lower and upper bound of the number of iterations, &amp;lt;m&amp;gt;0 &amp;lt; alpha_1 &amp;lt; alpha &amp;lt; alpha_2 &amp;lt; 1&amp;lt;/m&amp;gt; is some interval cut on the value range of &amp;lt;m&amp;gt;ir&amp;lt;/m&amp;gt;.&amp;lt;/p&amp;gt;[br][br]&amp;lt;h4&amp;gt;Simulated Heating (Zitzler, 2000; Bambha, 2004)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The authors introduce a technique, called &amp;lt;b&amp;gt;simulated heating&amp;lt;/b&amp;gt;, that systematically incorporates parameterized local search into the framework of global search. Let &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; be a parameter in the local search (e.g. LS intensity, LS frequency, size of LS neighborhood). The idea of simulated heating can be summarized as follows: Instead of keeping &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; constant for the entire optimization process, &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is initially given a low value (leading to low LS cost &amp;lt;i&amp;gt;C(p)&amp;lt;/i&amp;gt; and accuracy &amp;lt;i&amp;gt;A(p)&amp;lt;/i&amp;gt;) and increase it at certain points in time (which in turn increases &amp;lt;i&amp;gt;C(p)&amp;lt;/i&amp;gt; and &amp;lt;i&amp;gt;A(b)&amp;lt;/i&amp;gt;). &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;According to the authors, If &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is fixed, each iteration of the overall optimization procedure requires approximately the same amount of time. In contrast, with simulated heating the time resources used per iteration are lower at the beginning and higher at the end. That is, in the first half of the time a greater number of iterations is performed than in the second half with regard to a single run. In other words, the goal is to focus on the global search at the beginning and to find promising regions first; for this phase, LS runs with low accuracy, which in turn allows an greater number of optimization steps of GS. Afterwards, more time is spent by LS in order to improve the solutions found So far and/or to assess them more accurately. As a consequence, fewer global search operations are possible during this phase of optimization. &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is systematically increased in the course of time. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;The Heating Scheme &amp;lt;i&amp;gt;H(t)&amp;lt;/i&amp;gt; for parameter &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is a function of optimization iterations &amp;lt;i&amp;gt;t&amp;lt;/i&amp;gt; that defines the value of &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; at different iteration &amp;lt;i&amp;gt;t&amp;lt;/i&amp;gt;&amp;lt;/p&amp;gt;[br]In realizing simulated heating, there are two fundamental ways of implementing a heating scheme &amp;lt;i&amp;gt;H&amp;lt;/i&amp;gt;: by &amp;lt;b&amp;gt;static&amp;lt;/b&amp;gt; or &amp;lt;b&amp;gt;dynamic&amp;lt;/b&amp;gt; adaptation mechanisms. In the first case, it is assumed that the heating scheme is fixed per optimization run. Thus, it may be computed at compile-time or directly before the actual optimization. In the[br]latter case, the heating scheme evolves during run-time, i.e., is computed during the optimization run. Hence, it may vary for different runs&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Let a set of values &amp;lt;m&amp;gt;delim{lbrace}{p_1, p_2, cdots, p_i, cdots, p_m}{rbrace}&amp;lt;/m&amp;gt; be defined for the parameter &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt;, where &amp;lt;m&amp;gt;p_1 &amp;lt; p_2 &amp;lt; cdots &amp;lt; p_i &amp;lt; cdots &amp;lt; p_m&amp;lt;/m&amp;gt;, Example of Dynamic Heating Schemes are as follows: &amp;lt;/p&amp;gt;[br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Fixed Number of Iterations Per Parameter value(FIP): [br]Here, the next parameter value &amp;lt;m&amp;gt;p_i&amp;lt;/m&amp;gt; is taken when for a number &amp;lt;i&amp;gt;tStag&amp;lt;/i&amp;gt; of iterations the quality of the best solution in the solution candidate set has not improved. As a consequence, for each parameter a different amount of time may be considered until the stagnation condition is fulfilled.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Fixed Amount of Time Per Parameter value(FTP): [br]In this case, the next parameter &amp;lt;m&amp;gt;p_i&amp;lt;/m&amp;gt; is taken when for &amp;lt;i&amp;gt;Tstag&amp;lt;/i&amp;gt; seconds the quality of the best solution in the solution candidate set has not improved. As a consequence, for each parameter a different number of iterations may be considered until the stagnation condition is fulfilled.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br][br]"/>
  <memeticdictionary id="49" detail="LS Individual Subset Selection" parentid="33" description=""/>
  <memeticdictionary id="50" detail="Self-Generating Memetic Algorithms" parentid="51" description="Self-Generating Memetic Algorithms are MAs that is able to create its own local searchers and to co-evolve their behaviours as required to successfully solve a given problem.... In the case of self-adaptation, &amp;quot;the parameters to be adapted are encoded onto the chromosome(s) of the individual and undergo mutation and recombination.&amp;quot; (Jakob, 2006)[br][br]&amp;lt;h4&amp;gt;Self-Generating Memetic Algorithm (Krasnogor, 2004)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The Self-Generating Memetic Algorithm in (Krasnogor, 2004) evolves concurrently both the genetic solutions and the local search move operators that it needs to solve the problem instance at hand. The concurrent generation of local search strategies and solutions allows the Memetic Algorithm to produce better results than those given by a Genetic Algorithm and a Memetic Algorithm with human-designed local searchers.&amp;lt;/p&amp;gt; [br][br]&amp;lt;p&amp;gt;The authors seek to produce a metaheuristic that creates from scratch the appropriate local searcher to use under different circumstances. The local search involved can be very complex and composed of several phases and processes. In the most general case we want to be able to explore the space of all possible memes (i.e. local searchers). This is achieved by using a formal grammar that describes memeplexes and by letting a[br]genetic programming (Koza, 1999) based system to evolve sentences in the language generated by that grammar.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;The sentences in the language generated by this grammar represent syntactically valid complex local searchers and they are the instructions used to implement specific search[br]behaviors and strategies.&amp;lt;/p&amp;gt;[br][br]"/>
  <memeticdictionary id="52" detail="Population Size" parentid="33" description="&amp;lt;h4&amp;gt;FAMA: Fitness Diversity based on &amp;lt;m&amp;gt;xi&amp;lt;/m&amp;gt; (Caponio, 2007)&amp;lt;/h4&amp;gt;[br]In FAMA (Caponio, 2007), at the end of each generation the fitness diversity measure &amp;lt;m&amp;gt;xi&amp;lt;/m&amp;gt; is also used to adaptively control the size of the FAMA population as given by[br]&amp;lt;m&amp;gt;S_{pop}={S^f}_{pop}+{S^{nu}}_{pop}(1-xi)&amp;lt;/m&amp;gt;[br][br]where &amp;lt;m&amp;gt;{S^{f}}_{pop}&amp;lt;/m&amp;gt; is the minimum size of the population and &amp;lt;m&amp;gt;{S^{nu}}_{pop}&amp;lt;/m&amp;gt; is the maximal increase of the population size. If &amp;lt;m&amp;gt;xi approx 1&amp;lt;/m&amp;gt; the population contains high diversity and therefore a small number of solutions need to be exploited. if &amp;lt;m&amp;gt;xi approx 0&amp;lt;/m&amp;gt; the population contains low diversity and will likely converge; in such conditions a larger population size is required to increase the exploration. [br][br]"/>
  <memeticdictionary id="53" detail="Mutation Rate" parentid="33" description="&amp;lt;h4&amp;gt;FAMA: Fitness Diversity based on &amp;lt;m&amp;gt;xi&amp;lt;/m&amp;gt;&amp;lt;/h4&amp;gt;[br][br]In FAMA (Caponio, 2007), the mutation rate also dynamically depends on the fitness diversity measure &amp;lt;m&amp;gt;xi&amp;lt;/m&amp;gt;: &amp;lt;m&amp;gt;p_m = 0.4 (1 - xi)&amp;lt;/m&amp;gt;[br][br]"/>
  <memeticdictionary id="54" detail="Fitness Based" parentid="38" description="The category of MAs use of the measure of relative position of current individual solution in the fitness space to adaptively select which meme to apply to it.[br][br]&amp;lt;h4&amp;gt;Robust Objective (Burke, 2009)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;In (Burke, 2009), the diversity of the population is encouraged by hybridising the underlying genetic algorithm with three different local search operators. The algorithm is working on the multi-objective problem of airline scheduling (e.g. the problem has two objectives &amp;lt;m&amp;gt;R&amp;lt;/m&amp;gt;, &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt;), and proposes three local searches: &amp;lt;m&amp;gt;LS_{R,F}&amp;lt;/m&amp;gt; (for multi-objective LS), &amp;lt;m&amp;gt;LS_{R}&amp;lt;/m&amp;gt; (single objective LS), &amp;lt;m&amp;gt;LS_{F}&amp;lt;/m&amp;gt; (single objective LS). In all cases,the local search is greedy and continues until the local optimum for the partial schedule,with respect to the considered objectives,is found. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Two schemes are proposed in how to select a local search from the three local searches to apply on a candidate solution &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt;: &amp;lt;/p&amp;gt;[br][br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;[br]The first scheme is simple random in which one of the three local searches is picked and applied to &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt;; [br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;[br]&amp;lt;p&amp;gt;The second scheme is a biased roulette wheel selection process based on the relative position of &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; in the fitness space. The bias towards a certain local search is based on the relative position of &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; with respect to boundaries of the objective space as they are known in the current stage of the search, denoted by &amp;lt;m&amp;gt;R_{min}&amp;lt;/m&amp;gt; (min value in objective &amp;lt;m&amp;gt;R&amp;lt;/m&amp;gt;), &amp;lt;m&amp;gt;R_{max}&amp;lt;/m&amp;gt; (max value in objective &amp;lt;m&amp;gt;R&amp;lt;/m&amp;gt;), &amp;lt;m&amp;gt;F_{max}&amp;lt;/m&amp;gt; (max value in objective &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt;). The relative position of &amp;lt;m&amp;gt;x&amp;lt;/m&amp;gt; is defined by &amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;d_{R(x)} = 1 - {R_{max} - R(x)} / {R_{max}- R_{min}}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;d_{F(x)} = 1 - {F_{max} - F(x)} / {F_{max}- F_{min}}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;The value of &amp;lt;m&amp;gt;d_{R(x)}&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;d_{F(x)}&amp;lt;/m&amp;gt; are use to calculate the share in objectives &amp;lt;m&amp;gt;R&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt;, denoted by &amp;lt;m&amp;gt;s_{R(x)}&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;s_{R(x)}&amp;lt;/m&amp;gt;, as defined by&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;s_{R(x)}={d_{R(x)}} / {d_{R(x)} + d_{F(x)}}&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;s_{F(x)}={d_{F(x)}} / {d_{R(x)} + d_{F(x)}}&amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br][br]&amp;lt;p&amp;gt;Next the values &amp;lt;m&amp;gt;s_{LS_{R, F}}&amp;lt;/m&amp;gt;, &amp;lt;m&amp;gt;s_{LS_{R}}&amp;lt;/m&amp;gt;, &amp;lt;m&amp;gt;s_{LS_{F}}&amp;lt;/m&amp;gt; determine the shares of the local search operators on the roulette wheel that is used to select a local searcher in our biased selection scheme, which are defined by &amp;lt;/p&amp;gt;[br]&amp;lt;m&amp;gt;s_{LS_{R, F}} = {s_{R(x)} + s_{F(x)}} / {2} &amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;s_{LS_{R}} = {s_{R(x)}} / {2} &amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br]&amp;lt;m&amp;gt;s_{LS_{F}} = {s_{F(x)}} / {2} &amp;lt;/m&amp;gt; &amp;lt;br /&amp;gt;[br][br]Finally, an archive, &amp;lt;m&amp;gt;A&amp;lt;/m&amp;gt;, is maintained as an external population of well-distributed and non-dominated solutions using the adaptive grid archiving algorithm to prevent good solutions from getting lost due to the stochastic nature of genetic selection or non-improving moves with respect to one objective while a single objective local searcher improves the other objective.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br]&amp;lt;h4&amp;gt;Fitness based LS Frequency Adaptation(Gacia, 2008)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;(Gacia, 2008) have included in the algorithm the Adaptive &amp;lt;m&amp;gt;P_{LS}&amp;lt;/m&amp;gt; Mechanism, which is an adaptive fitness-based method that is very simple. Indeed, this scheme assigns a LS probability value to each chromosome generated by crossover and mutation, &amp;lt;m&amp;gt;c_{new}&amp;lt;/m&amp;gt;:&amp;lt;/p&amp;gt;[br][br]&amp;lt;m&amp;gt;P_{LS}=delim{lbrace}{matrix{2}{2}{1 {f(c_{new}) better than f(C_{worst})} 0.0625 {otherwise}}}{}&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br][br]where &amp;lt;m&amp;gt;f&amp;lt;/m&amp;gt; is the fitness function and &amp;lt;m&amp;gt;C_{worst}&amp;lt;/m&amp;gt; is the current worst element in the population. Applying LS to as little of 5% of each population results in[br]faster convergence to good solutions."/>
  <memeticdictionary id="55" detail="Random Memetic Selection" parentid="1" description="In this selection strategy, a meme is selected randomly before each local search. It is purely stochastic in nature, and the probability of choosing each meme is kept constant throughout the search.[br][br]&amp;lt;h4&amp;gt;Random Descent (Ong 2006; Ozcan, 2008)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;Initially the choice of memes is decided randomly. Subsequently, this same meme is used repeatedly until no further local improvements can be found. This same process then repeats to consider all the other memes.&amp;lt;/p&amp;gt;[br][br][br]&amp;lt;h4&amp;gt;RandomPermDescent (Ong 2006; Ozcan, 2008)&amp;lt;/h4&amp;gt;[br]A random permutation of memes {M&amp;lt;sub&amp;gt;1&amp;lt;/sub&amp;gt;, M&amp;lt;sub&amp;gt;2&amp;lt;/sub&amp;gt;, ... M&amp;lt;sub&amp;gt;n&amp;lt;/sub&amp;gt;} is fixed in advance, and when the application of a meme does not result in any improvement, the next meme in the permutation is used.[br][br]&amp;lt;h4&amp;gt;Random Selection (Neri, 2009)&amp;lt;/h4&amp;gt;[br]In (Neri, 2009), two local searchers (namely, Golden Section Search and Hill Climbing) are adaptively selected using probability &amp;lt;m&amp;gt;rho&amp;lt;/m&amp;gt; and &amp;lt;m&amp;gt;1 - rho&amp;lt;/m&amp;gt; (where &amp;lt;m&amp;gt;rho &amp;lt;= 1&amp;lt;/m&amp;gt;)[br][br][br]"/>
  <memeticdictionary id="57" detail="LS Neighborhood Size" parentid="58" description="&amp;lt;h4&amp;gt;Simulated Heating (Zitzler, 2000; Bambha, 2004)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The authors introduce a technique, called &amp;lt;b&amp;gt;simulated heating&amp;lt;/b&amp;gt;, that systematically incorporates parameterized local search into the framework of global search. Let &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; define the size of the LS neighborhood in the local search. The idea of simulated heating can be summarized as follows: Instead of keeping &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; constant for the entire optimization process, &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is initially given a low value (leading to low LS cost &amp;lt;i&amp;gt;C(p)&amp;lt;/i&amp;gt; and accuracy &amp;lt;i&amp;gt;A(p)&amp;lt;/i&amp;gt;) and increase it at certain points in time (which in turn increases &amp;lt;i&amp;gt;C(p)&amp;lt;/i&amp;gt; and &amp;lt;i&amp;gt;A(b)&amp;lt;/i&amp;gt;). &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;According to the authors, If &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is fixed, each iteration of the overall optimization procedure requires approximately the same amount of time. In contrast, with simulated heating the time resources used per iteration are lower at the beginning and higher at the end. That is, in the first half of the time a greater number of iterations is performed than in the second half with regard to a single run. In other words, the goal is to focus on the global search at the beginning and to find promising regions first; for this phase, LS runs with low accuracy, which in turn allows an greater number of optimization steps of GS. Afterwards, more time is spent by LS in order to improve the solutions found So far and/or to assess them more accurately. As a consequence, fewer global search operations are possible during this phase of optimization. &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is systematically increased in the course of time. &amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;The Heating Scheme &amp;lt;i&amp;gt;H(t)&amp;lt;/i&amp;gt; for parameter &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; is a function of optimization iterations &amp;lt;i&amp;gt;t&amp;lt;/i&amp;gt; that defines the value of &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt; at different iteration &amp;lt;i&amp;gt;t&amp;lt;/i&amp;gt;&amp;lt;/p&amp;gt;[br]In realizing simulated heating, there are two fundamental ways of implementing a heating scheme &amp;lt;i&amp;gt;H&amp;lt;/i&amp;gt;: by &amp;lt;b&amp;gt;static&amp;lt;/b&amp;gt; or &amp;lt;b&amp;gt;dynamic&amp;lt;/b&amp;gt; adaptation mechanisms. In the first case, it is assumed that the heating scheme is fixed per optimization run. Thus, it may be computed at compile-time or directly before the actual optimization. In the[br]latter case, the heating scheme evolves during run-time, i.e., is computed during the optimization run. Hence, it may vary for different runs&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;Let a set of values &amp;lt;m&amp;gt;delim{lbrace}{p_1, p_2, cdots, p_i, cdots, p_m}{rbrace}&amp;lt;/m&amp;gt; be defined for the parameter &amp;lt;i&amp;gt;p&amp;lt;/i&amp;gt;, where &amp;lt;m&amp;gt;p_1 &amp;lt; p_2 &amp;lt; cdots &amp;lt; p_i &amp;lt; cdots &amp;lt; p_m&amp;lt;/m&amp;gt;, Example of Dynamic Heating Schemes are as follows: &amp;lt;/p&amp;gt;[br]&amp;lt;ol&amp;gt;[br]&amp;lt;li&amp;gt;Fixed Number of Iterations Per Parameter value(FIP): [br]Here, the next parameter value &amp;lt;m&amp;gt;p_i&amp;lt;/m&amp;gt; is taken when for a number &amp;lt;i&amp;gt;tStag&amp;lt;/i&amp;gt; of iterations the quality of the best solution in the solution candidate set has not improved. As a consequence, for each parameter a different amount of time may be considered until the stagnation condition is fulfilled.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Fixed Amount of Time Per Parameter value(FTP): [br]In this case, the next parameter &amp;lt;m&amp;gt;p_i&amp;lt;/m&amp;gt; is taken when for &amp;lt;i&amp;gt;Tstag&amp;lt;/i&amp;gt; seconds the quality of the best solution in the solution candidate set has not improved. As a consequence, for each parameter a different number of iterations may be considered until the stagnation condition is fulfilled.[br]&amp;lt;/li&amp;gt;[br]&amp;lt;/ol&amp;gt;[br][br][br]"/>
  <memeticdictionary id="58" detail="LS Settings" parentid="33" description="&amp;lt;p&amp;gt;Adaptation in this category attempts to adaptively determine the parameter settings in a local search operator, such as: &amp;lt;/p&amp;gt;[br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;Size of LS neighborhood&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;LS move acceptance criteria&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;[br][br]&amp;lt;h4&amp;gt;Scale factor in memetic differential evolution (Neri, 2009)&amp;lt;/h4&amp;gt;[br]&amp;lt;p&amp;gt;The local search in (Neri, 2009) is applied to optimize the scale factor of DE, the scale factor &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt; is used during the crossover of DE to control its jump. The local search consists of two local searchers: namely Golden Section Search and Hill Climbing. During each iteration, the offspring solution &amp;lt;m&amp;gt;x_i&amp;lt;/m&amp;gt; is obtained using the DE crossover and the scale factor &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt;, if &amp;lt;m&amp;gt;x_i&amp;lt;/m&amp;gt; is the best, a probability &amp;lt;m&amp;gt;rho&amp;lt;/m&amp;gt; is applied to randomly select Golden Section Search or Hill Climbing as the local search to be applied to &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt;.&amp;lt;/p&amp;gt;[br][br]&amp;lt;p&amp;gt;The fitness &amp;lt;m&amp;gt;f(F)&amp;lt;/m&amp;gt; must be defined for the local search to identify the better scale factor in the neighborhood of current scale factor. the paper use the following procedure to compute &amp;lt;m&amp;gt;f(F)&amp;lt;/m&amp;gt;&amp;lt;/p&amp;gt;[br][br]&amp;lt;table border=&amp;quot;1&amp;quot;&amp;gt;[br]&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;[br]&amp;lt;b&amp;gt;Procedure::&amp;lt;/b&amp;gt;Compute &amp;lt;m&amp;gt;f(F)&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br]&amp;lt;b&amp;gt;BEGIN&amp;lt;/b&amp;gt; &amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;&amp;lt;m&amp;gt;v=x_t + F(x_{r1} - x_{r2})&amp;lt;/m&amp;gt;&amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;&amp;lt;m&amp;gt;u=crossover(x_t, x_{r3})&amp;lt;/m&amp;gt;; &amp;lt;br /&amp;gt;[br]&amp;amp;nbsp;&amp;amp;nbsp;&amp;lt;m&amp;gt;return&amp;lt;/m&amp;gt; &amp;lt;m&amp;gt;f(F)=fitness(u)&amp;lt;/m&amp;gt;; &amp;lt;br /&amp;gt;[br]&amp;lt;b&amp;gt;END&amp;lt;/b&amp;gt;&amp;lt;br /&amp;gt;[br]&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;[br]&amp;lt;/table&amp;gt;[br][br]&amp;lt;p&amp;gt;where &amp;lt;m&amp;gt;u&amp;lt;/m&amp;gt; is the offspring solution produced using DE crossover and the scale factor &amp;lt;m&amp;gt;F&amp;lt;/m&amp;gt;, and &amp;lt;m&amp;gt;f(F)=fitness(u)&amp;lt;/m&amp;gt; where &amp;lt;m&amp;gt;fitness(u)&amp;lt;/m&amp;gt; is the fitness of the offspring solution &amp;lt;m&amp;gt;u&amp;lt;/m&amp;gt;. The local search choice probability &amp;lt;m&amp;gt;rho&amp;lt;/m&amp;gt; seems is predefined value, therefore the LS selection is random.&amp;lt;/p&amp;gt; "/>
  <memeticdictionary id="59" detail="Acceptance" parentid="20" description="Determine how a meme is accepted as the selected meme at the decision point based on its meme fitness. The acceptance scheme can be either[br][br]&amp;lt;ul&amp;gt;[br]&amp;lt;li&amp;gt;Deterministic&amp;lt;/li&amp;gt;[br]&amp;lt;li&amp;gt;Probabilistic&amp;lt;/li&amp;gt;[br]&amp;lt;/ul&amp;gt;"/>
  <memeticdictionary id="60" detail="Probabilistic Acceptance" parentid="59" description=""/>
  <memeticdictionary id="61" detail="Deterministic Acceptance" parentid="59" description=""/>
</mcmemeticdictionary>
